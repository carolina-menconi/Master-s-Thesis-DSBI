{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be69ca7-5a2c-4f51-bd17-01ed312382d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, duckdb, torch, timm, gc, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import duckdb, torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "\n",
    "from scipy.sparse import load_npz, hstack, save_npz\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436868df-b725-4a6c-957d-1eb9f10f00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74cc10-5dce-437d-b5ba-58f3d2700570",
   "metadata": {},
   "source": [
    "# CROSS-ATTENTION TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d04d8c-3db9-4d47-a89d-e0f32577b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the image tokens related to the same post and compute the mean\n",
    "def aggregate_image_tokens_per_post(ids_txt, ids_img, img_tokens):\n",
    "    img_tokens_post = []\n",
    "\n",
    "    for pid in ids_txt:\n",
    "        mask = (ids_img == pid)\n",
    "        tokens_p = img_tokens[mask]\n",
    "        \n",
    "        if tokens_p.shape[0] == 0:\n",
    "            agg = np.zeros((50, img_tokens.shape[-1]), dtype=img_tokens.dtype)\n",
    "        else:\n",
    "            agg = tokens_p.mean(axis=0)\n",
    "        \n",
    "        img_tokens_post.append(agg)\n",
    "\n",
    "    return np.stack(img_tokens_post, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dc4276-351f-4061-8332-c8e06f5cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_batch_loader(txt, img, y, batch_size=32, shuffle=True):\n",
    "    N = len(y)\n",
    "    idxs = np.arange(N)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = idxs[start:end]\n",
    "\n",
    "        yield txt[batch_idx], img[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc271528-6534-4a89-bf32-035873853f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dim_txt = 512\n",
    "dim_img_in = 768\n",
    "dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Image projection to get 512 dimension\n",
    "W_img = nn.Linear(dim_img_in, dim).to(device)\n",
    "\n",
    "# Cross-attention layers\n",
    "attn_t2i = nn.MultiheadAttention(dim, num_heads, batch_first=True).to(device)\n",
    "attn_i2t = nn.MultiheadAttention(dim, num_heads, batch_first=True).to(device)\n",
    "\n",
    "# Feed-forward \n",
    "ff_t = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ").to(device)\n",
    "\n",
    "ff_i = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ").to(device)\n",
    "\n",
    "# Layer Norm \n",
    "norm_t1 = nn.LayerNorm(dim).to(device)\n",
    "norm_t2 = nn.LayerNorm(dim).to(device)\n",
    "norm_i1 = nn.LayerNorm(dim).to(device)\n",
    "norm_i2 = nn.LayerNorm(dim).to(device)\n",
    "\n",
    "# CLASSIFIER\n",
    "classifier = nn.Linear(2*dim, 5).to(device)\n",
    "\n",
    "# OPTIMIZER: it defines the parameters to train during the training of the last layer (the ones defined before)\n",
    "params = (\n",
    "    list(W_img.parameters()) +\n",
    "    list(attn_t2i.parameters()) +\n",
    "    list(attn_i2t.parameters()) +\n",
    "    list(ff_t.parameters()) +\n",
    "    list(ff_i.parameters()) +\n",
    "    list(norm_t1.parameters()) +\n",
    "    list(norm_t2.parameters()) +\n",
    "    list(norm_i1.parameters()) +\n",
    "    list(norm_i2.parameters()) +\n",
    "    list(classifier.parameters())\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss() # Loss function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f535df-e722-4460-89b3-6df969791ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_forward(txt, img):\n",
    "\n",
    "    # Image projection\n",
    "    img = W_img(img)\n",
    "\n",
    "    # TEXT to IMAGE\n",
    "    t2i, _ = attn_t2i(txt, img, img)\n",
    "    txt2 = norm_t1(txt + t2i)\n",
    "    txt3 = norm_t2(txt2 + ff_t(txt2))\n",
    "\n",
    "    # IMAGE to TEXT\n",
    "    i2t, _ = attn_i2t(img, txt3, txt3)\n",
    "    img2 = norm_i1(img + i2t)\n",
    "    img3 = norm_i2(img2 + ff_i(img2))\n",
    "\n",
    "    # Pooling\n",
    "    txt_repr = txt3.mean(dim=1)\n",
    "    img_repr = img3.mean(dim=1)\n",
    "\n",
    "    fused = torch.cat([txt_repr, img_repr], dim=-1)  \n",
    "\n",
    "    logits = classifier(fused)\n",
    "\n",
    "    return logits, fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae628bb-6311-41bb-85a0-f88bdb3b3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(txt, img, y, batch_size=32):\n",
    "    model_loss = 0\n",
    "    N = len(y)\n",
    "    idx = torch.randperm(N)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = start + batch_size\n",
    "        b = idx[start:end]\n",
    "\n",
    "        txt_b = txt[b].to(device)\n",
    "        img_b = img[b].to(device)\n",
    "        y_b   = y[b].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = multimodal_forward(txt_b, img_b)\n",
    "        loss = criterion(logits, y_b)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model_loss += loss.item()\n",
    "\n",
    "    return model_loss / (N // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee23122-df1f-41b4-ac83-6eb9fd746570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(txt, img, y, batch_size=32):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    N = len(y)\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = start + batch_size\n",
    "            txt_b = txt[start:end].to(device)\n",
    "            img_b = img[start:end].to(device)\n",
    "            y_b   = y[start:end].to(device)\n",
    "\n",
    "            logits, _ = multimodal_forward(txt_b, img_b)\n",
    "            loss = criterion(logits, y_b)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == y_b).sum().item()\n",
    "\n",
    "    return total_loss / (N // batch_size), correct / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d48e48c-87e4-4be5-962c-ff2226906d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def get_best_state():\n",
    "    return {\n",
    "        \"W_img\": copy.deepcopy(W_img.state_dict()),\n",
    "        \"attn_t2i\": copy.deepcopy(attn_t2i.state_dict()),\n",
    "        \"attn_i2t\": copy.deepcopy(attn_i2t.state_dict()),\n",
    "        \"ff_t\": copy.deepcopy(ff_t.state_dict()),\n",
    "        \"ff_i\": copy.deepcopy(ff_i.state_dict()),\n",
    "        \"norm_t1\": copy.deepcopy(norm_t1.state_dict()),\n",
    "        \"norm_t2\": copy.deepcopy(norm_t2.state_dict()),\n",
    "        \"norm_i1\": copy.deepcopy(norm_i1.state_dict()),\n",
    "        \"norm_i2\": copy.deepcopy(norm_i2.state_dict()),\n",
    "        \"classifier\": copy.deepcopy(classifier.state_dict()),\n",
    "    }\n",
    "\n",
    "def load_best_state(state):\n",
    "    W_img.load_state_dict(state[\"W_img\"])\n",
    "    attn_t2i.load_state_dict(state[\"attn_t2i\"])\n",
    "    attn_i2t.load_state_dict(state[\"attn_i2t\"])\n",
    "    ff_t.load_state_dict(state[\"ff_t\"])\n",
    "    ff_i.load_state_dict(state[\"ff_i\"])\n",
    "    norm_t1.load_state_dict(state[\"norm_t1\"])\n",
    "    norm_t2.load_state_dict(state[\"norm_t2\"])\n",
    "    norm_i1.load_state_dict(state[\"norm_i1\"])\n",
    "    norm_i2.load_state_dict(state[\"norm_i2\"])\n",
    "    classifier.load_state_dict(state[\"classifier\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2de7050-4a39-467c-aff6-3a4542e4911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 77, 512) (30893, 50, 768) (24993, 50, 768)\n",
      "Post solo nel testo: set()\n",
      "Post solo nelle immagini: set()\n"
     ]
    }
   ],
   "source": [
    "text_npz_tr  = \"D:/dataset/clip_cross_attention_emb25/clip-vit-base-patch32_TOKENS_train_ids_y.npz\"\n",
    "image_npz_tr = \"D:/dataset/clip_cross_attention_emb25/clip_vit_b32_IMG_TOKENS_train_ids_y.npz\"\n",
    "\n",
    "t_tr = np.load(text_npz_tr, allow_pickle = True)\n",
    "i_tr = np.load(image_npz_tr, allow_pickle = True)\n",
    "\n",
    "ids_txt_tr = t_tr[\"ids\"]\n",
    "txt_tokens_tr = t_tr[\"embeddings\"]\n",
    "\n",
    "ids_img_tr = i_tr[\"ids\"]\n",
    "img_tokens_tr = i_tr[\"embeddings\"]\n",
    "\n",
    "img_tokens_post_tr = aggregate_image_tokens_per_post(ids_txt_tr, ids_img_tr, img_tokens_tr)\n",
    "print(txt_tokens_tr.shape, img_tokens_tr.shape, img_tokens_post_tr.shape)\n",
    "set_txt = set(t_tr[\"ids\"])\n",
    "set_img = set(i_tr[\"ids\"])\n",
    "\n",
    "print(\"Post solo nel testo:\", set_txt - set_img)\n",
    "print(\"Post solo nelle immagini:\", set_img - set_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90516f29-602f-4885-ac5a-ab84044d5b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", allow_pickle = True)\n",
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118da2ce-6295-438b-8c1a-5e0c4ff24945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del t_tr, i_tr, \n",
    "del img_tokens_tr, set_txt, set_img\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3106309a-8893-4dea-99da-38ea3c0e232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24993, 77, 512]) torch.Size([24993, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_tr = torch.tensor(txt_tokens_tr, dtype=torch.float32)\n",
    "img_tr = torch.tensor(img_tokens_post_tr, dtype=torch.float32)\n",
    "print(txt_tr.shape, img_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e12c2159-5803-41c8-bef7-2f022b3fb585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del txt_tokens_tr, img_tokens_post_tr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b73b17-2d97-40bb-b638-33d42f043ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 77, 512) (6665, 50, 768) (5000, 50, 768)\n",
      "Post solo nel testo: set()\n",
      "Post solo nelle immagini: set()\n"
     ]
    }
   ],
   "source": [
    "text_npz_va  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_validation_ids_y.npz\"\n",
    "image_npz_va = \"D:/dataset/clip_cross_attention_emb/clip_vit_b32_IMG_TOKENS_validation_ids_y.npz\"\n",
    "\n",
    "t_va = np.load(text_npz_va, allow_pickle = True)\n",
    "i_va = np.load(image_npz_va, allow_pickle = True)\n",
    "\n",
    "ids_txt_va = t_va[\"ids\"]\n",
    "txt_tokens_va = t_va[\"embeddings\"] \n",
    "\n",
    "ids_img_va = i_va[\"ids\"]\n",
    "img_tokens_va = i_va[\"embeddings\"]\n",
    "\n",
    "img_tokens_post_va = aggregate_image_tokens_per_post(ids_txt_va, ids_img_va, img_tokens_va)\n",
    "print(txt_tokens_va.shape, img_tokens_va.shape, img_tokens_post_va.shape)\n",
    "set_txt = set(t_va[\"ids\"])\n",
    "set_img = set(i_va[\"ids\"])\n",
    "\n",
    "print(\"Post solo nel testo:\", set_txt - set_img)\n",
    "print(\"Post solo nelle immagini:\", set_img - set_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d2c1a7-bbda-4ca0-8dac-d2d117679f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del t_va, i_va, \n",
    "del img_tokens_va, set_txt, set_img\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803460b8-aa7a-4005-932b-120dc7d75b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 77, 512]) torch.Size([5000, 50, 768]) (5000,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_va = torch.tensor(txt_tokens_va, dtype=torch.float32)\n",
    "img_va = torch.tensor(img_tokens_post_va, dtype=torch.float32)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", allow_pickle = True)\n",
    "print(txt_va.shape, img_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcab480-f2b1-499c-a548-06d4418762e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del txt_tokens_va, img_tokens_post_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e0cf55-67bb-4348-835d-c1f83f3e53cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5573161-1cca-4f27-8c47-f5b19261f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "y_va = torch.tensor(y_va, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b1b502-f2c3-4902-a284-611516f192cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_loss: 1.6389 | val_loss: 1.6689 | val_acc: 0.207\n",
      "Epoch 2/10 - train_loss: 1.6002 | val_loss: 1.6823 | val_acc: 0.187\n",
      "Epoch 3/10 - train_loss: 1.5642 | val_loss: 1.6725 | val_acc: 0.185\n",
      "Epoch 4/10 - train_loss: 1.4788 | val_loss: 1.7656 | val_acc: 0.205\n",
      "Early stopping: val_loss non migliora da 3 epoche. Best val_loss = 1.6689\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "patience = 3\n",
    "min_delta = 1e-4\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "bad_epochs = 0\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    train_loss = train_one_epoch(txt_tr, img_tr, y_tr, batch_size=32)\n",
    "    val_loss, val_acc = evaluate(txt_va, img_va, y_va, batch_size=32)\n",
    "\n",
    "    print(f\"Epoch {ep}/{epochs} - train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_loss < best_val - min_delta:\n",
    "        best_val = val_loss\n",
    "        best_state = get_best_state()\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(f\"Early stopping: val_loss non migliora da {patience} epoche. Best val_loss = {best_val:.4f}\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    load_best_state(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9adcb769-90d3-433c-ba53-6752227df882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fused_features(txt, img, batch_size=32):\n",
    "    fused_all = []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(txt), batch_size):\n",
    "            end = start + batch_size\n",
    "            txt_b = txt[start:end].to(device)\n",
    "            img_b = img[start:end].to(device)\n",
    "\n",
    "            _, fused = multimodal_forward(txt_b, img_b)\n",
    "            fused_all.append(fused.cpu().numpy())\n",
    "\n",
    "    return np.vstack(fused_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d6db007-e088-4038-b385-d826b574ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the optimization prosses, the pytorch modules have been updated to the correct version, and uses it \n",
    "fused_train = generate_fused_features(txt_tr, img_tr)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_train_finetuned.npy\", fused_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f68d8aa1-6bef-4c3f-9ce3-e099822d5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24993, 1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7410964-1779-48f6-a165-30fd6081456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_val = generate_fused_features(txt_va, img_va)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_val_finetuned.npy\", fused_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "766bf88c-48fa-4a04-875f-251bad7dd4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1024)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd612d-109d-42d3-9faa-ff68db25d1a1",
   "metadata": {},
   "source": [
    "# CROSS-ATTENTION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dd4a8f8-711e-44bd-902a-6b0b49a19fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del txt_tr, img_tr, txt_va, img_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9550c1c-6790-4870-9f9e-4bde9b23d299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_npz_te  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_test_ids_y.npz\"\n",
    "image_npz_te = \"D:/dataset/clip_cross_attention_emb/clip_vit_b32_IMG_TOKENS_test_ids_y.npz\"\n",
    "\n",
    "t_te = np.load(text_npz_te, allow_pickle = True)\n",
    "i_te = np.load(image_npz_te, allow_pickle = True)\n",
    "\n",
    "ids_txt_te = t_te[\"ids\"]\n",
    "txt_tokens_te = t_te[\"embeddings\"]     \n",
    "\n",
    "ids_img_te = i_te[\"ids\"]\n",
    "img_tokens_te = i_te[\"embeddings\"] \n",
    "\n",
    "del t_te, i_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24a3b8bf-c4de-4a76-bce4-35b294bcd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tokens_post_te = aggregate_image_tokens_per_post(ids_txt_te, ids_img_te, img_tokens_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3323cdcc-643d-41e3-b404-7ac9094cf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 77, 512) (5000, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "print(txt_tokens_te.shape, img_tokens_post_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ecc94bf-ad7b-4b28-a97a-73df52cd7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te = np.load(\"D:/dataset/clip_cross_attention_emb/y_te_5.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffad56e-aab1-4fb5-8b01-35b2d5f820e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_te = torch.tensor(txt_tokens_te, dtype=torch.float32)\n",
    "img_te = torch.tensor(img_tokens_post_te, dtype=torch.float32)\n",
    "y_te = torch.tensor(y_te, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "101e87a9-743e-4a8d-b1a8-20a3a3da68a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4]\n",
      "shape: torch.Size([5000])\n",
      "torch.Size([5000, 77, 512]) torch.Size([5000, 50, 768]) torch.Size([5000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unique classes:\", np.unique(y_te))\n",
    "print(\"shape:\", y_te.shape)\n",
    "print(txt_te.shape, img_te.shape, y_te.shape)\n",
    "\n",
    "del txt_tokens_te, img_tokens_post_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f279f7f-7897-4ecf-9cb9-64eb3f496bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_test  = generate_fused_features(txt_te, img_te)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_test_finetuned.npy\", fused_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "518bb2d0-3db6-46c9-b7df-27b1d573f5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9b020-3e8a-476d-9e2a-11613ed3d068",
   "metadata": {},
   "source": [
    "# FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07912f87-db40-427b-b20c-935aa6461079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24993"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_txt_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e081050-2e33-48e6-904e-13ec97aaa9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA TRAIN\n",
    "meta_train_final = pd.read_csv(\"D:/dataset/meta_classification/meta_train_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_train_final = meta_train_final[meta_train_final[\"post_id\"].isin(ids_txt_tr)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_train_final = meta_train_final.set_index(\"post_id\").loc[ids_txt_tr].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_train_final[\"post_id\"].to_numpy() == ids_txt_tr).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_train = meta_train_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_train = X_meta_train.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_train = np.load(\"D:/dataset/clip_cross_attention_emb/fused_train_finetuned.npy\", allow_pickle = True)\n",
    "X_tr = np.hstack([fused_train, X_meta_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f174c0c6-d131-4c8e-b7f0-bb1896fee1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24993, 1052)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73ef75f1-dd8b-463b-9c31-b28a088f5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_train_ft5.npy\", X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89dd66db-758a-4781-84e8-7a8caf3af272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA VALIDATION\n",
    "meta_val_final = pd.read_csv(\"D:/dataset/meta_classification/meta_val_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_val_final = meta_val_final[meta_val_final[\"post_id\"].isin(ids_txt_va)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_val_final = meta_val_final.set_index(\"post_id\").loc[ids_txt_va].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_val_final[\"post_id\"].to_numpy() == ids_txt_va).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_val = meta_val_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_val = X_meta_val.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_val = np.load(\"D:/dataset/clip_cross_attention_emb/fused_val_finetuned.npy\", allow_pickle = True)\n",
    "X_va = np.hstack([fused_val, X_meta_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6840d809-89ac-45d2-945f-7523c74579e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1052)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "801a8570-b22f-4944-abda-d4ab1b0c31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_val_ft5.npy\", X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f31fe56-2aba-4124-8539-26c3c57fef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA TEST\n",
    "meta_test_final = pd.read_csv(\"D:/dataset/meta_classification/meta_test_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_test_final = meta_test_final[meta_test_final[\"post_id\"].isin(ids_txt_te)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_test_final = meta_test_final.set_index(\"post_id\").loc[ids_txt_te].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_test_final[\"post_id\"].to_numpy() == ids_txt_te).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_test = meta_test_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_test = X_meta_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_test = np.load(\"D:/dataset/clip_cross_attention_emb/fused_test_finetuned.npy\", allow_pickle = True)\n",
    "X_te = np.hstack([fused_test, X_meta_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8a88adc-50c9-40f1-bb89-cf1ad4cea23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1052)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5583ef8-c353-467b-9abb-c7903ef13ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_test_ft5.npy\", X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f0328-750f-47ed-a0eb-d280ea516fc6",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "186201a3-4a5f-4e49-8004-f7148370b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train_ft5.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val_ft5.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a70bd1df-da95-4000-9862-8cc972df0674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 1052) (24993,) (5000, 1052) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51453ef1-a190-475f-9094-14dafb7e0429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c007cc7a-45f1-44d6-a5f7-fbb5837776ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 4, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de7b794d-0388-4b8c-84d2-de386a0212e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3042749 , -0.07988649,  0.18901303, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.11272   , -0.25869903,  0.4618878 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.80691516, -0.40040484, -0.19250205, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.8263543 , -0.32952374,  0.0795309 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.94892555, -0.8995705 , -0.93573654, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.508135  , -0.97229767, -0.6595417 , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57f0599-951e-4aa2-81a5-bb5b3d124833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.18562670582595545 | accuracy (val): 0.2058\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.18370460551107254 | accuracy (val): 0.193\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.1786325284442139 | accuracy (val): 0.191\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.18989751165887273 | accuracy (val): 0.194\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.17252566575132527 | accuracy (val): 0.1856\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.18551386517307436 | accuracy (val): 0.1916\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.1757454917038634 | accuracy (val): 0.1908\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.18596168038440036 | accuracy (val): 0.193\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "Validation macro-F1: 0.18989751165887273\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "3  0.00010     balanced      0.189898        0.1940\n",
      "7  0.01000     balanced      0.185962        0.1930\n",
      "0  0.00001         None      0.185627        0.2058\n",
      "5  0.00100     balanced      0.185514        0.1916\n",
      "1  0.00001     balanced      0.183705        0.1930\n",
      "2  0.00010         None      0.178633        0.1910\n",
      "6  0.01000         None      0.175745        0.1908\n",
      "4  0.00100         None      0.172526        0.1856\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f096a815-c7d8-474a-8eae-1badc53f2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.1897 | accuracy (val): 0.1948\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.1897 | accuracy (val): 0.1948\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.1897 | accuracy (val): 0.1948\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.1895 | accuracy (val): 0.1946\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.18969039937255408\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09      0.189690        0.1948\n",
      "1   1.000000e-08      0.189690        0.1948\n",
      "2   1.000000e-07      0.189690        0.1948\n",
      "3   1.000000e-06      0.189529        0.1946\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03835789-cbd5-4c3d-9925-fce5b3314813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1599 | accuracy (val): 0.1902\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1624 | accuracy (val): 0.1982\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1498 | accuracy (val): 0.1878\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1645 | accuracy (val): 0.1994\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1542 | accuracy (val): 0.1916\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1545 | accuracy (val): 0.1964\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1580 | accuracy (val): 0.1916\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1493 | accuracy (val): 0.1908\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1506 | accuracy (val): 0.1958\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1558 | accuracy (val): 0.1938\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1552 | accuracy (val): 0.1988\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1536 | accuracy (val): 0.1988\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1720 | accuracy (val): 0.1938\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1657 | accuracy (val): 0.1936\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1523 | accuracy (val): 0.1846\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1747 | accuracy (val): 0.2016\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1656 | accuracy (val): 0.1966\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1644 | accuracy (val): 0.2012\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1638 | accuracy (val): 0.1902\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1580 | accuracy (val): 0.1902\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1541 | accuracy (val): 0.1906\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1631 | accuracy (val): 0.1860\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1545 | accuracy (val): 0.1858\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1480 | accuracy (val): 0.1854\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1745 | accuracy (val): 0.1920\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1705 | accuracy (val): 0.1914\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1651 | accuracy (val): 0.1898\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1649 | accuracy (val): 0.1858\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1613 | accuracy (val): 0.1870\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1586 | accuracy (val): 0.1898\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1706 | accuracy (val): 0.1896\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1675 | accuracy (val): 0.1922\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1608 | accuracy (val): 0.1932\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1731 | accuracy (val): 0.1944\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1576 | accuracy (val): 0.1852\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1516 | accuracy (val): 0.1840\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "Validation macro-F1: 0.17469589988280493\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "15            30         10                 5         0.05      0.174696   \n",
      "24            30         12                 2         0.05      0.174544   \n",
      "33            30         12                 5         sqrt      0.173062   \n",
      "12            30         10                 2         0.05      0.171970   \n",
      "30            30         12                 2         sqrt      0.170570   \n",
      "25            50         12                 2         0.05      0.170450   \n",
      "31            50         12                 2         sqrt      0.167542   \n",
      "13            50         10                 2         0.05      0.165665   \n",
      "16            50         10                 5         0.05      0.165609   \n",
      "26            80         12                 2         0.05      0.165105   \n",
      "27            30         12                 5         0.05      0.164925   \n",
      "3             30          8                 5         0.05      0.164514   \n",
      "17            80         10                 5         0.05      0.164388   \n",
      "18            30         10                 2         sqrt      0.163764   \n",
      "21            30         10                 5         sqrt      0.163091   \n",
      "1             50          8                 2         0.05      0.162383   \n",
      "28            50         12                 5         0.05      0.161312   \n",
      "32            80         12                 2         sqrt      0.160801   \n",
      "0             30          8                 2         0.05      0.159909   \n",
      "29            80         12                 5         0.05      0.158585   \n",
      "19            50         10                 2         sqrt      0.158028   \n",
      "6             30          8                 2         sqrt      0.157988   \n",
      "34            50         12                 5         sqrt      0.157593   \n",
      "9             30          8                 5         sqrt      0.155844   \n",
      "10            50          8                 5         sqrt      0.155223   \n",
      "5             80          8                 5         0.05      0.154546   \n",
      "22            50         10                 5         sqrt      0.154515   \n",
      "4             50          8                 5         0.05      0.154220   \n",
      "20            80         10                 2         sqrt      0.154082   \n",
      "11            80          8                 5         sqrt      0.153594   \n",
      "14            80         10                 2         0.05      0.152273   \n",
      "35            80         12                 5         sqrt      0.151552   \n",
      "8             80          8                 2         sqrt      0.150649   \n",
      "2             80          8                 2         0.05      0.149751   \n",
      "7             50          8                 2         sqrt      0.149339   \n",
      "23            80         10                 5         sqrt      0.147967   \n",
      "\n",
      "    val_accuracy  \n",
      "15        0.2016  \n",
      "24        0.1920  \n",
      "33        0.1944  \n",
      "12        0.1938  \n",
      "30        0.1896  \n",
      "25        0.1914  \n",
      "31        0.1922  \n",
      "13        0.1936  \n",
      "16        0.1966  \n",
      "26        0.1898  \n",
      "27        0.1858  \n",
      "3         0.1994  \n",
      "17        0.2012  \n",
      "18        0.1902  \n",
      "21        0.1860  \n",
      "1         0.1982  \n",
      "28        0.1870  \n",
      "32        0.1932  \n",
      "0         0.1902  \n",
      "29        0.1898  \n",
      "19        0.1902  \n",
      "6         0.1916  \n",
      "34        0.1852  \n",
      "9         0.1938  \n",
      "10        0.1988  \n",
      "5         0.1964  \n",
      "22        0.1858  \n",
      "4         0.1916  \n",
      "20        0.1906  \n",
      "11        0.1988  \n",
      "14        0.1846  \n",
      "35        0.1840  \n",
      "8         0.1958  \n",
      "2         0.1878  \n",
      "7         0.1908  \n",
      "23        0.1854  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cac7132a-1670-4f17-a36d-1530ede2bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1822 | accuracy (val): 0.1964\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1818 | accuracy (val): 0.1936\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1896 | accuracy (val): 0.1992\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1889 | accuracy (val): 0.1978\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1871 | accuracy (val): 0.2006\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1857 | accuracy (val): 0.1972\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1904 | accuracy (val): 0.2004\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1851 | accuracy (val): 0.1940\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.19042472305834537\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "6               0.5      1            0.1          6           100   \n",
      "2               0.5      0            0.1          6           100   \n",
      "3               0.5      0            0.1          6           150   \n",
      "4               0.5      1            0.1          4           100   \n",
      "5               0.5      1            0.1          4           150   \n",
      "7               0.5      1            0.1          6           150   \n",
      "0               0.5      0            0.1          4           100   \n",
      "1               0.5      0            0.1          4           150   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "6           1        0.8      0.190425        0.2004  \n",
      "2           1        0.8      0.189576        0.1992  \n",
      "3           1        0.8      0.188857        0.1978  \n",
      "4           1        0.8      0.187088        0.2006  \n",
      "5           1        0.8      0.185745        0.1972  \n",
      "7           1        0.8      0.185134        0.1940  \n",
      "0           1        0.8      0.182185        0.1964  \n",
      "1           1        0.8      0.181755        0.1936  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5], \n",
    "    \"gamma\": [0, 1], \n",
    "    \"reg_lambda\": [1], \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bc0bd-e8e1-4f2d-a598-2fe100d87ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE ON TEST 5 CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28707e29-ead5-475a-82b8-f2c8e053cd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train_ft5.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val_ft5.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/clip_cross_attention_emb/X_test_ft5.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/clip_cross_attention_emb/y_te_5.npy\", allow_pickle = True)\n",
    "\n",
    "X_trva = np.concatenate((X_tr, X_va), axis = 0).astype(np.float32)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805ff20a-f37d-4d13-9cd1-0657c0e3dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2497 | accuracy (test): 0.2594\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 0.0001,\n",
    "        average = True,\n",
    "        class_weight = 'balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_trva, y_trva)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613cc18c-8219-4fe0-aae3-3f65c03b5675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (train): 0.2221 | accuracy (train): 0.2250\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=10, max_features=0.05, min_samples_leaf=5,\n",
      "                       n_estimators=30, n_jobs=-1, random_state=42)\n",
      "macro-F1 (train): 0.2162 | accuracy (train): 0.2242\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=1,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (train): 0.2806 | accuracy (train): 0.2820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_trva_enc = le.fit_transform(y_trva)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=10, max_features=0.05, min_samples_leaf=5, n_estimators=30, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 1, learning_rate = 0.1, max_depth= 6, n_estimators= 100, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_trva_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_trva, y_trva_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_trva, y_trva)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (train): {macro_f1:.4f} | accuracy (train): {acc:.4f}\")\n",
    "\n",
    "del X_trva, X_te, y_trva, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7861968-84cd-4c18-b3c8-1ed4a210f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
