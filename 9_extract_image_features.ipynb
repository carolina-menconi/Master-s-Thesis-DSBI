{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97cedf18-5eb5-4b43-bd7e-a7fb2c901c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os, sys, gc, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7a214e-e5f1-40ab-a205-548ff08584a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "DB_PATH       = r\"D:/db/meta.duckdb\"\n",
    "CLEAN_IMG_DIR = r\"D:/dataset/images_224_rgb\"\n",
    "OUT_DIR       = r\"D:/dataset/img_features\"\n",
    "\n",
    "# DuckDB tables\n",
    "METADATA_TABLE   = \"metadata1718_ready\" # metadata \n",
    "MANIFEST_TABLE   = \"images_manifest1718_clean\" # manifest\n",
    "TRAIN_POSTS_TBL  = \"train_balanced\" # undersampled metadata for training\n",
    "\n",
    "# Key column names\n",
    "POST_ID_COL_METADATA  = \"post_id\" # metadata post id\n",
    "POST_ID_COL_MANIFEST  = \"post_id\" # manifest post id\n",
    "IMG_NAME_COL_MANIFEST = \"full_image_file\" # images filename column in the manifest\n",
    "POST_ID_COL_TRAIN     = \"post_id\" # undersampled metadata for training post id\n",
    "\n",
    "INCLUDE_LBP  = True\n",
    "INCLUDE_GLCM = True\n",
    "\n",
    "POOLING = \"mean\"\n",
    "SAVE_FLOAT16 = True  \n",
    "BATCH_IMG = 8192 # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12f13cd2-db46-4a3f-84ba-a336d23aa5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Path presente: D:/db\n",
      "[OK] Path presente: D:/dataset/images_224_rgb\n",
      "[OK] Path presente: D:/dataset/img_features\n",
      "[OK] DuckDB trovato: D:/db/meta.duckdb\n",
      "[OK] metadata1718_ready con colonne richieste\n",
      "[OK] images_manifest1718_clean con colonne richieste\n",
      "[OK] train_balanced con colonne richieste\n",
      "\n",
      " Setup ok\n"
     ]
    }
   ],
   "source": [
    "# Set up checks\n",
    "def check_setup() -> None:\n",
    "    ok = True\n",
    "    for p in [os.path.dirname(DB_PATH), CLEAN_IMG_DIR, OUT_DIR]:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[MISSING] Non existing path: {p}\")\n",
    "            ok = False\n",
    "        else:\n",
    "            print(f\"[OK] Path present: {p}\")\n",
    "\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        print(f\"[MISSING] DuckDB not found: {DB_PATH}\")\n",
    "        ok = False\n",
    "    else:\n",
    "        print(f\"[OK] DuckDB found: {DB_PATH}\")\n",
    "\n",
    "    # check tables and columns in duckdb tables\n",
    "    try:\n",
    "        con = duckdb.connect(DB_PATH, read_only=True)\n",
    "        def table_has_cols(tbl, cols):\n",
    "            info = con.execute(f\"PRAGMA table_info('{tbl}')\").fetchdf()\n",
    "            have = set(info['name'].str.lower().tolist())\n",
    "            need = set([c.lower() for c in cols])\n",
    "            missing = [c for c in need if c not in have]\n",
    "            return missing\n",
    "\n",
    "        miss_meta = table_has_cols(METADATA_TABLE, [POST_ID_COL_METADATA, \"split\"])\n",
    "        miss_man  = table_has_cols(MANIFEST_TABLE, [POST_ID_COL_MANIFEST, IMG_NAME_COL_MANIFEST])\n",
    "        miss_train= table_has_cols(TRAIN_POSTS_TBL, [POST_ID_COL_TRAIN])\n",
    "\n",
    "        if miss_meta:\n",
    "            print(f\"[MISSING] Missing columns in {METADATA_TABLE}: {miss_meta}\"); ok = False\n",
    "        else:\n",
    "            print(f\"[OK] {METADATA_TABLE} with requested columns\")\n",
    "\n",
    "        if miss_man:\n",
    "            print(f\"[MISSING] Missing columns in {MANIFEST_TABLE}: {miss_man}\"); ok = False\n",
    "        else:\n",
    "            print(f\"[OK] {MANIFEST_TABLE} with requested columns\")\n",
    "\n",
    "        if miss_train:\n",
    "            print(f\"[MISSING] Missing columns in {TRAIN_POSTS_TBL}: {miss_train}\"); ok = False\n",
    "        else:\n",
    "            print(f\"[OK] {TRAIN_POSTS_TBL} with requested columns\")\n",
    "\n",
    "        con.close()\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Impossible to connect/read the duckdb schema:\", e)\n",
    "        ok = False\n",
    "\n",
    "    if not ok:\n",
    "        print(\"\\n Review the missing and re-run the cell\")\n",
    "    else:\n",
    "        print(\"\\n Setup ok\")\n",
    "\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71c440-52e6-4112-9cda-eae194ec8e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the set of images per split\n",
    "CLEAN_FILES_TABLE   = \"clean_files\"\n",
    "CLEAN_FILENAME_COL  = \"filename\"\n",
    "\n",
    "# Training images view\n",
    "VIEW_TRAIN_IMAGES            = \"images_train\"\n",
    "VIEW_TRAIN_POST_ID_COL       = \"post_id\"\n",
    "VIEW_TRAIN_IMG_NAME_COL      = \"full_image_file\"\n",
    "\n",
    "def _relation_exists(con, name: str) -> bool:\n",
    "    try:\n",
    "        q = f\"SELECT table_name FROM information_schema.tables WHERE lower(table_name)=lower('{name}')\"\n",
    "        return len(con.execute(q).fetchall()) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_images_for_split(split: str) -> pd.DataFrame:\n",
    "\n",
    "    con = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "    if split == \"train\":\n",
    "        # If the training set of images exists it is used\n",
    "        if _relation_exists(con, VIEW_TRAIN_IMAGES):\n",
    "            q = f\"\"\"\n",
    "            SELECT\n",
    "              {VIEW_TRAIN_POST_ID_COL} AS post_id,\n",
    "              {VIEW_TRAIN_IMG_NAME_COL} AS image_rel\n",
    "            FROM {VIEW_TRAIN_IMAGES}\n",
    "            \"\"\"\n",
    "            df = con.execute(q).fetchdf()\n",
    "\n",
    "        else:\n",
    "            # otherwise the set of images to train it is defined\n",
    "            # train = (train_balanced) + (manifest) + (clean_files)\n",
    "            q = f\"\"\"\n",
    "            WITH tr AS (\n",
    "              SELECT {POST_ID_COL_TRAIN} AS post_id\n",
    "              FROM {TRAIN_POSTS_TBL}\n",
    "            ),\n",
    "            join_im AS (\n",
    "              SELECT tr.post_id,\n",
    "                     im.{IMG_NAME_COL_MANIFEST} AS image_rel\n",
    "              FROM tr\n",
    "              JOIN {MANIFEST_TABLE} AS im\n",
    "                ON im.{POST_ID_COL_MANIFEST} = tr.post_id\n",
    "            ),\n",
    "            ok AS (\n",
    "              SELECT j.post_id, j.image_rel\n",
    "              FROM join_im j\n",
    "              JOIN {CLEAN_FILES_TABLE} cl\n",
    "                ON cl.{CLEAN_FILENAME_COL} = j.image_rel\n",
    "            )\n",
    "            SELECT * FROM ok\n",
    "            \"\"\"\n",
    "            df = con.execute(q).fetchdf()\n",
    "\n",
    "    else:\n",
    "        # validation/test: select the metadata posts for the split and the retrieve the images by merging the manifest and the clean files\n",
    "        q = f\"\"\"\n",
    "        WITH ids AS (\n",
    "          SELECT {POST_ID_COL_METADATA} AS post_id\n",
    "          FROM {METADATA_TABLE}\n",
    "          WHERE split = '{split}'\n",
    "        ),\n",
    "        join_im AS (\n",
    "          SELECT i.post_id, im.{IMG_NAME_COL_MANIFEST} AS image_rel\n",
    "          FROM ids i\n",
    "          JOIN {MANIFEST_TABLE} im\n",
    "            ON im.{POST_ID_COL_MANIFEST} = i.post_id\n",
    "        ),\n",
    "        ok AS (\n",
    "          SELECT j.post_id, j.image_rel\n",
    "          FROM join_im j\n",
    "          JOIN {CLEAN_FILES_TABLE} cl\n",
    "            ON cl.{CLEAN_FILENAME_COL} = j.image_rel\n",
    "        )\n",
    "        SELECT * FROM ok\n",
    "        \"\"\"\n",
    "        df = con.execute(q).fetchdf()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # Build absolute path and additional cleaning if something went wrong\n",
    "    df[\"image_rel\"]  = df[\"image_rel\"].astype(str)\n",
    "    df[\"post_id\"]    = df[\"post_id\"].astype(str)\n",
    "    df[\"image_path\"] = df[\"image_rel\"].map(lambda t: os.path.join(CLEAN_IMG_DIR, t))\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"post_id\",\"image_path\"])\n",
    "    df = df[df[\"image_path\"].map(os.path.exists)].reset_index(drop=True)\n",
    "\n",
    "    return df[[\"post_id\",\"image_path\"]]\n",
    "\n",
    "# Some sanity checks\n",
    "for sp in (\"train\",\"validation\",\"test\"):\n",
    "    try:\n",
    "        tmp = get_images_for_split(sp)\n",
    "        print(f\"[{sp}] righe: {len(tmp):,} | post unici: {tmp['post_id'].nunique():,}\")\n",
    "        print(tmp.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"[{sp}] errore ->\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db793ff6-f408-45f9-8de8-a8d9467d01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim. feature con i flag correnti: 152\n"
     ]
    }
   ],
   "source": [
    "# Features extracted for each image\n",
    "def colorfulness_hasler(img_bgr: np.ndarray) -> float:\n",
    "    B, G, R = cv2.split(img_bgr.astype(np.float32))\n",
    "    rg = np.abs(R - G)\n",
    "    yb = np.abs(0.5*(R + G) - B)\n",
    "    return float(np.sqrt(rg.std()**2 + yb.std()**2) + 0.3*np.sqrt(rg.mean()**2 + yb.mean()**2))\n",
    "\n",
    "def hsv_stats_and_hists(img_bgr: np.ndarray, bins: int = 16):\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    feats = [\n",
    "        float(h.mean()), float(h.std()),\n",
    "        float(s.mean()), float(s.std()),\n",
    "        float(v.mean()), float(v.std()),\n",
    "    ]\n",
    "    for ch in (h, s, v):\n",
    "        hist = cv2.calcHist([ch],[0],None,[bins],[0,256]).ravel()\n",
    "        hist = hist / (hist.sum() + 1e-8)\n",
    "        feats += hist.astype(np.float32).tolist()\n",
    "    return feats  \n",
    "\n",
    "def gray_histo_entropy_sharp_edges(img_bgr: np.ndarray, bins: int = 16):\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    # gray normalized histogram\n",
    "    hist = cv2.calcHist([gray],[0],None,[bins],[0,256]).ravel()\n",
    "    hist = hist / (hist.sum() + 1e-8)\n",
    "    ent = float(shannon_entropy(gray))\n",
    "    lap_var = float(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_density = float((edges > 0).mean())\n",
    "    return hist.astype(np.float32).tolist(), ent, lap_var, edge_density, gray\n",
    "\n",
    "def feat_lbp59(gray: np.ndarray) -> np.ndarray:\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp, bins=np.arange(0, 60), range=(0,59), density=True)\n",
    "    return hist.astype(np.float32)  \n",
    "\n",
    "def feat_glcm16(gray: np.ndarray) -> np.ndarray:\n",
    "    g = gray if gray.dtype == np.uint8 else cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    gl = graycomatrix(g, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                      levels=256, symmetric=True, normed=True)\n",
    "    vals = []\n",
    "    for p in [\"contrast\",\"homogeneity\",\"energy\",\"correlation\"]:\n",
    "        vals.extend(graycoprops(gl, p).ravel().tolist())  \n",
    "    return np.array(vals, dtype=np.float32)  \n",
    "\n",
    "def feature_columns() -> list:\n",
    "    cols = []\n",
    "    cols += [\"h_mean\",\"h_std\",\"s_mean\",\"s_std\",\"v_mean\",\"v_std\"]\n",
    "    cols += [f\"h_hist_{i:02d}\" for i in range(16)]\n",
    "    cols += [f\"s_hist_{i:02d}\" for i in range(16)]\n",
    "    cols += [f\"v_hist_{i:02d}\" for i in range(16)]\n",
    "    cols += [f\"gray_hist_{i:02d}\" for i in range(16)]\n",
    "    cols += [\"laplacian_var\",\"edge_density\",\"entropy_gray\",\"colorfulness\"]\n",
    "    if INCLUDE_LBP:\n",
    "        cols += [f\"lbp_u59_{i:02d}\" for i in range(59)]\n",
    "    if INCLUDE_GLCM:\n",
    "        for prop in (\"contrast\",\"homogeneity\",\"energy\",\"correlation\"):\n",
    "            for ang in (\"0\",\"45\",\"90\",\"135\"):\n",
    "                cols.append(f\"glcm_{prop}_{ang}\")\n",
    "    return cols\n",
    "\n",
    "def extract_handcrafted_one(img_path: str) -> np.ndarray | None:\n",
    "\n",
    "    p = os.path.normpath(img_path)\n",
    "    img_bgr = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        return None\n",
    "        \n",
    "    f_hsv = hsv_stats_and_hists(img_bgr, bins=16)\n",
    "    histg, ent, lap_var, edge_den, gray = gray_histo_entropy_sharp_edges(img_bgr, bins=16)\n",
    "    cf = colorfulness_hasler(img_bgr) # 1\n",
    "    feats = f_hsv + histg + [lap_var, edge_den, ent, cf]\n",
    "\n",
    "    if INCLUDE_LBP:\n",
    "        feats.extend(feat_lbp59(gray).tolist())\n",
    "    if INCLUDE_GLCM:\n",
    "        feats.extend(feat_glcm16(gray).tolist())\n",
    "    \n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "print(\"Total number of features:\", len(feature_columns()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34a089fa-99da-4419-9a7a-5f76b9d9c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features in batches and shards, with the option to resume the work if something goes wrong\n",
    "\n",
    "def compute_post_features_from_df(df_post_img: pd.DataFrame,\n",
    "                                  batch_size: int = BATCH_IMG,\n",
    "                                  cast_float16: bool = True,\n",
    "                                  verbose: bool = True) -> pd.DataFrame:\n",
    "    img_paths = df_post_img[\"image_path\"].tolist()\n",
    "    img_post  = df_post_img[\"post_id\"].tolist()\n",
    "\n",
    "    agg_mean: dict[str, np.ndarray] = {}\n",
    "    agg_cnt:  dict[str, int]        = {}\n",
    "    D: int | None = None\n",
    "    n_img = len(img_paths)\n",
    "    n_err = 0\n",
    "\n",
    "    it = range(0, n_img, batch_size)\n",
    "    if verbose:\n",
    "        it = tqdm(it, total=math.ceil(n_img / batch_size), desc=\"batches\")\n",
    "\n",
    "    for i in it:\n",
    "        for p, pid in zip(img_paths[i:i+batch_size], img_post[i:i+batch_size]):\n",
    "            vec = extract_handcrafted_one(p)\n",
    "            if vec is None:\n",
    "                n_err += 1\n",
    "                continue\n",
    "            if D is None:\n",
    "                D = int(vec.shape[0])\n",
    "            if pid not in agg_mean:\n",
    "                agg_mean[pid] = vec.astype(np.float32, copy=True)\n",
    "                agg_cnt[pid]  = 1\n",
    "            else:\n",
    "                c = agg_cnt[pid] + 1\n",
    "                agg_mean[pid] += (vec - agg_mean[pid]) / c  # running mean\n",
    "                agg_cnt[pid]   = c\n",
    "\n",
    "    if D is None:\n",
    "        raise RuntimeError(\"No valid image: impossible to determine feature dimension\")\n",
    "\n",
    "    post_ids = list(agg_mean.keys())\n",
    "    mat = np.vstack([agg_mean[pid] for pid in post_ids]).astype(np.float32)\n",
    "    if cast_float16:\n",
    "        try:\n",
    "            mat = mat.astype(np.float16)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    cols = feature_columns()\n",
    "    if len(cols) != mat.shape[1]:\n",
    "        cols = [f\"f_{i:03d}\" for i in range(mat.shape[1])]\n",
    "    out = pd.DataFrame(mat, columns=cols)\n",
    "    out.insert(0, \"post_id\", post_ids)\n",
    "    out = out.sort_values(\"post_id\").reset_index(drop=True)\n",
    "    if verbose and n_err:\n",
    "        print(f\"Warning: {n_err} unreadable images (ignored)\")\n",
    "    return out\n",
    "\n",
    "def compute_and_save_split_sharded_resume(split: str,\n",
    "                                          n_shards: int = 8,\n",
    "                                          batch_size: int = 2048,\n",
    "                                          cast_float16: bool = True,\n",
    "                                          out_dir: str = OUT_DIR,\n",
    "                                          force: bool = False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df_all = get_images_for_split(split)\n",
    "    if df_all.empty:\n",
    "        raise RuntimeError(f\"No image for split={split}\")\n",
    "\n",
    "    # Deterministic partitioning per post_id\n",
    "    h = pd.util.hash_pandas_object(df_all[\"post_id\"], index=False).astype(\"uint64\")\n",
    "    df_all = df_all.assign(_shard=(h % n_shards).astype(int))\n",
    "\n",
    "    part_paths = []\n",
    "    for k in range(n_shards):\n",
    "        part_path = os.path.join(out_dir, f\"{split}_handcrafted_post.part{k}.parquet\")\n",
    "        if os.path.exists(part_path) and not force:\n",
    "            print(f\"[{split}] shard {k+1}/{n_shards} già presente → skip\")\n",
    "            part_paths.append(part_path)\n",
    "            continue\n",
    "\n",
    "        df_k = df_all[df_all[\"_shard\"] == k][[\"post_id\",\"image_path\"]].reset_index(drop=True)\n",
    "        if df_k.empty:\n",
    "            print(f\"[{split}] shard {k+1}/{n_shards}: vuoto → skip\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[{split}] shard {k+1}/{n_shards}: images={len(df_k):,}, post≈{df_k['post_id'].nunique():,}\")\n",
    "        feats_k = compute_post_features_from_df(df_k, batch_size=batch_size,\n",
    "                                                cast_float16=cast_float16, verbose=True)\n",
    "        try:\n",
    "            feats_k.to_parquet(part_path, index=False)\n",
    "        except Exception:\n",
    "            num_cols = [c for c in feats_k.columns if c != \"post_id\"]\n",
    "            feats_k[num_cols] = feats_k[num_cols].astype(\"float32\")\n",
    "            feats_k.to_parquet(part_path, index=False, compression=\"zstd\")\n",
    "        part_paths.append(part_path)\n",
    "        print(f\" Saved: {part_path}\")\n",
    "\n",
    "    # Merge all available shards\n",
    "    parts = [p for p in part_paths if os.path.exists(p)]\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"Nessuno shard disponibile da mergiare.\")\n",
    "    frames = [pd.read_parquet(p) for p in parts]\n",
    "    full = pd.concat(frames, ignore_index=True)\n",
    "    full = full.drop_duplicates(subset=[\"post_id\"], keep=\"last\").sort_values(\"post_id\").reset_index(drop=True)\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{split}_handcrafted_post.parquet\")\n",
    "    try:\n",
    "        full.to_parquet(out_path, index=False)\n",
    "    except Exception:\n",
    "        num_cols = [c for c in full.columns if c != \"post_id\"]\n",
    "        full[num_cols] = full[num_cols].astype(\"float32\")\n",
    "        full.to_parquet(out_path, index=False, compression=\"zstd\")\n",
    "\n",
    "    print(f\"Merge: {out_path}  ({full.shape[0]} post, {full.shape[1]-1} dim)\")\n",
    "    print(\"The function can be relaunched: shards already present are skipped (resume)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35676d96-1602-42a2-aa03-0d4f52ad3b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb4c1f481494c1297fa4b0dfc301084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] shard 1/12 già presente → skip\n",
      "[train] shard 2/12 già presente → skip\n",
      "[train] shard 3/12 già presente → skip\n",
      "[train] shard 4/12 già presente → skip\n",
      "[train] shard 5/12 già presente → skip\n",
      "[train] shard 6/12 già presente → skip\n",
      "[train] shard 7/12 già presente → skip\n",
      "[train] shard 8/12 già presente → skip\n",
      "[train] shard 9/12 già presente → skip\n",
      "[train] shard 10/12 già presente → skip\n",
      "[train] shard 11/12 già presente → skip\n",
      "[train] shard 12/12: images=78,673, post≈64,292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 39/39 [2:09:08<00:00, 198.67s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\train_handcrafted_post.part11.parquet\n",
      "Merge: D:/dataset/img_features\\train_handcrafted_post.parquet  (773307 post, 152 dim)\n",
      "The function can be relaunched: shards already present are skipped (resume)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "compute_and_save_split_sharded_resume(\"train\", n_shards=12, batch_size=2048, cast_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c2cdda-5a16-476e-989e-b67be7f50bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946d4c8dbfe34b1ba360fa4f1a167c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] shard 1/16: images=85,514, post≈64,544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 42/42 [2:24:33<00:00, 206.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part0.parquet\n",
      "[validation] shard 2/16: images=86,609, post≈64,776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:35:32<00:00, 217.03s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part1.parquet\n",
      "[validation] shard 3/16: images=86,364, post≈64,690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:35:17<00:00, 216.69s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part2.parquet\n",
      "[validation] shard 4/16: images=86,834, post≈64,960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:37:23<00:00, 219.61s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part3.parquet\n",
      "[validation] shard 5/16: images=86,555, post≈64,793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:35:53<00:00, 217.52s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part4.parquet\n",
      "[validation] shard 6/16: images=86,526, post≈64,784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:35:08<00:00, 216.47s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part5.parquet\n",
      "[validation] shard 7/16: images=87,177, post≈65,034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:34:06<00:00, 215.04s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part6.parquet\n",
      "[validation] shard 8/16: images=86,066, post≈64,621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:31:59<00:00, 212.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part7.parquet\n",
      "[validation] shard 9/16: images=86,382, post≈64,711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:32:09<00:00, 212.31s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part8.parquet\n",
      "[validation] shard 10/16: images=86,148, post≈64,478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:31:27<00:00, 211.34s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part9.parquet\n",
      "[validation] shard 11/16: images=86,721, post≈64,999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:33:51<00:00, 214.68s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part10.parquet\n",
      "[validation] shard 12/16: images=86,933, post≈65,200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:34:07<00:00, 215.06s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part11.parquet\n",
      "[validation] shard 13/16: images=85,346, post≈64,307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 42/42 [2:30:52<00:00, 215.54s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part12.parquet\n",
      "[validation] shard 14/16: images=87,678, post≈65,100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:34:44<00:00, 215.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part13.parquet\n",
      "[validation] shard 15/16: images=85,966, post≈64,489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 42/42 [2:31:01<00:00, 215.74s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part14.parquet\n",
      "[validation] shard 16/16: images=86,187, post≈64,719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 43/43 [2:30:47<00:00, 210.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\validation_handcrafted_post.part15.parquet\n",
      "Merge: D:/dataset/img_features\\validation_handcrafted_post.parquet  (1036205 post, 152 dim)\n",
      "The function can be relaunched: shards already present are skipped (resume)\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "compute_and_save_split_sharded_resume(\"validation\", n_shards=16, batch_size=2048, cast_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd63593-3e82-4d89-91d4-77d125921c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b400f1ee0974ec7bdd0b3d49d71964c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] shard 1/16 già presente → skip\n",
      "[test] shard 2/16 già presente → skip\n",
      "[test] shard 3/16 già presente → skip\n",
      "[test] shard 4/16 già presente → skip\n",
      "[test] shard 5/16 già presente → skip\n",
      "[test] shard 6/16 già presente → skip\n",
      "[test] shard 7/16: images=90,882, post≈65,739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:38:43<00:00, 211.63s/it]  \n",
      "C:\\Users\\mimox\\AppData\\Local\\Temp\\ipykernel_16456\\648442350.py:50: RuntimeWarning: overflow encountered in cast\n",
      "  mat = mat.astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part6.parquet\n",
      "[test] shard 8/16: images=89,990, post≈65,462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 44/44 [2:37:49<00:00, 215.22s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part7.parquet\n",
      "[test] shard 9/16: images=89,799, post≈65,198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 44/44 [2:36:09<00:00, 212.94s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part8.parquet\n",
      "[test] shard 10/16: images=91,336, post≈65,661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:37:11<00:00, 209.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part9.parquet\n",
      "[test] shard 11/16: images=90,555, post≈65,669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:30:10<00:00, 200.23s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part10.parquet\n",
      "[test] shard 12/16: images=90,780, post≈65,844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:40:45<00:00, 214.35s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part11.parquet\n",
      "[test] shard 13/16: images=90,502, post≈65,478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:37:30<00:00, 210.01s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part12.parquet\n",
      "[test] shard 14/16: images=91,204, post≈65,959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:44:11<00:00, 218.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part13.parquet\n",
      "[test] shard 15/16: images=89,996, post≈65,653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 44/44 [2:45:54<00:00, 226.23s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part14.parquet\n",
      "[test] shard 16/16: images=91,212, post≈65,993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 45/45 [2:44:47<00:00, 219.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: D:/dataset/img_features\\test_handcrafted_post.part15.parquet\n",
      "Merge: D:/dataset/img_features\\test_handcrafted_post.parquet  (1050569 post, 152 dim)\n",
      "The function can be relaunched: shards already present are skipped (resume)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "compute_and_save_split_sharded_resume(\"test\", n_shards=16, batch_size=2048, cast_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3f0560-7ae5-401c-ac01-db17c449a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = r\"D:/db/meta.duckdb\"\n",
    "\n",
    "con = duckdb.connect(DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed986b56-36e2-4596-b126-6e21311fcc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with handcrafted features\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE features.img_handcrafted_post AS\n",
    "SELECT *\n",
    "FROM read_parquet([\n",
    "  'D:/dataset/img_features/train_handcrafted_post.parquet',\n",
    "  'D:/dataset/img_features/validation_handcrafted_post.parquet',\n",
    "  'D:/dataset/img_features/test_handcrafted_post.parquet'\n",
    "]);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ab4de7e-edcd-4600-b627-e34bc4b189f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214b4d271a174cbab85a95393168237f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1609426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_star()\n",
       "0       1609426"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"CREATE OR REPLACE TABLE md1718 AS\n",
    "SELECT * FROM metadata1718_ready WHERE split = 'validation' OR split = 'test'\n",
    "UNION ALL\n",
    "SELECT * FROM train_balanced\"\"\")\n",
    "\n",
    "con.sql(\"\"\"SELECT COUNT(*) FROM md1718\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e3ae987-dc8a-48c1-84e1-594b8db47f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531276e40edf4e34928e5f76eacc8d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1611159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_star()\n",
       "0       1611159"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add the metadata split\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE features.img_handcrafted AS\n",
    "SELECT\n",
    "    i.*\n",
    "    m.split                \n",
    "FROM features.img_handcrafted_post i\n",
    "JOIN md1718 m\n",
    "  ON i.post_id = m.post_id;\n",
    "\"\"\")\n",
    "\n",
    "con.sql(\"SELECT COUNT(*) FROM features.img_handcrafted\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71fc8985-2ec6-4158-8a98-b9d659d3efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duplicates\n",
       "0        1733"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"SELECT COUNT(*) - COUNT(DISTINCT post_id) AS duplicates from features.img_handcrafted\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f71ba543-e377-402f-b140-340b731b6881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaceb1dc45e9443b93679814eebdd6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x2a1b81260b0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"CREATE OR REPLACE TABLE features.img_handcrafted AS\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY post_id ORDER BY post_id) AS rn\n",
    "    FROM features.img_handcrafted\n",
    ")\n",
    "WHERE rn = 1;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbd26e6b-f78b-4e7b-bd50-98acd9be0a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duplicates\n",
       "0           0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"\"\"SELECT COUNT(*) - COUNT(DISTINCT post_id) AS duplicates from features.img_handcrafted\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c80c792b-7a27-4c8f-8da6-c41fec76da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬────────┐\n",
      "│   split    │   n    │\n",
      "│  varchar   │ int64  │\n",
      "├────────────┼────────┤\n",
      "│ test       │ 423604 │\n",
      "│ train      │ 773497 │\n",
      "│ validation │ 412325 │\n",
      "└────────────┴────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(con.sql(\"\"\"\n",
    "SELECT split, COUNT(*) AS n\n",
    "FROM features.img_handcrafted\n",
    "GROUP BY split\n",
    "ORDER BY 1;\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59275f28-e00f-486f-8c92-30d93a1cd2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5689d1a9b4ea4040ae269840f779a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬──────────┐\n",
      "│   split    │ n_images │\n",
      "│  varchar   │  int64   │\n",
      "├────────────┼──────────┤\n",
      "│ train      │   960048 │\n",
      "│ test       │   588557 │\n",
      "│ validation │   556982 │\n",
      "└────────────┴──────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of images from manifest (il numero è più alto perchè abbiamo poi una riga per ogni post dato che calcoliamo la running mean in caso di più immagini per post)\n",
    "print(con.sql(\"\"\"\n",
    "SELECT split, COUNT(*) AS n_images\n",
    "FROM md1718 m JOIN images_manifest1718_clean i ON m.post_id = i.post_id\n",
    "GROUP BY split\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8afe11e-e981-4c69-8b29-a278c412d4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cdfe7641e04e58b2e40dbcb4167e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed0982fb1fd4f63aa22e1b02bb403be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b7bf2c588d4e2f82408168ce208902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = con.sql(\"\"\"\n",
    "    SELECT DISTINCT split FROM features.img_handcrafted\n",
    "\"\"\").df()[\"split\"]\n",
    "\n",
    "for s in splits:\n",
    "    con.sql(f\"\"\"\n",
    "        COPY (\n",
    "            SELECT *\n",
    "            FROM features.img_handcrafted\n",
    "            WHERE split = '{s}'\n",
    "        ) TO 'D:/dataset/img_features_final/{s}_handcrafted_post.parquet' (FORMAT 'parquet');\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa44052d-27cb-4630-92f9-9d9bffb3224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬─────────┐\n",
      "│ post_id │  split  │\n",
      "│ varchar │ varchar │\n",
      "├─────────┴─────────┤\n",
      "│      0 rows       │\n",
      "└───────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(con.sql(\"\"\"SELECT f.post_id, f.split \n",
    "FROM features.img_handcrafted f LEFT JOIN images_manifest1718_clean i ON f.post_id = i.post_id\n",
    "WHERE i.post_id IS NULL\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a21219b-e11d-4c7e-881a-c07af7a6c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
