{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33f21b1-0e1c-4b22-b2cf-ed96d1c9a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import duckdb, torch, time, os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz, hstack, save_npz\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d4ec7-3199-44a5-9a58-6089a05dd487",
   "metadata": {},
   "source": [
    "# Check which model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928ee0bc-72e6-49af-8b5e-339f39b2077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65362339b774e1bbd6b10a6d3d12492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campione estratto da DuckDB: 5000 righe\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "\n",
    "TEXT_COLUMN = \"caption_bert_clip\"\n",
    "TARGET_COLUMN = \"er_bins\"\n",
    "TABLE_NAME = \"md1718\"\n",
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "\n",
    "SAMPLE_SIZE = 5000 \n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT\n",
    "        {TEXT_COLUMN} AS {TEXT_COLUMN},\n",
    "        {TARGET_COLUMN} AS {TARGET_COLUMN}\n",
    "    FROM {TABLE_NAME}\n",
    "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
    "      AND {TARGET_COLUMN} IS NOT NULL\n",
    "    ORDER BY random()\n",
    "    LIMIT {SAMPLE_SIZE}\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).df()\n",
    "\n",
    "print(f\"Campione estratto da DuckDB: {len(df)} righe\")\n",
    "\n",
    "texts = df[TEXT_COLUMN].astype(str).tolist()\n",
    "\n",
    "# Numeric target for XGBoost\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[TARGET_COLUMN])\n",
    "\n",
    "# Fixed split on indexes to ensure the same are used\n",
    "indices = np.arange(len(df))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14254d3b-7a9b-463a-b356-373dcea60096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Upload encoder: mpnet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6915349eab2348d58d60ebb0124eb380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time embedding extraction: 455.04 s\n",
      "\n",
      " Upload encoder: minilm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa9fc58fdf545dbb2859c55f15de207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time embedding extraction: 72.18 s\n",
      "\n",
      " Upload encoder: e5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a17eedd40c40eaade63afb95d78a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/356 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimox\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mimox\\.cache\\huggingface\\hub\\models--intfloat--e5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a64c7f81f1c4086b0be8f33d22ce682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dca24a29294de9b712045f7e2106ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd86fb805bc34cabab75503c49c68116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b690e0c22f49f9a8e39bd2edc17076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e040d144354fa0a7be688e1f8bce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time embedding extraction: 725.68 s\n",
      "\n",
      " FINAL RESULTS\n",
      "Encoder  Embedding Time (s) Classifier  Train Time (s)  Inference Time (s)  Accuracy  F1-macro\n",
      "e5-base          725.681344    XGBoost      169.532226            0.035311     0.247  0.245108\n",
      "e5-base          725.681344  LinearSVM        4.000240            0.006941     0.246  0.244317\n",
      "e5-base          725.681344 NaiveBayes        0.030148            0.047309     0.252  0.242722\n",
      "  mpnet          455.042816    XGBoost      143.430291            0.023739     0.232  0.233500\n",
      "  mpnet          455.042816  LinearSVM        5.008187            0.027876     0.233  0.232215\n",
      " minilm           72.177455 NaiveBayes        0.023734            0.044004     0.231  0.225793\n",
      " minilm           72.177455  LinearSVM        5.487381            0.000000     0.226  0.225073\n",
      " minilm           72.177455    XGBoost       77.640962            0.025781     0.219  0.219217\n",
      "  mpnet          455.042816 NaiveBayes        0.031173            0.042938     0.206  0.199581\n"
     ]
    }
   ],
   "source": [
    "# List of encoders to test\n",
    "ENCODERS = {\n",
    "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"minilm\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    \"e5-base\": \"intfloat/e5-base\",\n",
    "}\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(model_name, model_path, texts):\n",
    "    print(f\"\\n Upload encoder: {model_name}\")\n",
    "    model = SentenceTransformer(model_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    extraction_time = end_time - start_time\n",
    "    print(f\" Time embedding extraction: {extraction_time:.2f} s\")\n",
    "\n",
    "    return np.array(embeddings), extraction_time\n",
    "\n",
    "# Function classify ER based on the extracted embeddings\n",
    "def benchmark_classifiers(X, y, train_idx, test_idx):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1) Linear SVM\n",
    "    svm = LinearSVC(random_state=RANDOM_STATE)\n",
    "    start_train = time.time()\n",
    "    svm.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    svm_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    svm_infer_time = end_pred - start_pred\n",
    "\n",
    "    svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "    svm_f1 = f1_score(y_test, y_pred_svm, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"LinearSVM\",\n",
    "        \"Train Time (s)\": svm_train_time,\n",
    "        \"Inference Time (s)\": svm_infer_time,\n",
    "        \"Accuracy\": svm_acc,\n",
    "        \"F1-macro\": svm_f1\n",
    "    })\n",
    "\n",
    "    # 2) Naive Bayes (GaussianNB)\n",
    "    nb = GaussianNB()\n",
    "    start_train = time.time()\n",
    "    nb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    nb_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_nb = nb.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    nb_infer_time = end_pred - start_pred\n",
    "\n",
    "    nb_acc = accuracy_score(y_test, y_pred_nb)\n",
    "    nb_f1 = f1_score(y_test, y_pred_nb, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"NaiveBayes\",\n",
    "        \"Train Time (s)\": nb_train_time,\n",
    "        \"Inference Time (s)\": nb_infer_time,\n",
    "        \"Accuracy\": nb_acc,\n",
    "        \"F1-macro\": nb_f1\n",
    "    })\n",
    "\n",
    "    # 3) XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softmax\" if len(np.unique(y)) > 2 else \"binary:logistic\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"auto\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    xgb_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    xgb_infer_time = end_pred - start_pred\n",
    "\n",
    "    xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "    xgb_f1 = f1_score(y_test, y_pred_xgb, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"XGBoost\",\n",
    "        \"Train Time (s)\": xgb_train_time,\n",
    "        \"Inference Time (s)\": xgb_infer_time,\n",
    "        \"Accuracy\": xgb_acc,\n",
    "        \"F1-macro\": xgb_f1\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for enc_name, enc_path in ENCODERS.items():\n",
    "    # 1) embedding + time\n",
    "    X, emb_time = extract_embeddings(enc_name, enc_path, texts)\n",
    "\n",
    "    # 2) classification + time\n",
    "    clf_results = benchmark_classifiers(X, y, train_idx, test_idx)\n",
    "\n",
    "    # 3) encoder info\n",
    "    for r in clf_results:\n",
    "        r_with_enc = {\n",
    "            \"Encoder\": enc_name,\n",
    "            \"Embedding Time (s)\": emb_time,\n",
    "            **r\n",
    "        }\n",
    "        all_results.append(r_with_enc)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values([\"F1-macro\", \"Accuracy\"], ascending=False)\n",
    "\n",
    "print(\"\\n FINAL RESULTS\")\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(\"classification_embedding_benchmark_results.csv\", index=False)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e14ba-300e-4e26-a5da-a955488a2343",
   "metadata": {},
   "source": [
    "# EMBEDDINGS EXTRACTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ae834b-8dfc-4133-a831-1fee53ead328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbc3214-c29f-4c1f-a287-1b6eb70ec657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name):\n",
    "    print(f\"Loading {split_name}...\")\n",
    "    df = con.sql(f\"\"\"\n",
    "        SELECT post_id, caption_bert_clip, er_bins\n",
    "        FROM md1718\n",
    "        WHERE split = '{split_name}'\n",
    "    \"\"\").df()\n",
    "    ids = df[\"post_id\"].to_numpy()\n",
    "    texts = df[\"caption_bert_clip\"].tolist()\n",
    "    y = df[\"er_bins\"]\n",
    "    del df; gc.collect()\n",
    "    print(f\"{split_name} done.\")\n",
    "    return ids, texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854327df-20ac-4f5a-a993-f81225641de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, Xtr_text, y_tr = load_split(\"train\")\n",
    "val_ids, Xva_text, y_val  = load_split(\"validation\")\n",
    "test_ids, Xte_text, y_te  = load_split(\"test\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e467909-150c-41c6-ac1f-929bea11ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = Path(\"D:/dataset/sbert_emb\")\n",
    "\n",
    "np.save('D:/dataset/sbert_emb/y_tr.npy', y_tr)\n",
    "np.save('D:/dataset/sbert_emb/y_val.npy', y_val)\n",
    "np.save('D:/dataset/sbert_emb/y_te.npy',  y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0609d50a-acf4-4747-afff-c5028f71cd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "model.max_seq_length = 256 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b8f4ac-88e4-49c6-aed7-d40761e9386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a3e446-fdaa-4ad4-b9f9-ea3c88d41c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts, bs=64):\n",
    "    return model.encode(\n",
    "        texts,\n",
    "        batch_size=bs,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True # ogni vettore avrà lunghezza 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e47bff3-0101-49fe-8e3d-acb0da3c546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(8)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "emb_dir = Path(\"D:/dataset/sbert_emb\")\n",
    "emb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save embeddings, ids, y aligned in shards\n",
    "def embed_sharded_cached(texts, ids, y, split_name, shard=5000):\n",
    "    prefix = f\"{model_name.split('/')[-1]}_{split_name}\"\n",
    "    out_files = []\n",
    "\n",
    "    # Sanity check\n",
    "    n = len(texts)\n",
    "    assert len(ids) == n, f\"ids len {len(ids)} != texts len {n}\"\n",
    "    assert len(y) == n,   f\"y len {len(y)} != texts len {n}\"\n",
    "\n",
    "    for i in range(0, n, shard):\n",
    "        f = emb_dir / f\"{prefix}_{i:07d}.npy\"\n",
    "        if f.exists():\n",
    "            print(f\"[skip] {f.name}\")\n",
    "            out_files.append(f)\n",
    "            continue\n",
    "\n",
    "        part = texts[i:i+shard]\n",
    "        t0 = time.time()\n",
    "        E = embed(part, bs=64).astype(\"float32\")\n",
    "        np.save(f, E)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[saved] {f.name}  [{i+len(part)}/{n}]  ({len(part)} samples in {dt:.1f}s)\")\n",
    "\n",
    "        out_files.append(f)\n",
    "\n",
    "    # Concatenate all shards\n",
    "    arrays = [np.load(f) for f in out_files]\n",
    "    E_all = np.vstack(arrays).astype(\"float32\")\n",
    "\n",
    "    # Safety check\n",
    "    assert E_all.shape[0] == n, \\\n",
    "        f\"Emb rows {E_all.shape[0]} != texts len {n}\"\n",
    "\n",
    "    # Save all files in a uniquel ALL file\n",
    "    np.save(emb_dir / f\"{prefix}_ALL.npy\", E_all)\n",
    "\n",
    "    # Save in .npz aligning embeddings, with ids and y\n",
    "    npz_path = emb_dir / f\"{prefix}_ids_y.npz\"\n",
    "    np.savez(\n",
    "        npz_path,\n",
    "        ids=np.asarray(ids),\n",
    "        embeddings=E_all,\n",
    "        y=np.asarray(y)\n",
    "    )\n",
    "\n",
    "    print(f\"[done] Saved aligned ids + embeddings + y → {npz_path.name}\")\n",
    "\n",
    "    return E_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a550d4ca-d170-4299-9bb7-b6b0e1b3822c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train…\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0000000.npy  [20000/773497]  (20000 samples in 207.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0020000.npy  [40000/773497]  (20000 samples in 181.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0040000.npy  [60000/773497]  (20000 samples in 230.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0060000.npy  [80000/773497]  (20000 samples in 230.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0080000.npy  [100000/773497]  (20000 samples in 283.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0100000.npy  [120000/773497]  (20000 samples in 276.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0120000.npy  [140000/773497]  (20000 samples in 268.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0140000.npy  [160000/773497]  (20000 samples in 281.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0160000.npy  [180000/773497]  (20000 samples in 300.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0180000.npy  [200000/773497]  (20000 samples in 276.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0200000.npy  [220000/773497]  (20000 samples in 256.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0220000.npy  [240000/773497]  (20000 samples in 244.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0240000.npy  [260000/773497]  (20000 samples in 267.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0260000.npy  [280000/773497]  (20000 samples in 271.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0280000.npy  [300000/773497]  (20000 samples in 247.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0300000.npy  [320000/773497]  (20000 samples in 253.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0320000.npy  [340000/773497]  (20000 samples in 265.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0340000.npy  [360000/773497]  (20000 samples in 229.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0360000.npy  [380000/773497]  (20000 samples in 279.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0380000.npy  [400000/773497]  (20000 samples in 276.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0400000.npy  [420000/773497]  (20000 samples in 246.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0420000.npy  [440000/773497]  (20000 samples in 240.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0440000.npy  [460000/773497]  (20000 samples in 217.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0460000.npy  [480000/773497]  (20000 samples in 208.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0480000.npy  [500000/773497]  (20000 samples in 259.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0500000.npy  [520000/773497]  (20000 samples in 246.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0520000.npy  [540000/773497]  (20000 samples in 243.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0540000.npy  [560000/773497]  (20000 samples in 268.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0560000.npy  [580000/773497]  (20000 samples in 239.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0580000.npy  [600000/773497]  (20000 samples in 279.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0600000.npy  [620000/773497]  (20000 samples in 273.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0620000.npy  [640000/773497]  (20000 samples in 279.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0640000.npy  [660000/773497]  (20000 samples in 217.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0660000.npy  [680000/773497]  (20000 samples in 261.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0680000.npy  [700000/773497]  (20000 samples in 254.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0700000.npy  [720000/773497]  (20000 samples in 259.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0720000.npy  [740000/773497]  (20000 samples in 265.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0740000.npy  [760000/773497]  (20000 samples in 271.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_train_0760000.npy  [773497/773497]  (13497 samples in 180.0s)\n",
      "[done] Saved aligned ids + embeddings + y → paraphrase-MiniLM-L6-v2_train_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Train…\")\n",
    "E_tr = embed_sharded_cached(Xtr_text, train_ids, y_tr, \"train\", shard=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba471b40-e67a-40eb-bce5-b589da0e45a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val…\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0000000.npy  [20000/412325]  (20000 samples in 193.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0020000.npy  [40000/412325]  (20000 samples in 235.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0040000.npy  [60000/412325]  (20000 samples in 258.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0060000.npy  [80000/412325]  (20000 samples in 261.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0080000.npy  [100000/412325]  (20000 samples in 258.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0100000.npy  [120000/412325]  (20000 samples in 259.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0120000.npy  [140000/412325]  (20000 samples in 276.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0140000.npy  [160000/412325]  (20000 samples in 244.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0160000.npy  [180000/412325]  (20000 samples in 239.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0180000.npy  [200000/412325]  (20000 samples in 248.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0200000.npy  [220000/412325]  (20000 samples in 255.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0220000.npy  [240000/412325]  (20000 samples in 280.3s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0240000.npy  [260000/412325]  (20000 samples in 251.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0260000.npy  [280000/412325]  (20000 samples in 256.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0280000.npy  [300000/412325]  (20000 samples in 247.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0300000.npy  [320000/412325]  (20000 samples in 246.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0320000.npy  [340000/412325]  (20000 samples in 248.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0340000.npy  [360000/412325]  (20000 samples in 266.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0360000.npy  [380000/412325]  (20000 samples in 273.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0380000.npy  [400000/412325]  (20000 samples in 299.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_val_0400000.npy  [412325/412325]  (12325 samples in 167.9s)\n",
      "[done] Saved aligned ids + embeddings + y → paraphrase-MiniLM-L6-v2_val_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Val…\")\n",
    "E_va = embed_sharded_cached(Xva_text, val_ids, y_val, \"val\",   shard=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd918396-bcdc-4d2f-bfed-ff399d3a3161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test…\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0000000.npy  [20000/423604]  (20000 samples in 223.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0020000.npy  [40000/423604]  (20000 samples in 250.1s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0040000.npy  [60000/423604]  (20000 samples in 258.4s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0060000.npy  [80000/423604]  (20000 samples in 260.7s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0080000.npy  [100000/423604]  (20000 samples in 260.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0100000.npy  [120000/423604]  (20000 samples in 251.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0120000.npy  [140000/423604]  (20000 samples in 229.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0140000.npy  [160000/423604]  (20000 samples in 249.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0160000.npy  [180000/423604]  (20000 samples in 282.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0180000.npy  [200000/423604]  (20000 samples in 272.2s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0200000.npy  [220000/423604]  (20000 samples in 231.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0220000.npy  [240000/423604]  (20000 samples in 243.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0240000.npy  [260000/423604]  (20000 samples in 269.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0260000.npy  [280000/423604]  (20000 samples in 249.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0280000.npy  [300000/423604]  (20000 samples in 261.0s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0300000.npy  [320000/423604]  (20000 samples in 255.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0320000.npy  [340000/423604]  (20000 samples in 254.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0340000.npy  [360000/423604]  (20000 samples in 249.9s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0360000.npy  [380000/423604]  (20000 samples in 238.5s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0380000.npy  [400000/423604]  (20000 samples in 259.6s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0400000.npy  [420000/423604]  (20000 samples in 255.8s)\n",
      "[saved] paraphrase-MiniLM-L6-v2_test_0420000.npy  [423604/423604]  (3604 samples in 51.6s)\n",
      "[done] Saved aligned ids + embeddings + y → paraphrase-MiniLM-L6-v2_test_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Test…\")\n",
    "E_te = embed_sharded_cached(Xte_text, test_ids, y_te, \"test\",  shard=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b3de9-f966-48a1-8d55-1c415b96f935",
   "metadata": {},
   "source": [
    "# LOAD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5803defd-711e-4bd1-a4e9-8dafd7dab5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"D:/dataset/sbert_emb/paraphrase-MiniLM-L6-v2_train_ids_y.npz\", allow_pickle=True)\n",
    "E_tr = train[\"embeddings\"]\n",
    "y_tr = train[\"y\"]\n",
    "\n",
    "val = np.load(\"D:/dataset/sbert_emb/paraphrase-MiniLM-L6-v2_val_ids_y.npz\", allow_pickle=True)\n",
    "E_va = val[\"embeddings\"]\n",
    "y_va = val[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2916635-da45-4e18-ae1c-b216ad9f3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773497, 384) (412325, 384) (773497,) (412325,)\n"
     ]
    }
   ],
   "source": [
    "print(E_tr.shape, E_va.shape, y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672d7439-7021-4f44-9f8d-e4f7edb7d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.12581869535837908 | accuracy (val): 0.20248105256775603\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.1544386658411066 | accuracy (val): 0.2149420966470624\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.23603247877747063 | accuracy (val): 0.24126114109015945\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2390775257199076 | accuracy (val): 0.24866064390953738\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.22067993556451188 | accuracy (val): 0.22984053841023463\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.22141086314808947 | accuracy (val): 0.22974110228581823\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.23735477518888679 | accuracy (val): 0.2413460255866125\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2378513100885339 | accuracy (val): 0.246312981264779\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.1964612198135833 | accuracy (val): 0.22479839932092402\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.16778839297028766 | accuracy (val): 0.2196325713939247\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.23375002622608246 | accuracy (val): 0.23784878433274723\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.23895280779875297 | accuracy (val): 0.24464439459164494\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.14061268119111164 | accuracy (val): 0.22387922148790396\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.13422269141220672 | accuracy (val): 0.22171587946401503\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.23041562365198578 | accuracy (val): 0.23603225610865217\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.23629616975560416 | accuracy (val): 0.24321833505123386\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 1e-05, 'average': True, 'class_weight': 'balanced'}\n",
      "Validation macro-F1: 0.2390775257199076\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "      alpha class_weight  average  val_macro_f1  val_accuracy\n",
      "3   0.00001     balanced     True      0.239078      0.248661\n",
      "11  0.00100     balanced     True      0.238953      0.244644\n",
      "7   0.00010     balanced     True      0.237851      0.246313\n",
      "6   0.00010         None     True      0.237355      0.241346\n",
      "15  0.01000     balanced     True      0.236296      0.243218\n",
      "2   0.00001         None     True      0.236032      0.241261\n",
      "10  0.00100         None     True      0.233750      0.237849\n",
      "14  0.01000         None     True      0.230416      0.236032\n",
      "5   0.00010     balanced    False      0.221411      0.229741\n",
      "4   0.00010         None    False      0.220680      0.229841\n",
      "8   0.00100         None    False      0.196461      0.224798\n",
      "9   0.00100     balanced    False      0.167788      0.219633\n",
      "1   0.00001     balanced    False      0.154439      0.214942\n",
      "12  0.01000         None    False      0.140613      0.223879\n",
      "13  0.01000     balanced    False      0.134223      0.221716\n",
      "0   0.00001         None    False      0.125819      0.202481\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(E_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(E_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471fcd62-4045-4661-9e1d-a73a40376a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.2224 | accuracy (val): 0.2428\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.2224 | accuracy (val): 0.2428\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.2224 | accuracy (val): 0.2428\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.2224 | accuracy (val): 0.2428\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.2223940710152486\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09      0.222394      0.242753\n",
      "1   1.000000e-08      0.222394      0.242753\n",
      "2   1.000000e-07      0.222394      0.242753\n",
      "3   1.000000e-06      0.222394      0.242753\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(E_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(E_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910cce2a-e45a-48ee-81c5-f67a6a3e1d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1735 | accuracy (val): 0.2169\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1716 | accuracy (val): 0.2169\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1708 | accuracy (val): 0.2175\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1746 | accuracy (val): 0.2174\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1723 | accuracy (val): 0.2178\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1713 | accuracy (val): 0.2183\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1735 | accuracy (val): 0.2169\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1716 | accuracy (val): 0.2169\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1708 | accuracy (val): 0.2175\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1746 | accuracy (val): 0.2174\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1723 | accuracy (val): 0.2178\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1713 | accuracy (val): 0.2183\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1868 | accuracy (val): 0.2203\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1836 | accuracy (val): 0.2205\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1815 | accuracy (val): 0.2208\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1880 | accuracy (val): 0.2204\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1845 | accuracy (val): 0.2205\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1819 | accuracy (val): 0.2203\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1868 | accuracy (val): 0.2203\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1836 | accuracy (val): 0.2205\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1815 | accuracy (val): 0.2208\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1880 | accuracy (val): 0.2204\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1845 | accuracy (val): 0.2205\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1819 | accuracy (val): 0.2203\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2002 | accuracy (val): 0.2220\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1970 | accuracy (val): 0.2230\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1943 | accuracy (val): 0.2234\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2000 | accuracy (val): 0.2226\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1958 | accuracy (val): 0.2226\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1936 | accuracy (val): 0.2233\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2002 | accuracy (val): 0.2220\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1970 | accuracy (val): 0.2230\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1943 | accuracy (val): 0.2234\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2000 | accuracy (val): 0.2226\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1958 | accuracy (val): 0.2226\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1936 | accuracy (val): 0.2233\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "Validation macro-F1: 0.2002431410047622\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "30            30         12                 2         sqrt      0.200243   \n",
      "24            30         12                 2         0.05      0.200243   \n",
      "33            30         12                 5         sqrt      0.200016   \n",
      "27            30         12                 5         0.05      0.200016   \n",
      "31            50         12                 2         sqrt      0.196983   \n",
      "25            50         12                 2         0.05      0.196983   \n",
      "34            50         12                 5         sqrt      0.195775   \n",
      "28            50         12                 5         0.05      0.195775   \n",
      "32            80         12                 2         sqrt      0.194264   \n",
      "26            80         12                 2         0.05      0.194264   \n",
      "35            80         12                 5         sqrt      0.193576   \n",
      "29            80         12                 5         0.05      0.193576   \n",
      "15            30         10                 5         0.05      0.188028   \n",
      "21            30         10                 5         sqrt      0.188028   \n",
      "18            30         10                 2         sqrt      0.186822   \n",
      "12            30         10                 2         0.05      0.186822   \n",
      "22            50         10                 5         sqrt      0.184514   \n",
      "16            50         10                 5         0.05      0.184514   \n",
      "19            50         10                 2         sqrt      0.183558   \n",
      "13            50         10                 2         0.05      0.183558   \n",
      "17            80         10                 5         0.05      0.181931   \n",
      "23            80         10                 5         sqrt      0.181931   \n",
      "14            80         10                 2         0.05      0.181522   \n",
      "20            80         10                 2         sqrt      0.181522   \n",
      "9             30          8                 5         sqrt      0.174634   \n",
      "3             30          8                 5         0.05      0.174634   \n",
      "6             30          8                 2         sqrt      0.173524   \n",
      "0             30          8                 2         0.05      0.173524   \n",
      "10            50          8                 5         sqrt      0.172288   \n",
      "4             50          8                 5         0.05      0.172288   \n",
      "7             50          8                 2         sqrt      0.171611   \n",
      "1             50          8                 2         0.05      0.171611   \n",
      "5             80          8                 5         0.05      0.171290   \n",
      "11            80          8                 5         sqrt      0.171290   \n",
      "8             80          8                 2         sqrt      0.170792   \n",
      "2             80          8                 2         0.05      0.170792   \n",
      "\n",
      "    val_accuracy  \n",
      "30      0.222000  \n",
      "24      0.222000  \n",
      "33      0.222630  \n",
      "27      0.222630  \n",
      "31      0.223021  \n",
      "25      0.223021  \n",
      "34      0.222599  \n",
      "28      0.222599  \n",
      "32      0.223418  \n",
      "26      0.223418  \n",
      "35      0.223263  \n",
      "29      0.223263  \n",
      "15      0.220428  \n",
      "21      0.220428  \n",
      "18      0.220261  \n",
      "12      0.220261  \n",
      "22      0.220508  \n",
      "16      0.220508  \n",
      "19      0.220525  \n",
      "13      0.220525  \n",
      "17      0.220273  \n",
      "23      0.220273  \n",
      "14      0.220758  \n",
      "20      0.220758  \n",
      "9       0.217399  \n",
      "3       0.217399  \n",
      "6       0.216870  \n",
      "0       0.216870  \n",
      "10      0.217758  \n",
      "4       0.217758  \n",
      "7       0.216938  \n",
      "1       0.216938  \n",
      "5       0.218282  \n",
      "11      0.218282  \n",
      "8       0.217503  \n",
      "2       0.217503  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(E_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(E_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b2cbd0-9eb1-4244-975f-a3e5fcd6a1a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2241 | accuracy (val): 0.2349\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2310 | accuracy (val): 0.2388\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2348 | accuracy (val): 0.2412\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2399 | accuracy (val): 0.2447\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2241 | accuracy (val): 0.2349\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2310 | accuracy (val): 0.2388\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2348 | accuracy (val): 0.2409\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2392 | accuracy (val): 0.2440\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.23988546249700807\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "3               0.5      0            0.1          6           150   \n",
      "7               0.5      1            0.1          6           150   \n",
      "2               0.5      0            0.1          6           100   \n",
      "6               0.5      1            0.1          6           100   \n",
      "1               0.5      0            0.1          4           150   \n",
      "5               0.5      1            0.1          4           150   \n",
      "0               0.5      0            0.1          4           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "3           1        0.8      0.239885      0.244657  \n",
      "7           1        0.8      0.239214      0.243958  \n",
      "2           1        0.8      0.234830      0.241184  \n",
      "6           1        0.8      0.234754      0.240919  \n",
      "1           1        0.8      0.230993      0.238753  \n",
      "5           1        0.8      0.230993      0.238753  \n",
      "0           1        0.8      0.224068      0.234934  \n",
      "4           1        0.8      0.224068      0.234934  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5], \n",
    "    \"gamma\": [0, 1], \n",
    "    \"reg_lambda\": [1], \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(E_tr, y_tr_enc)\n",
    "\n",
    "    y_val_pred = clf.predict(E_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d72a9-1b1b-4a3b-8f64-e6757151ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE SUL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e50ef-ebb7-448f-934a-d6eff9725892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"D:/dataset/sbert_emb/paraphrase-MiniLM-L6-v2_train_ids_y.npz\", allow_pickle=True)\n",
    "E_tr = train[\"embeddings\"]\n",
    "y_tr = train[\"y\"]\n",
    "\n",
    "val = np.load(\"D:/dataset/sbert_emb/paraphrase-MiniLM-L6-v2_val_ids_y.npz\", allow_pickle=True)\n",
    "E_va = val[\"embeddings\"]\n",
    "y_va = val[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbd98e1-2dcb-4525-98ca-2a3b6f644569",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"D:/dataset/sbert_emb/paraphrase-MiniLM-L6-v2_test_ids_y.npz\", allow_pickle=True)\n",
    "E_te = test[\"embeddings\"]\n",
    "y_te = test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abb5855-f0f7-4ce0-a003-c964b59c6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_trva = np.concatenate((E_tr, E_va), axis = 0)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1db6c3-bd0f-4696-87d7-fa315994ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_trva_enc = le.fit_transform(y_trva)\n",
    "y_te_enc = le.transform(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d69ea24-38df-44bd-9c56-a63f69742659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1185822, 384) (1185822,) (423604, 384) (423604,)\n"
     ]
    }
   ],
   "source": [
    "print(E_trva.shape, y_trva.shape, E_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d67c49d6-d287-4836-8940-28a917446eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (test): 0.2201 | accuracy (test): 0.2447\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=12, max_features=0.05, min_samples_leaf=2,\n",
      "                       n_estimators=30, n_jobs=-1, random_state=42)\n",
      "macro-F1 (test): 0.2282 | accuracy (test): 0.2352\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=0,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=150, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (test): 0.2526 | accuracy (test): 0.2591\n",
      "\n",
      "Configuration: LinearSVC(C=10, max_iter=5000, random_state=42)\n",
      "macro-F1 (test): 0.2452 | accuracy (test): 0.2586\n"
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=12, max_features=0.05, min_samples_leaf=2, n_estimators=30, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.1, max_depth= 6, n_estimators= 150, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_trva_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(E_trva, y_trva_enc)\n",
    "        y_te_pred = cfg.predict(E_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(E_trva, y_trva)\n",
    "        y_te_pred = cfg.predict(E_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1f2007-85b4-473f-8f35-2bb2e125a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2378 | accuracy (test): 0.2589\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 1e-05,\n",
    "        average = True,\n",
    "        class_weight = 'balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(E_trva, y_trva)\n",
    "y_te_pred = cfg.predict(E_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7520809-042f-45ae-8fd3-af32af04f52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
