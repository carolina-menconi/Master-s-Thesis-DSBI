{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58ade61-60e0-4fbc-8500-7af2d5cff3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, duckdb, joblib, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz, load_npz, vstack, csr_matrix, hstack\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a73a373-ddec-431b-be1b-fc1bdb370b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "DB_PATH   = r\"D:/db/meta.duckdb\"\n",
    "OUT_DIR   = r\"D:/dataset/text_features/tfidf_v2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "   \n",
    "table = \"md1718\"\n",
    "\n",
    "# Connection\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe49ff16-5665-4b0e-b7e8-57973a7ab07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f219daf99de4342a15f783a7a0a882f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬──────────────┐\n",
      "│   split    │ null_caption │\n",
      "│  varchar   │    int64     │\n",
      "├────────────┼──────────────┤\n",
      "│ test       │        12683 │\n",
      "│ validation │        11916 │\n",
      "│ train      │        21571 │\n",
      "└────────────┴──────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of null captions\n",
    "print(con.sql(\"\"\"SELECT split, COUNT(*) AS null_caption FROM md1718 WHERE caption_tfidf ='' GROUP BY split\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b62f3b-ccec-44bc-b92a-d367843c1fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a130cd7786014c67ae63c5ec82430d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          post_id split  \\\n",
      "0  la.brandon-1936393348858127807  test   \n",
      "1  la.brandon-1937213763687991478  test   \n",
      "2  la.brandon-1937218941271111413  test   \n",
      "3  la.brandon-1938181925006920700  test   \n",
      "4  la.brandon-1938620907515771752  test   \n",
      "\n",
      "                                       caption_clean    er_bins  \n",
      "0  thinking things may slide home plate year end ...  very_high  \n",
      "1  still looking gift ideas grab streaming stick ...  very_high  \n",
      "2  often answers looking around corner keepwalkin...  very_high  \n",
      "3  friends christmas countdown received early pre...  very_high  \n",
      "4  god grant serenity accept things cannot change...  very_high  \n"
     ]
    }
   ],
   "source": [
    "# Upload TF-IDF cleaned captions, split, post_id and target er_log\n",
    "\n",
    "df_caps = con.execute(f\"\"\"\n",
    "    SELECT post_id, split, caption_tfidf AS caption_clean, er_bins\n",
    "    FROM md1718\n",
    "\"\"\").df()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(df_caps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0410d0df-9ec4-4547-b46b-0a233b0fa06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train = df_caps[df_caps[\"split\"] == \"train\"].copy()\n",
    "val   = df_caps[df_caps[\"split\"] == \"validation\"].copy()\n",
    "test  = df_caps[df_caps[\"split\"] == \"test\"].copy()\n",
    "\n",
    "# Cleaned captions \n",
    "X_train_text = train[\"caption_clean\"].tolist()\n",
    "X_val_text   = val[\"caption_clean\"].tolist()\n",
    "X_test_text  = test[\"caption_clean\"].tolist()\n",
    "\n",
    "# Target columns\n",
    "y_tr = train[\"er_bins\"].to_numpy()\n",
    "y_va = val[\"er_bins\"].to_numpy()\n",
    "y_te = test[\"er_bins\"].to_numpy()\n",
    "\n",
    "# Post ids for future joins\n",
    "train_ids = train[\"post_id\"].to_numpy()\n",
    "val_ids   = val[\"post_id\"].to_numpy()\n",
    "test_ids  = test[\"post_id\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b2af39-9a5f-4d31-8bab-b4fa2708b1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Config TF-IDF 1/6: {'ngram_range': (1, 1), 'min_df': 15, 'max_df': 0.9, 'max_features': 30000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 30000), Xva: (412325, 30000)\n",
      "     C=0.01    F1_macro=0.2593, accuracy = 0.2601976596131692\n",
      "     C=0.1     F1_macro=0.2573, accuracy = 0.25774328503001276\n",
      "     C=1.0     F1_macro=0.2538, accuracy = 0.25411507912447706\n",
      "     C=10.0    F1_macro=0.2522, accuracy = 0.2526817437700843\n",
      "\n",
      "=== Config TF-IDF 2/6: {'ngram_range': (1, 1), 'min_df': 15, 'max_df': 0.7, 'max_features': 40000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 40000), Xva: (412325, 40000)\n",
      "     C=0.01    F1_macro=0.2605, accuracy = 0.2612963075244043\n",
      "     C=0.1     F1_macro=0.2590, accuracy = 0.25917419511307827\n",
      "     C=1.0     F1_macro=0.2546, accuracy = 0.2547286727702662\n",
      "     C=10.0    F1_macro=0.2524, accuracy = 0.25257260656035896\n",
      "\n",
      "=== Config TF-IDF 3/6: {'ngram_range': (1, 1), 'min_df': 20, 'max_df': 0.7, 'max_features': 50000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 50000), Xva: (412325, 50000)\n",
      "     C=0.01    F1_macro=0.2608, accuracy = 0.2619147517128479\n",
      "     C=0.1     F1_macro=0.2607, accuracy = 0.26089128721275695\n",
      "     C=1.0     F1_macro=0.2562, accuracy = 0.25609895107015096\n",
      "     C=10.0    F1_macro=0.2533, accuracy = 0.2532298550900382\n",
      "\n",
      "=== Config TF-IDF 4/6: {'ngram_range': (1, 2), 'min_df': 15, 'max_df': 0.9, 'max_features': 30000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 30000), Xva: (412325, 30000)\n",
      "     C=0.01    F1_macro=0.2582, accuracy = 0.2590723337173346\n",
      "     C=0.1     F1_macro=0.2564, accuracy = 0.2566179591341781\n",
      "     C=1.0     F1_macro=0.2531, accuracy = 0.2533583944703814\n",
      "     C=10.0    F1_macro=0.2522, accuracy = 0.25250712423452376\n",
      "\n",
      "=== Config TF-IDF 5/6: {'ngram_range': (1, 2), 'min_df': 15, 'max_df': 0.7, 'max_features': 40000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 40000), Xva: (412325, 40000)\n",
      "     C=0.01    F1_macro=0.2586, accuracy = 0.25978536348754017\n",
      "     C=0.1     F1_macro=0.2574, accuracy = 0.2574643788273813\n",
      "     C=1.0     F1_macro=0.2536, accuracy = 0.2535475656339053\n",
      "     C=10.0    F1_macro=0.2516, accuracy = 0.25166798035530225\n",
      "\n",
      "=== Config TF-IDF 6/6: {'ngram_range': (1, 2), 'min_df': 20, 'max_df': 0.7, 'max_features': 50000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']} ===\n",
      "  -> Shape Xtr: (773497, 50000), Xva: (412325, 50000)\n",
      "     C=0.01    F1_macro=0.2596, accuracy = 0.2608161038016128\n",
      "     C=0.1     F1_macro=0.2593, accuracy = 0.2594603771296914\n",
      "     C=1.0     F1_macro=0.2548, accuracy = 0.2547213969562845\n",
      "     C=10.0    F1_macro=0.2523, accuracy = 0.2523713090401989\n",
      "\n",
      "=== Best configuration (on validation, LinearSVC) ===\n",
      "Best F1_macro: 0.2608\n",
      "Best C: 0.01\n",
      "Best TF-IDF cfg: {'ngram_range': (1, 1), 'min_df': 20, 'max_df': 0.7, 'max_features': 50000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b', 'stop_words': ['hearts', 'tone', 'love', 'day', 'today', 'time', 'one', 'two', 'get', 'like', 'eyes', 'hands', 'red', 'see', 'happy', 'life', 'hand', 'hands', 'back', 'link', 'flash', 'http', 'liketk', 'liketkit', 'much', 'know', 'woman', 'man', 'you', 'new', 'go', 'make', 'good', 'best', 'beautiful']}\n"
     ]
    }
   ],
   "source": [
    "# Define the best combination of parameters for the encoding with TF-IDF\n",
    "stop_words = [\n",
    "    \"hearts\", \"tone\", \"love\",\"day\",\"today\",\"time\",\"one\",\"two\",\"get\",\"like\",\n",
    "    \"eyes\",\"hands\",\"red\", \"see\", \"happy\", \"life\", \"hand\", \"hands\", \"back\", \"link\",\"flash\",\n",
    "    \"http\",\"liketk\",\"liketkit\", \"much\", \"know\", \"woman\", \"man\", \n",
    "    \"you\",\"new\", \"go\", \"make\", \"good\", \"best\", \"beautiful\"\n",
    "]\n",
    "\n",
    "cands_vec = [\n",
    "\n",
    "    dict(ngram_range=(1,1), min_df=15, max_df=0.9, max_features=30000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words),\n",
    "\n",
    "    dict(ngram_range=(1,1), min_df=15, max_df=0.7, max_features=40000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words),\n",
    "\n",
    "    dict(ngram_range=(1,1), min_df=20, max_df=0.7, max_features=50000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words),\n",
    "\n",
    "    # --- Bigrammi ---\n",
    "    dict(ngram_range=(1,2), min_df=15, max_df=0.9, max_features=30000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words),\n",
    "\n",
    "    dict(ngram_range=(1,2), min_df=15, max_df=0.7, max_features=40000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words),\n",
    "\n",
    "    dict(ngram_range=(1,2), min_df=20, max_df=0.7, max_features=50000,\n",
    "         token_pattern=r\"(?u)\\b[^\\W\\d_]{2,}\\b\", stop_words = stop_words)\n",
    "]\n",
    "\n",
    "Cs = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_cfg = None\n",
    "best_C = None\n",
    "\n",
    "results = [] \n",
    "\n",
    "for i, cfg in enumerate(cands_vec):\n",
    "    print(f\"\\n=== Config TF-IDF {i+1}/{len(cands_vec)}: {cfg} ===\")\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        dtype=np.float32,\n",
    "        **cfg\n",
    "    )\n",
    "\n",
    "    # FIT sul train e TRANSFORM su train/val\n",
    "    Xtr = vec.fit_transform(X_train_text)\n",
    "    Xva = vec.transform(X_val_text)\n",
    "\n",
    "    print(f\"  -> Shape Xtr: {Xtr.shape}, Xva: {Xva.shape}\")\n",
    "\n",
    "    for C in Cs:\n",
    "        mdl = LinearSVC(C=C, random_state=0)\n",
    "        mdl.fit(Xtr, y_tr)\n",
    "\n",
    "        p_va = mdl.predict(Xva)\n",
    "        f1 = f1_score(y_va, p_va, average=\"macro\")\n",
    "        acc = accuracy_score(y_va, p_va)\n",
    "\n",
    "\n",
    "        results.append((f1, acc, C, cfg))\n",
    "\n",
    "        print(f\"     C={C:<6}  F1_macro={f1:.4f}, accuracy = {acc}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_cfg = cfg\n",
    "            best_C = C\n",
    "            bect_acc = acc\n",
    "\n",
    "print(\"\\n=== Best configuration (on validation, LinearSVC) ===\")\n",
    "print(\"Best F1_macro:\", round(best_f1, 4))\n",
    "print(\"Best C:\", best_C)\n",
    "print(\"Best TF-IDF cfg:\", best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81561d8b-f430-4ebf-bf74-143dee8912cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.22      0.25      0.23     80291\n",
      "         low       0.23      0.26      0.24     81827\n",
      "      medium       0.22      0.21      0.21     81667\n",
      "   very_high       0.31      0.23      0.26     86194\n",
      "    very_low       0.30      0.32      0.31     82346\n",
      "\n",
      "    accuracy                           0.25    412325\n",
      "   macro avg       0.26      0.25      0.25    412325\n",
      "weighted avg       0.26      0.25      0.25    412325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_va, p_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c54dce2-dcd4-4b49-bd07-d7f87302732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: matrices, mapping post ids and vectorizer saved\n"
     ]
    }
   ],
   "source": [
    "# Best TF-IDF cfg: {'ngram_range': (1, 1), 'min_df': 20, 'max_df': 0.7, 'max_features': 50000, 'token_pattern': '(?u)\\\\b[^\\\\W\\\\d_]{2,}\\\\b',\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    analyzer=\"word\", # default\n",
    "    token_pattern = r\"(?u)\\b[^\\W\\d_]{2,}\\b\", # just characters without digits or underscores \n",
    "    ngram_range=(1, 1), # unigrams and bigrams\n",
    "    min_df=20, # discard rare terms (they must appean in at least 10 captions)\n",
    "    max_df=0.7, # discard too frequent terms (if they appear in 80% of the captions or more)\n",
    "    max_features=50000, \n",
    "    dtype=np.float32,\n",
    "    stop_words = stop_words\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_val   = vectorizer.transform(X_val_text)\n",
    "X_test  = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Save the train, validation and test text features in sparse format .npz (reducing the space occupied by not writing zeros)\n",
    "save_npz('D:/dataset/text_features/tfidf_v3/tfidf_train.npz', X_train)\n",
    "save_npz('D:/dataset/text_features/tfidf_v3/tfidf_val.npz',   X_val)\n",
    "save_npz('D:/dataset/text_features/tfidf_v3/tfidf_test.npz',  X_test)\n",
    "\n",
    "# Save post_ids in the same order to allow future joins\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_train_post_ids.npy', train_ids)\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_val_post_ids.npy',   val_ids)\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_test_post_ids.npy',  test_ids)\n",
    "\n",
    "# Save target values in the same order\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_train_y.npy', y_tr)\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_val_y.npy',   y_va)\n",
    "np.save('D:/dataset/text_features/tfidf_v3/tfidf_test_y.npy',  y_te)\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'D:/dataset/text_features/tfidf_v3/tfidf_vectorizer.joblib')\n",
    "print(\"Done: matrices, mapping post ids and vectorizer saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38721530-6508-4d2e-bc31-880bb48cdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKS\n",
    "# Import the data\n",
    "OUT_DIR = r\"D:/dataset/text_features/tfidf_v3\"\n",
    "\n",
    "Xtr = load_npz(os.path.join(OUT_DIR, \"tfidf_train.npz\"))\n",
    "Xva = load_npz(os.path.join(OUT_DIR, \"tfidf_val.npz\"))\n",
    "Xte = load_npz(os.path.join(OUT_DIR, \"tfidf_test.npz\"))\n",
    "\n",
    "tr_ids = np.load(os.path.join(OUT_DIR, \"tfidf_train_post_ids.npy\"), allow_pickle=True)\n",
    "va_ids = np.load(os.path.join(OUT_DIR, \"tfidf_val_post_ids.npy\"), allow_pickle=True)\n",
    "te_ids = np.load(os.path.join(OUT_DIR, \"tfidf_test_post_ids.npy\"), allow_pickle=True)\n",
    "\n",
    "y_tr = np.load(os.path.join(OUT_DIR, \"tfidf_train_y.npy\"), allow_pickle = True)\n",
    "y_va = np.load(os.path.join(OUT_DIR, \"tfidf_val_y.npy\"), allow_pickle = True)\n",
    "y_te = np.load(os.path.join(OUT_DIR, \"tfidf_test_y.npy\"), allow_pickle = True)\n",
    "\n",
    "# Check the train, validation and test set have the same number of columns as the vocabulary\n",
    "vectorizer = joblib.load(os.path.join(OUT_DIR, \"tfidf_vectorizer.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a223a4-5e6c-4dba-bb05-2e41c9683893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (773497, 50000) 773497\n",
      "Val  : (412325, 50000) 412325\n",
      "Test : (423604, 50000) 423604\n",
      "Vocab size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Check the train, validation and test set have the same number of columns as the vocabulary\n",
    "V = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Train:\", Xtr.shape, len(tr_ids))\n",
    "print(\"Val  :\", Xva.shape, len(va_ids))\n",
    "print(\"Test :\", Xte.shape, len(te_ids))\n",
    "print(\"Vocab size:\", V)\n",
    "\n",
    "assert Xtr.shape[1] == Xva.shape[1] == Xte.shape[1] == V, \"The sets have different number of features than the vocabulary\"\n",
    "assert Xtr.shape[0] == len(tr_ids) and Xva.shape[0] == len(va_ids) and Xte.shape[0] == len(te_ids), \"The number of rows is different than the number of post ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4933021-fff7-424d-9e09-3a2f03ff7cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF empty rows % - train: 3.16%\n",
      "                      val : 3.32%\n",
      "                      test : 3.43%\n"
     ]
    }
   ],
   "source": [
    "# Percentage of rows with no caption\n",
    "def zero_row_pct(X):\n",
    "    return float((X.getnnz(axis=1) == 0).sum()) / X.shape[0] * 100\n",
    "\n",
    "print(\"TF-IDF empty rows % - train:\", f\"{zero_row_pct(Xtr):.2f}%\")\n",
    "print(\"                      val :\", f\"{zero_row_pct(Xva):.2f}%\")\n",
    "print(\"                      test :\", f\"{zero_row_pct(Xte):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3301dac4-8f51-41e4-9e91-760c5e8baa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post: georgina.gigi-1824168801363440426\n",
      "Top-10 termini: [('party', 0.719), ('green', 0.368), ('pink', 0.36), ('remember', 0.359), ('well', 0.299)]\n"
     ]
    }
   ],
   "source": [
    "# TOP K TERMS in a caption (example)\n",
    "feat = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "i = np.random.randint(0, Xtr.shape[0])  # un post random del train\n",
    "row = Xtr[i]\n",
    "coo = row.tocoo()\n",
    "order = np.argsort(coo.data)[::-1][:10]\n",
    "print(\"Post:\", tr_ids[i])\n",
    "print(\"Top-10 termini:\", list(zip(feat[coo.col[order]], np.round(coo.data[order], 3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "242be9c0-a0c7-40c3-953b-e65b1368f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "['aa' 'aaa' 'aaaaaand' 'aaaaand' 'aaaand' 'aaand' 'aaddephotography'\n",
      " 'aafamily' 'aaliyah' 'aand' 'aaqshah' 'aaqshahbrides' 'aarhus' 'aaron'\n",
      " 'aavi' 'ab' 'abandon' 'abandoned' 'abandonedplaces' 'abaya' 'abba'\n",
      " 'abbey' 'abbiamo' 'abbiethevizsla' 'abbotsford']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "feat = vectorizer.get_feature_names_out()\n",
    "print(len(feat))        # number of features\n",
    "print(feat[:25])        # first 25\n",
    "print(feat[50000:50010]) # other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65d1470c-5503-4fea-99f2-e4cad46478a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec719c9188604da99b292f593f318efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             post_id category\n",
      "0    jessieonair-1832772436464878417    other\n",
      "1  jess_soothill-1814050234055839189  fashion\n",
      "2  jess_soothill-1815434266849071530  fashion\n",
      "3  jess_soothill-1816220883029348816  fashion\n",
      "4  jess_soothill-1817582292300511649  fashion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beauty': ['makeup',\n",
       "  'palette',\n",
       "  'hair',\n",
       "  'beauty',\n",
       "  'look',\n",
       "  'lashes',\n",
       "  'lipstick',\n",
       "  'foundation',\n",
       "  'brow',\n",
       "  'lips'],\n",
       " 'family': ['little',\n",
       "  'kids',\n",
       "  'baby',\n",
       "  'family',\n",
       "  'fun',\n",
       "  'year',\n",
       "  'motherhood',\n",
       "  'us',\n",
       "  'first',\n",
       "  'weekend'],\n",
       " 'fashion': ['ootd',\n",
       "  'fashion',\n",
       "  'outfit',\n",
       "  'look',\n",
       "  'style',\n",
       "  'summer',\n",
       "  'weekend',\n",
       "  'shop',\n",
       "  'ad',\n",
       "  'dress'],\n",
       " 'fitness': ['fitness',\n",
       "  'workout',\n",
       "  'yoga',\n",
       "  'gym',\n",
       "  'body',\n",
       "  'week',\n",
       "  'fit',\n",
       "  'work',\n",
       "  'training',\n",
       "  'motivation'],\n",
       " 'food': ['recipe',\n",
       "  'food',\n",
       "  'foodie',\n",
       "  'delicious',\n",
       "  'breakfast',\n",
       "  'chocolate',\n",
       "  'foodporn',\n",
       "  'instafood',\n",
       "  'chicken',\n",
       "  'made'],\n",
       " 'interior': ['home',\n",
       "  'interiordesign',\n",
       "  'room',\n",
       "  'interior',\n",
       "  'design',\n",
       "  'little',\n",
       "  'house',\n",
       "  'homedecor',\n",
       "  'kitchen',\n",
       "  'weekend'],\n",
       " 'other': ['art',\n",
       "  'us',\n",
       "  'thank',\n",
       "  'bio',\n",
       "  'great',\n",
       "  'night',\n",
       "  'thanks',\n",
       "  'fun',\n",
       "  'ad',\n",
       "  'year'],\n",
       " 'pet': ['dog',\n",
       "  'dogs',\n",
       "  'dogsofinstagram',\n",
       "  'cats',\n",
       "  'puppy',\n",
       "  'weeklyfluff',\n",
       "  'cat',\n",
       "  'weekend',\n",
       "  'cute',\n",
       "  'use'],\n",
       " 'travel': ['travel',\n",
       "  'summer',\n",
       "  'photo',\n",
       "  'nature',\n",
       "  'sunset',\n",
       "  'weekend',\n",
       "  'place',\n",
       "  'city',\n",
       "  'beach',\n",
       "  'world']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESTRAZIONE TOP 10 PAROLE PER CATEGORIA\n",
    "\n",
    "# Setup\n",
    "\n",
    "DB_PATH   = r\"D:/db/meta.duckdb\"\n",
    "\n",
    "# Connection\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")\n",
    "\n",
    "cat = con.sql(\"\"\"SELECT post_id, category FROM md1718\"\"\").df()\n",
    "\n",
    "# dataframe train con id in ordine del TF-IDF\n",
    "df_train_ids = pd.DataFrame({'post_id': train_ids})\n",
    "df_train = df_train_ids.merge(cat, on='post_id', how='left')\n",
    "print(df_train.head())\n",
    "\n",
    "# Feature names out of the matrix\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "def top_k_words_for_category(X, categories, feature_names, k=10):\n",
    "    results = {}\n",
    "\n",
    "    for cat in sorted(np.unique(categories)):\n",
    "        idx = np.where(categories == cat)[0]\n",
    "\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        \n",
    "        # subset righe del training per quella categoria\n",
    "        X_cat = X[idx]\n",
    "\n",
    "        # somma TF-IDF della categoria\n",
    "        tfidf_sum = np.asarray(X_cat.sum(axis=0)).flatten()\n",
    "\n",
    "        # top k colonne\n",
    "        top_idx = tfidf_sum.argsort()[::-1][:k]\n",
    "\n",
    "        results[cat] = list(feature_names[top_idx])\n",
    "\n",
    "    return results\n",
    "\n",
    "#Extract top words from the train set\n",
    "categories_train = df_train['category'].values\n",
    "\n",
    "top_words = top_k_words_for_category(\n",
    "    Xtr,\n",
    "    categories_train,\n",
    "    feature_names,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399ee56c-3430-401e-9f69-a3825e95f15d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aaaaaand', 'aaaaand', 'aaaand', 'aaand',\n",
       "       'aaddephotography', 'aafamily', 'aaliyah', 'aand', 'aaqshah',\n",
       "       'aaqshahbrides', 'aarhus', 'aaron', 'aavi', 'ab', 'abandon',\n",
       "       'abandoned', 'abandonedplaces', 'abaya', 'abba', 'abbey',\n",
       "       'abbiamo', 'abbiethevizsla', 'abbotsford', 'abby', 'abbysatlas',\n",
       "       'abbywoodwear', 'abc', 'abcoasia', 'abcommunity', 'abcs',\n",
       "       'abdomen', 'abdominal', 'abeautifulmess', 'abeautyedit', 'abel',\n",
       "       'abend', 'abenteuervorderhaustuer', 'aber', 'abercrombie',\n",
       "       'aberdeen', 'abessinier', 'abh', 'abhbronzer', 'abhbrows',\n",
       "       'abhbrowwiz', 'abhcontourkit', 'abhcosmetics', 'abhdipbrow',\n",
       "       'abhfam', 'abhfoundation', 'abhglowkit', 'abhjunkies',\n",
       "       'abhliquidlipstick', 'abhlook', 'abhmodernrenaissance', 'abhprism',\n",
       "       'abhprsearch', 'abhshadows', 'abhsoftglam', 'abhsubculture',\n",
       "       'abhxamrezy', 'abhxamrezyhighlighter', 'abhxnorvina', 'abide',\n",
       "       'abigail', 'abigailahern', 'abilities', 'ability', 'abit', 'abito',\n",
       "       'abitohjem', 'ablaze', 'able', 'abmathome', 'abmcrafty',\n",
       "       'abmfoodie', 'abmhappy', 'abmhappyhour', 'abmhappylife',\n",
       "       'abmholidayspirit', 'abmhome', 'abmlife', 'abmlifeisbeautiful',\n",
       "       'abmlifeiscolorful', 'abmlifeiscolourful', 'abmlifeissweet',\n",
       "       'abmlittlethings', 'abmplantlady', 'abmstyle', 'abmtravel',\n",
       "       'abmtravelbug', 'abnormal', 'aboard', 'abode', 'abound',\n",
       "       'aboutalook', 'aboutlastnight', 'aboutme', 'aboutus', 'abparks',\n",
       "       'abparksambassador', 'abraham', 'abroad', 'abrushwithcolour',\n",
       "       'abs', 'absaremadeinthekitchen', 'absbydags', 'absence', 'absent',\n",
       "       'absolu', 'absolut', 'absolute', 'absolutely',\n",
       "       'absolutelyscottsdale', 'absorb', 'absorbed', 'absorbent',\n",
       "       'absorbing', 'absorbs', 'absorption', 'abstract', 'abstractart',\n",
       "       'abstractnetflix', 'absurd', 'absworkout', 'abt', 'abu',\n",
       "       'abudhabi', 'abuela', 'abundance', 'abundant', 'abundantly',\n",
       "       'abuse', 'abused', 'abusive', 'abworkout', 'abyss', 'ac',\n",
       "       'academic', 'academy', 'academyawards', 'acadia', 'acai',\n",
       "       'acaibowl', 'acatalogofbooks', 'acba', 'acc', 'accelerate',\n",
       "       'accent', 'accentchair', 'accented', 'accents', 'accentuate',\n",
       "       'accentwall', 'accept', 'acceptable', 'acceptance', 'accepted',\n",
       "       'accepting', 'accepts', 'accesories', 'access', 'accessible',\n",
       "       'accessoires', 'accessori', 'accessoricasa', 'accessoricasadesign',\n",
       "       'accessories', 'accessorize', 'accessorizing', 'accessory',\n",
       "       'accident', 'accidental', 'accidentally',\n",
       "       'accidentallywesanderson', 'accidently', 'accidents', 'acclaimed',\n",
       "       'accolades', 'accommodate', 'accommodating', 'accommodation',\n",
       "       'accommodations', 'accomodation', 'accompanied', 'accompaniment',\n",
       "       'accompany', 'accompanying', 'accomplish', 'accomplished',\n",
       "       'accomplishing', 'accomplishment', 'accomplishments', 'accord',\n",
       "       'according', 'accordingly', 'account', 'accountability'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8009b74a-1515-4519-9bb5-40867d87a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of words with no duplicates\n",
    "words_for_features = sorted({w for lst in top_words.values() for w in lst})\n",
    "\n",
    "# Map, word - index column TF-IDF\n",
    "# It simplifies the creation of the boolean feature, as it directly looks at the word column to understand whether it is present or not\n",
    "word_to_idx = {\n",
    "    w: np.where(feature_names == w)[0][0]\n",
    "    for w in words_for_features\n",
    "    if w in feature_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12830d55-a796-4e68-8556-f5666a793c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad': 325,\n",
       " 'art': 1874,\n",
       " 'baby': 2595,\n",
       " 'beach': 3291,\n",
       " 'beauty': 3476,\n",
       " 'bio': 4261,\n",
       " 'body': 4905,\n",
       " 'breakfast': 5576,\n",
       " 'brow': 5854,\n",
       " 'cat': 7007,\n",
       " 'cats': 7083,\n",
       " 'chicken': 7725,\n",
       " 'chocolate': 7908,\n",
       " 'city': 8161,\n",
       " 'cute': 10393,\n",
       " 'delicious': 11163,\n",
       " 'design': 11359,\n",
       " 'dog': 12167,\n",
       " 'dogs': 12289,\n",
       " 'dogsofinstagram': 12351,\n",
       " 'dress': 12813,\n",
       " 'family': 14859,\n",
       " 'fashion': 15038,\n",
       " 'first': 15721,\n",
       " 'fit': 15765,\n",
       " 'fitness': 15846,\n",
       " 'food': 16410,\n",
       " 'foodie': 16479,\n",
       " 'foodporn': 16544,\n",
       " 'foundation': 16747,\n",
       " 'fun': 17231,\n",
       " 'great': 18793,\n",
       " 'gym': 19175,\n",
       " 'hair': 19262,\n",
       " 'home': 20621,\n",
       " 'homedecor': 20655,\n",
       " 'house': 20984,\n",
       " 'instafood': 22492,\n",
       " 'interior': 22798,\n",
       " 'interiordesign': 22815,\n",
       " 'kids': 24114,\n",
       " 'kitchen': 24312,\n",
       " 'lashes': 24852,\n",
       " 'lips': 25695,\n",
       " 'lipstick': 25700,\n",
       " 'little': 25762,\n",
       " 'look': 26144,\n",
       " 'made': 26804,\n",
       " 'makeup': 26981,\n",
       " 'motherhood': 29354,\n",
       " 'motivation': 29408,\n",
       " 'nature': 30523,\n",
       " 'night': 30992,\n",
       " 'ootd': 32013,\n",
       " 'outfit': 32365,\n",
       " 'palette': 32653,\n",
       " 'photo': 33690,\n",
       " 'place': 34104,\n",
       " 'puppy': 35703,\n",
       " 'recipe': 36482,\n",
       " 'room': 37602,\n",
       " 'shop': 39495,\n",
       " 'style': 42132,\n",
       " 'summer': 42408,\n",
       " 'sunset': 42584,\n",
       " 'thank': 43797,\n",
       " 'thanks': 43807,\n",
       " 'training': 45194,\n",
       " 'travel': 45275,\n",
       " 'us': 46549,\n",
       " 'use': 46565,\n",
       " 'week': 48141,\n",
       " 'weekend': 48144,\n",
       " 'weeklyfluff': 48175,\n",
       " 'work': 49143,\n",
       " 'workout': 49187,\n",
       " 'world': 49220,\n",
       " 'year': 49497,\n",
       " 'yoga': 49572}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2bc59bf-9f78-4420-b160-66fc4232a3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773497, 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bool_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9573ffd-aeb8-47ca-a10c-7a41d71f4f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412325, 79)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bool_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d22dc7d-f3ab-4bbc-9f59-08cb8c1b6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423604, 79)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bool_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65346025-238d-422b-af6a-c3fbdda70b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c3543c5-645d-46db-8873-717c0d47b547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'art',\n",
       " 'baby',\n",
       " 'beach',\n",
       " 'beauty',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'breakfast',\n",
       " 'brow',\n",
       " 'cat',\n",
       " 'cats',\n",
       " 'chicken',\n",
       " 'chocolate',\n",
       " 'city',\n",
       " 'cute',\n",
       " 'delicious',\n",
       " 'design',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'dogsofinstagram',\n",
       " 'dress',\n",
       " 'family',\n",
       " 'fashion',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'fitness',\n",
       " 'food',\n",
       " 'foodie',\n",
       " 'foodporn',\n",
       " 'foundation',\n",
       " 'fun',\n",
       " 'great',\n",
       " 'gym',\n",
       " 'hair',\n",
       " 'home',\n",
       " 'homedecor',\n",
       " 'house',\n",
       " 'instafood',\n",
       " 'interior',\n",
       " 'interiordesign',\n",
       " 'kids',\n",
       " 'kitchen',\n",
       " 'lashes',\n",
       " 'lips',\n",
       " 'lipstick',\n",
       " 'little',\n",
       " 'look',\n",
       " 'made',\n",
       " 'makeup',\n",
       " 'motherhood',\n",
       " 'motivation',\n",
       " 'nature',\n",
       " 'night',\n",
       " 'ootd',\n",
       " 'outfit',\n",
       " 'palette',\n",
       " 'photo',\n",
       " 'place',\n",
       " 'puppy',\n",
       " 'recipe',\n",
       " 'room',\n",
       " 'shop',\n",
       " 'style',\n",
       " 'summer',\n",
       " 'sunset',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'training',\n",
       " 'travel',\n",
       " 'us',\n",
       " 'use',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeklyfluff',\n",
       " 'work',\n",
       " 'workout',\n",
       " 'world',\n",
       " 'year',\n",
       " 'yoga']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_for_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0bd9335-1cc2-4e9e-9d2f-14e1776d1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a df with boolean features\n",
    "# The index of the word, allows us to easily identify if the word is present or not (X[:, idx] > 0)\n",
    "def build_bool_features(X, word_to_idx, prefix=\"\"):\n",
    "    out = {}\n",
    "    for w, idx in word_to_idx.items():\n",
    "        out[f\"{prefix}has_{w}\"] = (X[:, idx] > 0).astype(int).toarray().ravel()\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4d31299-c3af-48e8-b75c-af177c282c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bool_train = build_bool_features(Xtr, word_to_idx, prefix=\"tfidf_\")\n",
    "df_bool_val   = build_bool_features(Xva, word_to_idx, prefix=\"tfidf_\")\n",
    "df_bool_test  = build_bool_features(Xte, word_to_idx, prefix=\"tfidf_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73f71bc2-3c58-4a58-87fa-9a47811d9b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_has_ad</th>\n",
       "      <th>tfidf_has_art</th>\n",
       "      <th>tfidf_has_baby</th>\n",
       "      <th>tfidf_has_beach</th>\n",
       "      <th>tfidf_has_beauty</th>\n",
       "      <th>tfidf_has_bio</th>\n",
       "      <th>tfidf_has_body</th>\n",
       "      <th>tfidf_has_breakfast</th>\n",
       "      <th>tfidf_has_brow</th>\n",
       "      <th>tfidf_has_cat</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_has_us</th>\n",
       "      <th>tfidf_has_use</th>\n",
       "      <th>tfidf_has_week</th>\n",
       "      <th>tfidf_has_weekend</th>\n",
       "      <th>tfidf_has_weeklyfluff</th>\n",
       "      <th>tfidf_has_work</th>\n",
       "      <th>tfidf_has_workout</th>\n",
       "      <th>tfidf_has_world</th>\n",
       "      <th>tfidf_has_year</th>\n",
       "      <th>tfidf_has_yoga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_has_ad  tfidf_has_art  tfidf_has_baby  tfidf_has_beach  \\\n",
       "0             0              0               0                0   \n",
       "1             0              0               0                0   \n",
       "2             0              0               0                0   \n",
       "3             0              0               0                0   \n",
       "4             0              0               0                0   \n",
       "\n",
       "   tfidf_has_beauty  tfidf_has_bio  tfidf_has_body  tfidf_has_breakfast  \\\n",
       "0                 0              0               0                    0   \n",
       "1                 0              0               0                    0   \n",
       "2                 0              0               0                    0   \n",
       "3                 0              0               0                    0   \n",
       "4                 0              0               0                    0   \n",
       "\n",
       "   tfidf_has_brow  tfidf_has_cat  ...  tfidf_has_us  tfidf_has_use  \\\n",
       "0               0              0  ...             0              0   \n",
       "1               0              0  ...             0              0   \n",
       "2               0              0  ...             0              0   \n",
       "3               0              0  ...             1              0   \n",
       "4               0              0  ...             0              0   \n",
       "\n",
       "   tfidf_has_week  tfidf_has_weekend  tfidf_has_weeklyfluff  tfidf_has_work  \\\n",
       "0               0                  1                      0               0   \n",
       "1               0                  0                      0               0   \n",
       "2               0                  0                      0               0   \n",
       "3               1                  0                      0               0   \n",
       "4               0                  0                      0               0   \n",
       "\n",
       "   tfidf_has_workout  tfidf_has_world  tfidf_has_year  tfidf_has_yoga  \n",
       "0                  0                0               0               0  \n",
       "1                  0                0               1               0  \n",
       "2                  0                0               0               0  \n",
       "3                  0                0               0               0  \n",
       "4                  0                0               0               0  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bool_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f7dbd432-4f84-48d7-a40b-27bd86c028ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 773497 entries, 0 to 773496\n",
      "Data columns (total 79 columns):\n",
      " #   Column                     Non-Null Count   Dtype\n",
      "---  ------                     --------------   -----\n",
      " 0   tfidf_has_ad               773497 non-null  int32\n",
      " 1   tfidf_has_art              773497 non-null  int32\n",
      " 2   tfidf_has_baby             773497 non-null  int32\n",
      " 3   tfidf_has_beach            773497 non-null  int32\n",
      " 4   tfidf_has_beauty           773497 non-null  int32\n",
      " 5   tfidf_has_bio              773497 non-null  int32\n",
      " 6   tfidf_has_body             773497 non-null  int32\n",
      " 7   tfidf_has_breakfast        773497 non-null  int32\n",
      " 8   tfidf_has_brow             773497 non-null  int32\n",
      " 9   tfidf_has_cat              773497 non-null  int32\n",
      " 10  tfidf_has_cats             773497 non-null  int32\n",
      " 11  tfidf_has_chicken          773497 non-null  int32\n",
      " 12  tfidf_has_chocolate        773497 non-null  int32\n",
      " 13  tfidf_has_city             773497 non-null  int32\n",
      " 14  tfidf_has_cute             773497 non-null  int32\n",
      " 15  tfidf_has_delicious        773497 non-null  int32\n",
      " 16  tfidf_has_design           773497 non-null  int32\n",
      " 17  tfidf_has_dog              773497 non-null  int32\n",
      " 18  tfidf_has_dogs             773497 non-null  int32\n",
      " 19  tfidf_has_dogsofinstagram  773497 non-null  int32\n",
      " 20  tfidf_has_dress            773497 non-null  int32\n",
      " 21  tfidf_has_family           773497 non-null  int32\n",
      " 22  tfidf_has_fashion          773497 non-null  int32\n",
      " 23  tfidf_has_first            773497 non-null  int32\n",
      " 24  tfidf_has_fit              773497 non-null  int32\n",
      " 25  tfidf_has_fitness          773497 non-null  int32\n",
      " 26  tfidf_has_food             773497 non-null  int32\n",
      " 27  tfidf_has_foodie           773497 non-null  int32\n",
      " 28  tfidf_has_foodporn         773497 non-null  int32\n",
      " 29  tfidf_has_foundation       773497 non-null  int32\n",
      " 30  tfidf_has_fun              773497 non-null  int32\n",
      " 31  tfidf_has_great            773497 non-null  int32\n",
      " 32  tfidf_has_gym              773497 non-null  int32\n",
      " 33  tfidf_has_hair             773497 non-null  int32\n",
      " 34  tfidf_has_home             773497 non-null  int32\n",
      " 35  tfidf_has_homedecor        773497 non-null  int32\n",
      " 36  tfidf_has_house            773497 non-null  int32\n",
      " 37  tfidf_has_instafood        773497 non-null  int32\n",
      " 38  tfidf_has_interior         773497 non-null  int32\n",
      " 39  tfidf_has_interiordesign   773497 non-null  int32\n",
      " 40  tfidf_has_kids             773497 non-null  int32\n",
      " 41  tfidf_has_kitchen          773497 non-null  int32\n",
      " 42  tfidf_has_lashes           773497 non-null  int32\n",
      " 43  tfidf_has_lips             773497 non-null  int32\n",
      " 44  tfidf_has_lipstick         773497 non-null  int32\n",
      " 45  tfidf_has_little           773497 non-null  int32\n",
      " 46  tfidf_has_look             773497 non-null  int32\n",
      " 47  tfidf_has_made             773497 non-null  int32\n",
      " 48  tfidf_has_makeup           773497 non-null  int32\n",
      " 49  tfidf_has_motherhood       773497 non-null  int32\n",
      " 50  tfidf_has_motivation       773497 non-null  int32\n",
      " 51  tfidf_has_nature           773497 non-null  int32\n",
      " 52  tfidf_has_night            773497 non-null  int32\n",
      " 53  tfidf_has_ootd             773497 non-null  int32\n",
      " 54  tfidf_has_outfit           773497 non-null  int32\n",
      " 55  tfidf_has_palette          773497 non-null  int32\n",
      " 56  tfidf_has_photo            773497 non-null  int32\n",
      " 57  tfidf_has_place            773497 non-null  int32\n",
      " 58  tfidf_has_puppy            773497 non-null  int32\n",
      " 59  tfidf_has_recipe           773497 non-null  int32\n",
      " 60  tfidf_has_room             773497 non-null  int32\n",
      " 61  tfidf_has_shop             773497 non-null  int32\n",
      " 62  tfidf_has_style            773497 non-null  int32\n",
      " 63  tfidf_has_summer           773497 non-null  int32\n",
      " 64  tfidf_has_sunset           773497 non-null  int32\n",
      " 65  tfidf_has_thank            773497 non-null  int32\n",
      " 66  tfidf_has_thanks           773497 non-null  int32\n",
      " 67  tfidf_has_training         773497 non-null  int32\n",
      " 68  tfidf_has_travel           773497 non-null  int32\n",
      " 69  tfidf_has_us               773497 non-null  int32\n",
      " 70  tfidf_has_use              773497 non-null  int32\n",
      " 71  tfidf_has_week             773497 non-null  int32\n",
      " 72  tfidf_has_weekend          773497 non-null  int32\n",
      " 73  tfidf_has_weeklyfluff      773497 non-null  int32\n",
      " 74  tfidf_has_work             773497 non-null  int32\n",
      " 75  tfidf_has_workout          773497 non-null  int32\n",
      " 76  tfidf_has_world            773497 non-null  int32\n",
      " 77  tfidf_has_year             773497 non-null  int32\n",
      " 78  tfidf_has_yoga             773497 non-null  int32\n",
      "dtypes: int32(79)\n",
      "memory usage: 233.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_bool_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39c2ce01-0077-48bc-ac65-418ac3d2f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert in sparse\n",
    "Xtr_bool = csr_matrix(df_bool_train.values.astype(np.float32))\n",
    "Xva_bool = csr_matrix(df_bool_val.values.astype(np.float32))\n",
    "Xte_bool = csr_matrix(df_bool_test.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcf27ea4-1702-4603-a8b4-b89db5ded7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773497, 79)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_bool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b316ddc-b6d8-4770-991f-e8f0bc508a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train full shape: (773497, 50079)\n",
      "Val   full shape: (412325, 50079)\n",
      "Test  full shape: (423604, 50079)\n"
     ]
    }
   ],
   "source": [
    "# Add the boolean variables to the matrix\n",
    "Xtr_full = hstack([Xtr, Xtr_bool])\n",
    "Xva_full = hstack([Xva, Xva_bool])\n",
    "Xte_full = hstack([Xte, Xte_bool])\n",
    "\n",
    "print(\"Train full shape:\", Xtr_full.shape)\n",
    "print(\"Val   full shape:\", Xva_full.shape)\n",
    "print(\"Test  full shape:\", Xte_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1819caa-5bee-46b4-8cff-d3de57ae79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = r\"D:/dataset/text_features/tfidf_v3\"\n",
    "\n",
    "save_npz(f\"{OUT_DIR}/tfidf_topwords_train.npz\", Xtr_full)\n",
    "save_npz(f\"{OUT_DIR}/tfidf_topwords_val.npz\",   Xva_full)\n",
    "save_npz(f\"{OUT_DIR}/tfidf_topwords_test.npz\",  Xte_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4ad5f7-310f-43d7-879e-48b642bf72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = r\"D:/dataset/text_features/tfidf_v3\"\n",
    "\n",
    "Xtr_full = load_npz(f\"{OUT_DIR}/tfidf_topwords_train.npz\").astype(np.float32)\n",
    "Xva_full = load_npz(f\"{OUT_DIR}/tfidf_topwords_val.npz\").astype(np.float32)\n",
    "\n",
    "tr_ids = np.load(os.path.join(OUT_DIR, \"tfidf_train_post_ids.npy\"), allow_pickle=True)\n",
    "va_ids = np.load(os.path.join(OUT_DIR, \"tfidf_val_post_ids.npy\"), allow_pickle=True)\n",
    "\n",
    "y_tr = np.load(os.path.join(OUT_DIR, \"tfidf_train_y.npy\"), allow_pickle = True)\n",
    "y_va = np.load(os.path.join(OUT_DIR, \"tfidf_val_y.npy\"), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6f1b69-790a-49d9-9e94-0442d0517965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.1786 | accuracy (val): 0.2396\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.1857 | accuracy (val): 0.2414\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.1942 | accuracy (val): 0.2432\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.2048 | accuracy (val): 0.2457\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-06}\n",
      "Validation macro-F1: 0.20482327670028494\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "3   1.000000e-06      0.204823      0.245736\n",
      "2   1.000000e-07      0.194223      0.243184\n",
      "1   1.000000e-08      0.185663      0.241402\n",
      "0   1.000000e-09      0.178621      0.239585\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "param_grid = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "batch_size = 256\n",
    "classes = np.unique(y_tr)\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit the model using minibatch for memory\n",
    "    for start in range(0, Xtr_full.shape[0], batch_size):\n",
    "        # print(f\"Batch {start} fit\")\n",
    "        end = min(start + batch_size, Xtr_full.shape[0])\n",
    "\n",
    "        Xb = Xtr_full[start:end].toarray()\n",
    "        yb = y_tr[start:end]\n",
    "\n",
    "        if start == 0:\n",
    "            clf.partial_fit(Xb, yb, classes=classes)\n",
    "        else:\n",
    "            clf.partial_fit(Xb, yb)\n",
    "\n",
    "        del Xb, yb\n",
    "        gc.collect()\n",
    "\n",
    "    # Predict using minibatches\n",
    "    y_val_pred = []\n",
    "\n",
    "    for start in range(0, Xva_full.shape[0], batch_size):\n",
    "        # print(f\"Batch {start} predict\")\n",
    "        end = min(start + batch_size, Xva_full.shape[0])\n",
    "\n",
    "        Xb = Xva_full[start:end].toarray()\n",
    "        y_val_pred.append(clf.predict(Xb))\n",
    "\n",
    "        del Xb\n",
    "        gc.collect()\n",
    "\n",
    "    y_val_pred = np.concatenate(y_val_pred)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\n",
    "    \"val_macro_f1\", ascending=False\n",
    ")\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0422feed-531f-4f6f-858f-83f5d28c6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.2549392831142973 | accuracy (val): 0.2574934820833081\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2515283705585684 | accuracy (val): 0.262855756987813\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.2605012964347888 | accuracy (val): 0.26534408536955073\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2580914164255799 | accuracy (val): 0.2693821621293882\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.25280690427831304 | accuracy (val): 0.25670041835930396\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.25045347173834576 | accuracy (val): 0.2593924695325289\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.2597298522188484 | accuracy (val): 0.26483235311950526\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2568884029154065 | accuracy (val): 0.2684678348390226\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.25228411342318874 | accuracy (val): 0.25764869944825075\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.24945290586958926 | accuracy (val): 0.2601249014733523\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.2595746199981532 | accuracy (val): 0.2641411507912448\n",
      "\n",
      "Combination: {'alpha': 0.001, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2567125720310076 | accuracy (val): 0.26797307948826776\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': False, 'class_weight': None}\n",
      "macro-F1 (val): 0.25140067661564847 | accuracy (val): 0.25373916206875646\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': False, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2492596030806366 | accuracy (val): 0.2564190868853453\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': True, 'class_weight': None}\n",
      "macro-F1 (val): 0.2560012457148924 | accuracy (val): 0.25784272115442913\n",
      "\n",
      "Combination: {'alpha': 0.01, 'average': True, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2549700240495776 | accuracy (val): 0.2616212938822531\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 1e-05, 'average': True, 'class_weight': None}\n",
      "Validation macro-F1: 0.2605012964347888\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "      alpha class_weight  average  val_macro_f1  val_accuracy\n",
      "2   0.00001         None     True      0.260501      0.265344\n",
      "6   0.00010         None     True      0.259730      0.264832\n",
      "10  0.00100         None     True      0.259575      0.264141\n",
      "3   0.00001     balanced     True      0.258091      0.269382\n",
      "7   0.00010     balanced     True      0.256888      0.268468\n",
      "11  0.00100     balanced     True      0.256713      0.267973\n",
      "14  0.01000         None     True      0.256001      0.257843\n",
      "15  0.01000     balanced     True      0.254970      0.261621\n",
      "0   0.00001         None    False      0.254939      0.257493\n",
      "4   0.00010         None    False      0.252807      0.256700\n",
      "8   0.00100         None    False      0.252284      0.257649\n",
      "1   0.00001     balanced    False      0.251528      0.262856\n",
      "12  0.01000         None    False      0.251401      0.253739\n",
      "5   0.00010     balanced    False      0.250453      0.259392\n",
      "9   0.00100     balanced    False      0.249453      0.260125\n",
      "13  0.01000     balanced    False      0.249260      0.256419\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(Xtr_full, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(Xva_full)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1bb0083-5090-4bf3-bd7f-cf0e096a6ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1358 | accuracy (val): 0.2089\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1347 | accuracy (val): 0.2078\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1346 | accuracy (val): 0.2079\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1333 | accuracy (val): 0.2082\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1347 | accuracy (val): 0.2080\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1347 | accuracy (val): 0.2080\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1324 | accuracy (val): 0.2074\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1341 | accuracy (val): 0.2077\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1334 | accuracy (val): 0.2073\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1326 | accuracy (val): 0.2075\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1338 | accuracy (val): 0.2076\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1323 | accuracy (val): 0.2077\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1440 | accuracy (val): 0.2119\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1415 | accuracy (val): 0.2106\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1417 | accuracy (val): 0.2107\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1446 | accuracy (val): 0.2119\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1419 | accuracy (val): 0.2107\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1423 | accuracy (val): 0.2109\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1373 | accuracy (val): 0.2084\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1371 | accuracy (val): 0.2086\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1375 | accuracy (val): 0.2088\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1374 | accuracy (val): 0.2080\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1374 | accuracy (val): 0.2085\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1379 | accuracy (val): 0.2087\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1455 | accuracy (val): 0.2125\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1436 | accuracy (val): 0.2115\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1444 | accuracy (val): 0.2116\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1457 | accuracy (val): 0.2124\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1439 | accuracy (val): 0.2115\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1453 | accuracy (val): 0.2120\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1380 | accuracy (val): 0.2086\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1382 | accuracy (val): 0.2089\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1395 | accuracy (val): 0.2097\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.1389 | accuracy (val): 0.2086\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.1392 | accuracy (val): 0.2090\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.1396 | accuracy (val): 0.2094\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "Validation macro-F1: 0.14567243680288477\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "27            30         12                 5         0.05      0.145672   \n",
      "24            30         12                 2         0.05      0.145471   \n",
      "29            80         12                 5         0.05      0.145332   \n",
      "15            30         10                 5         0.05      0.144551   \n",
      "26            80         12                 2         0.05      0.144350   \n",
      "12            30         10                 2         0.05      0.143955   \n",
      "28            50         12                 5         0.05      0.143937   \n",
      "25            50         12                 2         0.05      0.143599   \n",
      "17            80         10                 5         0.05      0.142312   \n",
      "16            50         10                 5         0.05      0.141923   \n",
      "14            80         10                 2         0.05      0.141702   \n",
      "13            50         10                 2         0.05      0.141495   \n",
      "35            80         12                 5         sqrt      0.139561   \n",
      "32            80         12                 2         sqrt      0.139513   \n",
      "34            50         12                 5         sqrt      0.139165   \n",
      "33            30         12                 5         sqrt      0.138939   \n",
      "31            50         12                 2         sqrt      0.138219   \n",
      "30            30         12                 2         sqrt      0.138042   \n",
      "23            80         10                 5         sqrt      0.137878   \n",
      "20            80         10                 2         sqrt      0.137465   \n",
      "21            30         10                 5         sqrt      0.137407   \n",
      "22            50         10                 5         sqrt      0.137366   \n",
      "18            30         10                 2         sqrt      0.137316   \n",
      "19            50         10                 2         sqrt      0.137058   \n",
      "0             30          8                 2         0.05      0.135771   \n",
      "1             50          8                 2         0.05      0.134749   \n",
      "4             50          8                 5         0.05      0.134705   \n",
      "5             80          8                 5         0.05      0.134669   \n",
      "2             80          8                 2         0.05      0.134650   \n",
      "7             50          8                 2         sqrt      0.134065   \n",
      "10            50          8                 5         sqrt      0.133791   \n",
      "8             80          8                 2         sqrt      0.133411   \n",
      "3             30          8                 5         0.05      0.133335   \n",
      "9             30          8                 5         sqrt      0.132590   \n",
      "6             30          8                 2         sqrt      0.132438   \n",
      "11            80          8                 5         sqrt      0.132345   \n",
      "\n",
      "    val_accuracy  \n",
      "27      0.212391  \n",
      "24      0.212471  \n",
      "29      0.212024  \n",
      "15      0.211855  \n",
      "26      0.211573  \n",
      "12      0.211908  \n",
      "28      0.211535  \n",
      "25      0.211513  \n",
      "17      0.210856  \n",
      "16      0.210729  \n",
      "14      0.210715  \n",
      "13      0.210647  \n",
      "35      0.209412  \n",
      "32      0.209694  \n",
      "34      0.208983  \n",
      "33      0.208612  \n",
      "31      0.208947  \n",
      "30      0.208568  \n",
      "23      0.208665  \n",
      "20      0.208760  \n",
      "21      0.208049  \n",
      "22      0.208515  \n",
      "18      0.208379  \n",
      "19      0.208634  \n",
      "0       0.208884  \n",
      "1       0.207829  \n",
      "4       0.207991  \n",
      "5       0.207996  \n",
      "2       0.207935  \n",
      "7       0.207715  \n",
      "10      0.207608  \n",
      "8       0.207315  \n",
      "3       0.208159  \n",
      "9       0.207475  \n",
      "6       0.207402  \n",
      "11      0.207657  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST \n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(Xtr_full, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(Xva_full)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d377fc-b92f-4329-bf2e-85133b1db3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1638 | accuracy (val): 0.2191\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1762 | accuracy (val): 0.2227\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1760 | accuracy (val): 0.2227\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1900 | accuracy (val): 0.2267\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1659 | accuracy (val): 0.2196\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1787 | accuracy (val): 0.2224\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1755 | accuracy (val): 0.2227\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.1878 | accuracy (val): 0.2262\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.18998331340011443\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "3               0.5      0            0.1          6           150   \n",
      "7               0.5      1            0.1          6           150   \n",
      "5               0.5      1            0.1          4           150   \n",
      "1               0.5      0            0.1          4           150   \n",
      "2               0.5      0            0.1          6           100   \n",
      "6               0.5      1            0.1          6           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "0               0.5      0            0.1          4           100   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "3           1        0.8      0.189983      0.226676  \n",
      "7           1        0.8      0.187848      0.226198  \n",
      "5           1        0.8      0.178683      0.222419  \n",
      "1           1        0.8      0.176217      0.222671  \n",
      "2           1        0.8      0.176032      0.222739  \n",
      "6           1        0.8      0.175531      0.222725  \n",
      "4           1        0.8      0.165897      0.219555  \n",
      "0           1        0.8      0.163807      0.219065  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150],\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.5],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"reg_lambda\": [1],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    clf.fit(Xtr_full, y_tr_enc)\n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = clf.predict(Xva_full)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b9fe8-14cd-4214-8702-6c180b4da5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6e21c-1f6d-426b-b193-6031bc3751a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = r\"D:/dataset/text_features/tfidf_v3\"\n",
    "\n",
    "Xtr_full = load_npz(f\"{OUT_DIR}/tfidf_topwords_train.npz\").astype(np.float32)\n",
    "Xva_full = load_npz(f\"{OUT_DIR}/tfidf_topwords_val.npz\").astype(np.float32)\n",
    "Xte_full = load_npz(f\"{OUT_DIR}/tfidf_topwords_test.npz\").astype(np.float32)\n",
    "\n",
    "tr_ids = np.load(os.path.join(OUT_DIR, \"tfidf_train_post_ids.npy\"), allow_pickle=True)\n",
    "va_ids = np.load(os.path.join(OUT_DIR, \"tfidf_val_post_ids.npy\"), allow_pickle=True)\n",
    "te_ids = np.load(os.path.join(OUT_DIR, \"tfidf_test_post_ids.npy\"), allow_pickle=True)\n",
    "\n",
    "y_tr = np.load(os.path.join(OUT_DIR, \"tfidf_train_y.npy\"), allow_pickle = True)\n",
    "y_va = np.load(os.path.join(OUT_DIR, \"tfidf_val_y.npy\"), allow_pickle = True)\n",
    "y_te = np.load(os.path.join(OUT_DIR, \"tfidf_test_y.npy\"), allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e2d533-8c92-49cd-a466-e636646e5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = vstack([Xtr_full, Xva_full])\n",
    "y_full = np.concatenate([y_tr, y_va])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full_enc = le.fit_transform(y_full)\n",
    "y_te_enc = le.transform(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70430b87-5468-4b19-b6df-ce1f58238818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: LinearSVC(C=0.1, max_iter=5000, random_state=42)\n",
      "macro-F1 (test): 0.3006 | accuracy (test): 0.3080\n",
      "\n",
      "Configuration: BernoulliNB(alpha=10)\n",
      "macro-F1 (test): 0.2412 | accuracy (test): 0.2810\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=12, max_features=0.05, min_samples_leaf=5,\n",
      "                       n_estimators=30, n_jobs=-1, random_state=42)\n",
      "macro-F1 (test): 0.1676 | accuracy (test): 0.2198\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=0,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=150, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (test): 0.2209 | accuracy (test): 0.2452\n"
     ]
    }
   ],
   "source": [
    "cfgs = [\n",
    "    RandomForestClassifier(\n",
    "        max_depth=12, max_features=0.05, min_samples_leaf=5, n_estimators=30, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.1, max_depth= 6, n_estimators= 150, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_full_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_full, y_full_enc)\n",
    "        y_te_pred = cfg.predict(Xte_full)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_full, y_full)\n",
    "        y_te_pred = cfg.predict(Xte_full)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b9c55a-7535-4a1e-ae6a-1a6f0b01e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2927 | accuracy (test): 0.3034\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 1e-05,\n",
    "        average = True,\n",
    "        class_weight = None,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_full, y_full)\n",
    "y_te_pred = cfg.predict(Xte_full)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29106cb-68ce-4da1-8ad3-3f54a6b74453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2153 | accuracy (test): 0.2711\n"
     ]
    }
   ],
   "source": [
    "# TEST SU GAUSSIAN NAIVE BAYES\n",
    "\n",
    "batch_size = 256\n",
    "classes = np.unique(y_full)\n",
    "\n",
    "clf = GaussianNB(var_smoothing = 1e-06)\n",
    "\n",
    "\n",
    "# Fit the model using minibatch for memory\n",
    "for start in range(0, X_full.shape[0], batch_size):\n",
    "    # print(f\"Batch {start} fit\")\n",
    "    end = min(start + batch_size, X_full.shape[0])\n",
    "\n",
    "    Xb = X_full[start:end].toarray()\n",
    "    yb = y_full[start:end]\n",
    "\n",
    "    if start == 0:\n",
    "        clf.partial_fit(Xb, yb, classes=classes)\n",
    "    else:\n",
    "        clf.partial_fit(Xb, yb)\n",
    "\n",
    "    del Xb, yb\n",
    "    gc.collect()\n",
    "\n",
    "# Predict using minibatches\n",
    "y_te_pred = []\n",
    "\n",
    "for start in range(0, Xte_full.shape[0], batch_size):\n",
    "    # print(f\"Batch {start} predict\")\n",
    "    end = min(start + batch_size, Xte_full.shape[0])\n",
    "\n",
    "    Xb = Xte_full[start:end].toarray()\n",
    "    y_te_pred.append(clf.predict(Xb))\n",
    "\n",
    "    del Xb\n",
    "    gc.collect()\n",
    "\n",
    "y_te_pred = np.concatenate(y_te_pred)\n",
    "\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
