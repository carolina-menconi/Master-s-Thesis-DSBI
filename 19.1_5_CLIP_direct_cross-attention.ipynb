{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be69ca7-5a2c-4f51-bd17-01ed312382d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, duckdb, torch, timm, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import duckdb, torch\n",
    "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer\n",
    "\n",
    "from scipy.sparse import load_npz, hstack, save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5b2cf3-f23b-4c27-a89b-10cfbba29a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8273158-7195-4cd0-84c4-32f884d9bd6f",
   "metadata": {},
   "source": [
    "# TEXT TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436868df-b725-4a6c-957d-1eb9f10f00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3426e1-cd81-4394-88aa-34a206942080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name):\n",
    "    print(f\"Loading {split_name}...\")\n",
    "    df = con.sql(f\"\"\"\n",
    "        SELECT post_id, caption_bert_clip, er_bins\n",
    "        FROM clip_full_sample\n",
    "        WHERE split = '{split_name}'\n",
    "    \"\"\").df()\n",
    "    ids = df[\"post_id\"].to_numpy()\n",
    "    texts = df[\"caption_bert_clip\"].tolist()\n",
    "    y = df[\"er_bins\"]\n",
    "    del df; gc.collect()\n",
    "    print(f\"{split_name} done.\")\n",
    "    return ids, texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7896b5a6-48d3-4249-a568-71eaa611fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train...\n",
      "train done.\n",
      "Loading validation...\n",
      "validation done.\n",
      "Loading test...\n",
      "test done.\n"
     ]
    }
   ],
   "source": [
    "train_ids, Xtr_text, y_tr = load_split(\"train\")\n",
    "val_ids, Xva_text, y_val  = load_split(\"validation\")\n",
    "test_ids, Xte_text, y_te  = load_split(\"test\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2aa3533-51ad-4894-9031-28ed877e79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24993 5000 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(Xtr_text), len(Xva_text), len(Xte_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3139dc97-0211-4963-b9f1-c9114ed832c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_name)\n",
    "model = CLIPModel.from_pretrained(clip_name)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c6532a-c913-440b-84a2-8f05a6149025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_clip_text_tokens(texts, bs=64):\n",
    "    all_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), bs):\n",
    "            # print(f\" processing batch {i}/{len(texts)}\")\n",
    "            batch = texts[i:i+bs]\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            # text_model is used for text embeddings\n",
    "            out = model.text_model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # last_hidden_state allows to extract the last layer tokens, before they are pooled together (for the cross-attention fusion)\n",
    "            tokens = out.last_hidden_state\n",
    "\n",
    "            all_tokens.append(tokens.cpu())\n",
    "\n",
    "    return torch.cat(all_tokens, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791f217d-1022-4913-b300-2ae7d4a7956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sharded_cached_tokens(texts, ids, y, split_name, emb_dir, shard=5000):\n",
    "    \n",
    "    prefix = f\"{clip_name.split('/')[-1]}_TOKENS_{split_name}\"\n",
    "    out_files = []\n",
    "\n",
    "    n = len(texts)\n",
    "    assert len(ids) == n\n",
    "    assert len(y) == n\n",
    "\n",
    "    for i in range(0, n, shard):\n",
    "        f = emb_dir / f\"{prefix}_{i:07d}.npy\"\n",
    "        \n",
    "        if f.exists():\n",
    "            print(f\"[skip] {f.name}\")\n",
    "            out_files.append(f)\n",
    "            continue\n",
    "\n",
    "        part = texts[i:i+shard]\n",
    "        t0 = time.time()\n",
    "\n",
    "        T = embed_clip_text_tokens(part, bs=1)\n",
    "        T = T.numpy().astype(\"float32\")\n",
    "\n",
    "        np.save(f, T)\n",
    "        out_files.append(f)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[saved] {f.name}  [{i+len(part)}/{n}]  ({len(part)} samples in {dt:.1f}s)\")\n",
    "\n",
    "    arrays = [np.load(f) for f in out_files]\n",
    "    E_all = np.concatenate(arrays, axis=0).astype(\"float32\")\n",
    "\n",
    "    assert E_all.shape[0] == n, f\"Rows {E_all.shape[0]} != {n}\"\n",
    "\n",
    "    np.save(emb_dir / f\"{prefix}_ALL.npy\", E_all)\n",
    "\n",
    "    npz_path = emb_dir / f\"{prefix}_ids_y.npz\"\n",
    "    np.savez(\n",
    "        npz_path,\n",
    "        ids=np.asarray(ids),\n",
    "        embeddings=E_all,\n",
    "        y=np.asarray(y)\n",
    "    )\n",
    "\n",
    "    print(f\"[done] Saved ids + embeddings + y → {npz_path.name}\")\n",
    "\n",
    "    return E_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2155c9a7-2036-4ce5-aa78-4338f198cccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip-vit-base-patch32_TOKENS_train_0000000.npy  [1000/24993]  (1000 samples in 83.7s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0001000.npy  [2000/24993]  (1000 samples in 80.6s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0002000.npy  [3000/24993]  (1000 samples in 83.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0003000.npy  [4000/24993]  (1000 samples in 84.5s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0004000.npy  [5000/24993]  (1000 samples in 86.8s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0005000.npy  [6000/24993]  (1000 samples in 85.7s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0006000.npy  [7000/24993]  (1000 samples in 83.5s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0007000.npy  [8000/24993]  (1000 samples in 85.8s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0008000.npy  [9000/24993]  (1000 samples in 88.5s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0009000.npy  [10000/24993]  (1000 samples in 85.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0010000.npy  [11000/24993]  (1000 samples in 86.4s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0011000.npy  [12000/24993]  (1000 samples in 84.6s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0012000.npy  [13000/24993]  (1000 samples in 86.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0013000.npy  [14000/24993]  (1000 samples in 87.6s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0014000.npy  [15000/24993]  (1000 samples in 86.2s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0015000.npy  [16000/24993]  (1000 samples in 87.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0016000.npy  [17000/24993]  (1000 samples in 87.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0017000.npy  [18000/24993]  (1000 samples in 84.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0018000.npy  [19000/24993]  (1000 samples in 85.8s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0019000.npy  [20000/24993]  (1000 samples in 84.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0020000.npy  [21000/24993]  (1000 samples in 87.4s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0021000.npy  [22000/24993]  (1000 samples in 85.8s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0022000.npy  [23000/24993]  (1000 samples in 86.2s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0023000.npy  [24000/24993]  (1000 samples in 81.5s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_train_0024000.npy  [24993/24993]  (993 samples in 84.0s)\n",
      "[done] Saved ids + embeddings + y → clip-vit-base-patch32_TOKENS_train_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "emb_dir = Path('D:/dataset/clip_cross_attention_emb')\n",
    "\n",
    "Etr = embed_sharded_cached_tokens(\n",
    "    texts=Xtr_text,\n",
    "    ids=train_ids,\n",
    "    y=y_tr,\n",
    "    split_name=\"train\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard = 1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf61461-7ac3-442a-b00b-db8d748b1cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip-vit-base-patch32_TOKENS_validation_0000000.npy  [500/5000]  (500 samples in 45.8s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0000500.npy  [1000/5000]  (500 samples in 42.6s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0001000.npy  [1500/5000]  (500 samples in 42.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0001500.npy  [2000/5000]  (500 samples in 42.4s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0002000.npy  [2500/5000]  (500 samples in 39.4s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0002500.npy  [3000/5000]  (500 samples in 41.3s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0003000.npy  [3500/5000]  (500 samples in 45.1s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0003500.npy  [4000/5000]  (500 samples in 42.5s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0004000.npy  [4500/5000]  (500 samples in 42.9s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_validation_0004500.npy  [5000/5000]  (500 samples in 45.5s)\n",
      "[done] Saved ids + embeddings + y → clip-vit-base-patch32_TOKENS_validation_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "Eva = embed_sharded_cached_tokens(\n",
    "    texts=Xva_text,\n",
    "    ids=val_ids,\n",
    "    y=y_val,\n",
    "    split_name=\"validation\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0db65b-bc8d-4e29-b42a-e6dde2bd6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip-vit-base-patch32_TOKENS_test_0000000.npy  [500/5000]  (500 samples in 39.4s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0000500.npy  [1000/5000]  (500 samples in 41.7s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0001000.npy  [1500/5000]  (500 samples in 45.2s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0001500.npy  [2000/5000]  (500 samples in 47.1s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0002000.npy  [2500/5000]  (500 samples in 52.3s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0002500.npy  [3000/5000]  (500 samples in 56.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0003000.npy  [3500/5000]  (500 samples in 61.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0003500.npy  [4000/5000]  (500 samples in 58.1s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0004000.npy  [4500/5000]  (500 samples in 51.0s)\n",
      "[saved] clip-vit-base-patch32_TOKENS_test_0004500.npy  [5000/5000]  (500 samples in 47.1s)\n",
      "[done] Saved ids + embeddings + y → clip-vit-base-patch32_TOKENS_test_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "Ete = embed_sharded_cached_tokens(\n",
    "    texts=Xte_text,\n",
    "    ids=test_ids,\n",
    "    y=y_te,\n",
    "    split_name=\"test\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e7096-5df6-42e6-93e6-7a019aa0bd49",
   "metadata": {},
   "source": [
    "# IMAGES PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15919f6a-0c5c-4ef9-bc6b-c9073ec3e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd5973c-e7eb-42b7-a7fe-17fd6b1270a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb387ad0993481d9a860b3973640d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x1d9e2d211b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"CREATE OR REPLACE TABLE img_splits_sample AS\n",
    "SELECT m.post_id, m.split, i.full_image_file, m.er_bins\n",
    "FROM clip_full_sample m\n",
    "JOIN images_manifest1718_clean i ON m.post_id = i.post_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba4c3d1-eb46-4838-b71b-5293a695dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso dispositivo: cpu | PyTorch threads: 4\n",
      "Carico CLIP: openai/clip-vit-base-patch32 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP patch tokens: 50 token dim: 768\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "MODEL_TAG  = \"clip_vit_b32\"\n",
    "IMG_DIR = r\"D:/dataset/images_224_rgb\"\n",
    "BATCH_SIZE = 1\n",
    "SHARD_SIZE = 50\n",
    "OUT_DIR    = \"D:/dataset/clip_cross_attention_emb\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "num_threads = max(1, (os.cpu_count() or 4) // 2)\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Uso dispositivo: {device} | PyTorch threads: {num_threads}\")\n",
    "\n",
    "print(f\"Carico CLIP: {MODEL_NAME} ...\")\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "clip_model.eval()\n",
    "clip_model.to(device)\n",
    "\n",
    "# To undesrtand the image embedding shape\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 224, 224)  \n",
    "    # vision_model for image embeddings\n",
    "    out = clip_model.vision_model(pixel_values=dummy.to(device))\n",
    "    num_tokens = out.last_hidden_state.shape[1] # 50 to get the last layer of image patches\n",
    "    feat_dim  = out.last_hidden_state.shape[-1] # 768\n",
    "print(\"CLIP patch tokens:\", num_tokens, \"token dim:\", feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8babe27-c6ee-4df0-915e-cf2cb7554723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabella img_splits caricata. Righe: 44688\n",
      "                                post_id       split  \\\n",
      "0  coffeethentravel-1821049604449171679       train   \n",
      "1      coffeevschai-1910450713893530118  validation   \n",
      "2        cohlab_nyc-1761057847632557771       train   \n",
      "3        cohlab_nyc-1805534068554420444       train   \n",
      "4        cohlab_nyc-1827671707013133135       train   \n",
      "\n",
      "                                     full_image_file  \n",
      "0  D:/dataset/images_224_rgb\\coffeethentravel-182...  \n",
      "1  D:/dataset/images_224_rgb\\coffeevschai-1910450...  \n",
      "2  D:/dataset/images_224_rgb\\cohlab_nyc-176105784...  \n",
      "3  D:/dataset/images_224_rgb\\cohlab_nyc-180553406...  \n",
      "4  D:/dataset/images_224_rgb\\cohlab_nyc-182767170...  \n",
      "train = 31090 | val = 6665 | test = 6933\n"
     ]
    }
   ],
   "source": [
    "df = con.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM img_splits_sample\n",
    "\"\"\").df()\n",
    "print(\"Tabella img_splits caricata. Righe:\", len(df))\n",
    "\n",
    "if \"full_image_file\" not in df.columns:\n",
    "    if \"image_file\" not in df.columns:\n",
    "        raise ValueError(\"Non trovo né 'full_image_file' né 'image_file' in img_splits.\")\n",
    "    df[\"full_image_file\"] = df[\"image_file\"].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "else:\n",
    "    df[\"full_image_file\"] = df[\"full_image_file\"].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "\n",
    "print(df[[\"post_id\", \"split\", \"full_image_file\"]].head())\n",
    "\n",
    "# Split\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"validation\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"train = {len(train_df)} | val = {len(val_df)} | test = {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1324800b-8ca8-45e0-ba5c-72643d8bc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_clip_image_tokens(image_paths, bs=1):\n",
    "    all_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), bs):\n",
    "            # print(f\"processing batch {i}/{len(image_paths)}\")\n",
    "\n",
    "            batch_paths = image_paths[i:i+bs]\n",
    "\n",
    "            images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "            inputs = clip_processor(images=images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "            # vision_model for images\n",
    "            out = clip_model.vision_model(pixel_values=inputs)\n",
    "\n",
    "            # Patch image tokens with last_hidden_state\n",
    "            tokens = out.last_hidden_state\n",
    "            all_tokens.append(tokens.cpu())\n",
    "\n",
    "    return torch.cat(all_tokens, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da33c4c-70df-4a2e-922a-3af013378af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sharded_cached_image_tokens(image_paths, ids, y, split_name, emb_dir, shard=500):\n",
    "    \n",
    "    prefix = f\"{MODEL_TAG}_IMG_TOKENS_{split_name}\"\n",
    "    out_files = []\n",
    "    \n",
    "    n = len(image_paths)\n",
    "    assert len(ids) == n\n",
    "    assert len(y) == n\n",
    "\n",
    "    for i in range(0, n, shard):\n",
    "        f = emb_dir / f\"{prefix}_{i:07d}.npy\"\n",
    "\n",
    "        if f.exists():\n",
    "            print(f\"[skip] {f.name}\")\n",
    "            out_files.append(f)\n",
    "            continue\n",
    "\n",
    "        part_paths = image_paths[i:i+shard]\n",
    "        t0 = time.time()\n",
    "\n",
    "        T = embed_clip_image_tokens(part_paths, bs=1)\n",
    "        T = T.numpy().astype(\"float32\")\n",
    "\n",
    "        np.save(f, T)\n",
    "        out_files.append(f)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[saved] {f.name}  [{i+len(part_paths)}/{n}]  ({len(part_paths)} images in {dt:.1f}s)\")\n",
    "\n",
    "    arrays = [np.load(f) for f in out_files]\n",
    "    E_all = np.concatenate(arrays, axis=0).astype(\"float32\")\n",
    "    assert E_all.shape[0] == n, f\"Embeddings rows {E_all.shape[0]} != n={n}\"\n",
    "\n",
    "    np.save(emb_dir / f\"{prefix}_ALL.npy\", E_all)\n",
    "\n",
    "    npz_path = emb_dir / f\"{prefix}_ids_y.npz\"\n",
    "    np.savez(\n",
    "        npz_path,\n",
    "        ids=np.asarray(ids),\n",
    "        embeddings=E_all,\n",
    "        y=np.asarray(y)\n",
    "    )\n",
    "\n",
    "    print(f\"[done] Saved final {split_name} → {npz_path.name}\")\n",
    "    \n",
    "    return E_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cc2d3a-8c3f-4405-8f7d-dcf156301a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>split</th>\n",
       "      <th>full_image_file</th>\n",
       "      <th>er_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffeethentravel-1821049604449171679</td>\n",
       "      <td>train</td>\n",
       "      <td>D:/dataset/images_224_rgb\\coffeethentravel-182...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cohlab_nyc-1761057847632557771</td>\n",
       "      <td>train</td>\n",
       "      <td>D:/dataset/images_224_rgb\\cohlab_nyc-176105784...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohlab_nyc-1805534068554420444</td>\n",
       "      <td>train</td>\n",
       "      <td>D:/dataset/images_224_rgb\\cohlab_nyc-180553406...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cohlab_nyc-1827671707013133135</td>\n",
       "      <td>train</td>\n",
       "      <td>D:/dataset/images_224_rgb\\cohlab_nyc-182767170...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cohlab_nyc-1870432573244539951</td>\n",
       "      <td>train</td>\n",
       "      <td>D:/dataset/images_224_rgb\\cohlab_nyc-187043257...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                post_id  split  \\\n",
       "0  coffeethentravel-1821049604449171679  train   \n",
       "1        cohlab_nyc-1761057847632557771  train   \n",
       "2        cohlab_nyc-1805534068554420444  train   \n",
       "3        cohlab_nyc-1827671707013133135  train   \n",
       "4        cohlab_nyc-1870432573244539951  train   \n",
       "\n",
       "                                     full_image_file er_bins  \n",
       "0  D:/dataset/images_224_rgb\\coffeethentravel-182...    high  \n",
       "1  D:/dataset/images_224_rgb\\cohlab_nyc-176105784...  medium  \n",
       "2  D:/dataset/images_224_rgb\\cohlab_nyc-180553406...  medium  \n",
       "3  D:/dataset/images_224_rgb\\cohlab_nyc-182767170...  medium  \n",
       "4  D:/dataset/images_224_rgb\\cohlab_nyc-187043257...    high  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6e4a8c-9a58-4ac2-8760-d647c5ead070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip_vit_b32_IMG_TOKENS_train_0000000.npy  [1000/31090]  (1000 images in 159.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0001000.npy  [2000/31090]  (1000 images in 158.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0002000.npy  [3000/31090]  (1000 images in 154.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0003000.npy  [4000/31090]  (1000 images in 155.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0004000.npy  [5000/31090]  (1000 images in 162.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0005000.npy  [6000/31090]  (1000 images in 163.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0006000.npy  [7000/31090]  (1000 images in 168.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0007000.npy  [8000/31090]  (1000 images in 166.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0008000.npy  [9000/31090]  (1000 images in 157.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0009000.npy  [10000/31090]  (1000 images in 155.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0010000.npy  [11000/31090]  (1000 images in 153.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0011000.npy  [12000/31090]  (1000 images in 179.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0012000.npy  [13000/31090]  (1000 images in 173.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0013000.npy  [14000/31090]  (1000 images in 167.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0014000.npy  [15000/31090]  (1000 images in 142.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0015000.npy  [16000/31090]  (1000 images in 137.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0016000.npy  [17000/31090]  (1000 images in 137.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0017000.npy  [18000/31090]  (1000 images in 134.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0018000.npy  [19000/31090]  (1000 images in 136.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0019000.npy  [20000/31090]  (1000 images in 143.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0020000.npy  [21000/31090]  (1000 images in 137.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0021000.npy  [22000/31090]  (1000 images in 139.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0022000.npy  [23000/31090]  (1000 images in 138.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0023000.npy  [24000/31090]  (1000 images in 135.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0024000.npy  [25000/31090]  (1000 images in 140.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0025000.npy  [26000/31090]  (1000 images in 135.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0026000.npy  [27000/31090]  (1000 images in 134.7s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0027000.npy  [28000/31090]  (1000 images in 141.2s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0028000.npy  [29000/31090]  (1000 images in 137.7s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0029000.npy  [30000/31090]  (1000 images in 132.2s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0030000.npy  [31000/31090]  (1000 images in 138.2s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_train_0031000.npy  [31090/31090]  (90 images in 12.2s)\n",
      "[done] Saved final train → clip_vit_b32_IMG_TOKENS_train_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "emb_dir = Path(\"D:/dataset/clip_cross_attention_emb\")\n",
    "image_paths = train_df[\"full_image_file\"].tolist()\n",
    "train_ids = train_df[\"post_id\"].tolist()\n",
    "train_y = train_df[\"er_bins\"].tolist()\n",
    "\n",
    "Etr_img = embed_sharded_cached_image_tokens(\n",
    "    image_paths=image_paths,\n",
    "    ids=train_ids,\n",
    "    y=train_y,\n",
    "    split_name=\"train\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce48aad3-61bb-41ba-8f6a-2074ec7c8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0000000.npy  [500/6665]  (500 images in 85.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0000500.npy  [1000/6665]  (500 images in 83.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0001000.npy  [1500/6665]  (500 images in 82.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0001500.npy  [2000/6665]  (500 images in 81.6s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0002000.npy  [2500/6665]  (500 images in 81.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0002500.npy  [3000/6665]  (500 images in 82.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0003000.npy  [3500/6665]  (500 images in 81.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0003500.npy  [4000/6665]  (500 images in 79.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0004000.npy  [4500/6665]  (500 images in 81.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0004500.npy  [5000/6665]  (500 images in 81.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0005000.npy  [5500/6665]  (500 images in 80.2s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0005500.npy  [6000/6665]  (500 images in 81.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0006000.npy  [6500/6665]  (500 images in 80.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_validation_0006500.npy  [6665/6665]  (165 images in 26.9s)\n",
      "[done] Saved final validation → clip_vit_b32_IMG_TOKENS_validation_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "emb_dir = Path(\"D:/dataset/clip_cross_attention_emb\")\n",
    "image_paths = val_df[\"full_image_file\"].tolist()\n",
    "val_ids = val_df[\"post_id\"].tolist()\n",
    "val_y = val_df[\"er_bins\"].tolist()\n",
    "\n",
    "Eva_img = embed_sharded_cached_image_tokens(\n",
    "    image_paths=image_paths,\n",
    "    ids=val_ids,\n",
    "    y=val_y,\n",
    "    split_name=\"validation\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f7e6b6-c473-4d47-b549-ff12474e1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] clip_vit_b32_IMG_TOKENS_test_0000000.npy  [500/6933]  (500 images in 81.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0000500.npy  [1000/6933]  (500 images in 78.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0001000.npy  [1500/6933]  (500 images in 81.0s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0001500.npy  [2000/6933]  (500 images in 78.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0002000.npy  [2500/6933]  (500 images in 74.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0002500.npy  [3000/6933]  (500 images in 77.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0003000.npy  [3500/6933]  (500 images in 80.2s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0003500.npy  [4000/6933]  (500 images in 78.9s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0004000.npy  [4500/6933]  (500 images in 77.3s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0004500.npy  [5000/6933]  (500 images in 76.4s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0005000.npy  [5500/6933]  (500 images in 77.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0005500.npy  [6000/6933]  (500 images in 77.1s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0006000.npy  [6500/6933]  (500 images in 80.5s)\n",
      "[saved] clip_vit_b32_IMG_TOKENS_test_0006500.npy  [6933/6933]  (433 images in 70.1s)\n",
      "[done] Saved final test → clip_vit_b32_IMG_TOKENS_test_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "emb_dir = Path(\"D:/dataset/clip_cross_attention_emb\")\n",
    "image_paths = test_df[\"full_image_file\"].tolist()\n",
    "test_ids = test_df[\"post_id\"].tolist()\n",
    "test_y = test_df[\"er_bins\"].tolist()\n",
    "\n",
    "Ete_img = embed_sharded_cached_image_tokens(\n",
    "    image_paths=image_paths,\n",
    "    ids=test_ids,\n",
    "    y=test_y,\n",
    "    split_name=\"test\",\n",
    "    emb_dir=emb_dir,\n",
    "    shard=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74cc10-5dce-437d-b5ba-58f3d2700570",
   "metadata": {},
   "source": [
    "# CROSS-ATTENTION TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d04d8c-3db9-4d47-a89d-e0f32577b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required functions\n",
    "\n",
    "# Aggregate the image tokens related to the same post and compute the mean\n",
    "def aggregate_image_tokens_per_post(ids_txt, ids_img, img_tokens):\n",
    "    img_tokens_post = []\n",
    "\n",
    "    for pid in ids_txt:\n",
    "        mask = (ids_img == pid)\n",
    "        tokens_p = img_tokens[mask]\n",
    "        \n",
    "        if tokens_p.shape[0] == 0:\n",
    "            agg = np.zeros((50, img_tokens.shape[-1]), dtype=img_tokens.dtype)\n",
    "        else:\n",
    "            agg = tokens_p.mean(axis=0)\n",
    "        \n",
    "        img_tokens_post.append(agg)\n",
    "\n",
    "    return np.stack(img_tokens_post, axis=0)\n",
    "\n",
    "\n",
    "# Define a loader to load the required data in the correct format\n",
    "def simple_batch_loader(txt, img, y, batch_size=32, shuffle=True):\n",
    "    N = len(y)\n",
    "    idxs = np.arange(N)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_idx = idxs[start:end]\n",
    "\n",
    "        yield txt[batch_idx], img[batch_idx], y[batch_idx]\n",
    "\n",
    "# Defines the computations required to compute cross attention between text and images\n",
    "def cross_attention_encode(txt, img):\n",
    "\n",
    "    # Image projection to aligne the image dimension to the text one (for the attention mechanism)\n",
    "    # It is needed as CLIP image produced embeddings with dimension 768 instead of 512\n",
    "    img = img_proj(img)\n",
    "\n",
    "    # TEXT looks at the IMAGE\n",
    "    t2i, _ = attn_t2i(query=txt, key=img, value=img) # cross-attention: each text token looks at the image\n",
    "    # t2i is the additional information that text got from looking at the image\n",
    "    txt2 = norm_t1(txt + t2i) # the new info is added to the original text, then the norm is computed for robustness\n",
    "    txt2 = norm_t2(txt2 + ff_t(txt2)) #ff_t adds non-linearity with the feed-forward step \n",
    "\n",
    "    # IMAGE looks at the TEXT\n",
    "    i2t, _ = attn_i2t(query=img, key=txt2, value=txt2) # cross-attention: each image patch looks at the text\n",
    "    img2 = norm_i1(img + i2t)\n",
    "    img2 = norm_i2(img2 + ff_i(img2))\n",
    "\n",
    "    # Pooling to get a single vector for caption/image\n",
    "    txt_repr = txt2.mean(dim=1)\n",
    "    img_repr = img2.mean(dim=1)\n",
    "\n",
    "    fused = torch.cat([txt_repr, img_repr], dim=-1)\n",
    "\n",
    "    return fused\n",
    "\n",
    "# Fuses together text and image\n",
    "def generate_fused_features(txt, img, batch_size=32):\n",
    "    fused_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for txt_b, img_b, _ in simple_batch_loader(\n",
    "            txt, img, torch.zeros(len(txt)), batch_size=batch_size, shuffle=False\n",
    "        ):\n",
    "            fused = cross_attention_encode(txt_b, img_b)\n",
    "            fused_list.append(fused.cpu().numpy())\n",
    "\n",
    "    return np.vstack(fused_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2de7050-4a39-467c-aff6-3a4542e4911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_npz_tr  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_train_ids_y.npz\"\n",
    "image_npz_tr = \"D:/dataset/clip_cross_attention_emb/clip_vit_b32_IMG_TOKENS_train_ids_y.npz\"\n",
    "\n",
    "t_tr = np.load(text_npz_tr, allow_pickle = True)\n",
    "i_tr = np.load(image_npz_tr, allow_pickle = True)\n",
    "\n",
    "ids_txt_tr = t_tr[\"ids\"]\n",
    "txt_tokens_tr = t_tr[\"embeddings\"]\n",
    "\n",
    "ids_img_tr = i_tr[\"ids\"]\n",
    "img_tokens_tr = i_tr[\"embeddings\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff32f7b-46fb-4139-b5e3-b844508d0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tokens_post_tr = aggregate_image_tokens_per_post(ids_txt_tr, ids_img_tr, img_tokens_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9ed253-32d1-45b1-88f5-c9f8947a171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 77, 512) (31090, 50, 768) (24993, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "print(txt_tokens_tr.shape, img_tokens_tr.shape, img_tokens_post_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ead299b-272a-4d3d-9042-d69400c50562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post solo nel testo: set()\n",
      "Post solo nelle immagini: set()\n"
     ]
    }
   ],
   "source": [
    "set_txt = set(t_tr[\"ids\"])\n",
    "set_img = set(i_tr[\"ids\"])\n",
    "\n",
    "print(\"Post solo nel testo:\", set_txt - set_img)\n",
    "print(\"Post solo nelle immagini:\", set_img - set_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc271528-6534-4a89-bf32-035873853f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_text = 512 # token text\n",
    "dim_img  = 768 # patch images\n",
    "dim = dim_text # common space must be 512\n",
    "\n",
    "# Projection to get the image to a 512 dimension\n",
    "img_proj = nn.Linear(dim_img, dim)\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "# Attention layers text to image and image to text\n",
    "attn_t2i = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "attn_i2t = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "# Feed-forward operations for text and images\n",
    "ff_t = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "ff_i = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "# Layer norms\n",
    "norm_t1 = nn.LayerNorm(dim)\n",
    "norm_t2 = nn.LayerNorm(dim)\n",
    "norm_i1 = nn.LayerNorm(dim)\n",
    "norm_i2 = nn.LayerNorm(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74669644-4ab7-4651-9144-16d2f6a24e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_tr = torch.tensor(txt_tokens_tr, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb431c6-10ae-4610-b799-41557de8ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del txt_tokens_tr, img_tokens_tr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0cfb38-797e-4b98-b275-e138c61c8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tr = torch.tensor(img_tokens_post_tr, dtype=torch.float32)\n",
    "\n",
    "fused_train = generate_fused_features(txt_tr, img_tr, batch_size=32)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_train.npy\", fused_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68d8aa1-6bef-4c3f-9ce3-e099822d5dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24993, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf6c371-9ad0-45fb-9705-e5e0cf816bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4]\n",
      "shape: torch.Size([24993])\n",
      "torch.Size([24993, 77, 512]) torch.Size([24993, 50, 768]) torch.Size([24993])\n",
      "Unique classes: [0 1 2]\n",
      "shape: torch.Size([24993])\n",
      "torch.Size([24993, 77, 512]) torch.Size([24993, 50, 768]) torch.Size([24993])\n",
      "Unique classes: [0 1]\n",
      "shape: torch.Size([24993])\n",
      "torch.Size([24993, 77, 512]) torch.Size([24993, 50, 768]) torch.Size([24993])\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "\n",
    "# 5 classes\n",
    "y_df = con.execute(\"\"\"\n",
    "    SELECT post_id, er_bins, er_bins3, er_bins2\n",
    "    FROM clip_full_sample\n",
    "    WHERE split = 'train'\"\"\").df()\n",
    "\n",
    "\n",
    "y_tr_aligned5 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_tr, \"er_bins\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"very_low\": 0,\n",
    "    \"low\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"high\": 3,\n",
    "    \"very_high\": 4\n",
    "}\n",
    "y_tr_int = np.array([class_to_int[c] for c in y_tr_aligned5])\n",
    "y_tr_5 = torch.tensor(y_tr_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_tr_int))\n",
    "print(\"shape:\", y_tr_5.shape)\n",
    "print(txt_tr.shape, img_tr.shape, y_tr_5.shape)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", y_tr_5)\n",
    "\n",
    "# 3 classes\n",
    "\n",
    "y_tr_aligned3 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_tr, \"er_bins3\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"medium\": 1,\n",
    "    \"high\": 2\n",
    "}\n",
    "y_tr_int = np.array([class_to_int[c] for c in y_tr_aligned3])\n",
    "y_tr_3 = torch.tensor(y_tr_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_tr_int))\n",
    "print(\"shape:\", y_tr_3.shape)\n",
    "print(txt_tr.shape, img_tr.shape, y_tr_3.shape)\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_tr_3.npy\", y_tr_3)\n",
    "\n",
    "y_tr_aligned2 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_tr, \"er_bins2\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"high\": 1\n",
    "}\n",
    "y_tr_int = np.array([class_to_int[c] for c in y_tr_aligned2])\n",
    "y_tr_2 = torch.tensor(y_tr_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_tr_int))\n",
    "print(\"shape:\", y_tr_2.shape)\n",
    "print(txt_tr.shape, img_tr.shape, y_tr_2.shape)\n",
    "\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_tr_2.npy\", y_tr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7499513-fde4-46b4-b5b1-4d41d2bb6db4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CROSS-ATTENTION VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aab7932-6626-4493-8b35-10c088509005",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_npz_va  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_validation_ids_y.npz\"\n",
    "image_npz_va = \"D:/dataset/clip_cross_attention_emb/clip_vit_b32_IMG_TOKENS_validation_ids_y.npz\"\n",
    "\n",
    "t_va = np.load(text_npz_va, allow_pickle = True)\n",
    "i_va = np.load(image_npz_va, allow_pickle = True)\n",
    "\n",
    "ids_txt_va = t_va[\"ids\"]\n",
    "txt_tokens_va = t_va[\"embeddings\"]\n",
    "y_va = t_va[\"y\"]\n",
    "\n",
    "ids_img_va = i_va[\"ids\"]\n",
    "img_tokens_va = i_va[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14135e1c-9ae3-42a8-8a2b-6c9f3deb5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tokens_post_va = aggregate_image_tokens_per_post(ids_txt_va, ids_img_va, img_tokens_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795d7117-2199-498b-86a0-38a72181d327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 77, 512) (6665, 50, 768) (5000, 50, 768) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(txt_tokens_va.shape, img_tokens_va.shape, img_tokens_post_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4df46b-e26b-4c3d-bd94-4f6a6fb498c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post solo nel testo: set()\n",
      "Post solo nelle immagini: set()\n"
     ]
    }
   ],
   "source": [
    "set_txt = set(t_va[\"ids\"])\n",
    "set_img = set(i_va[\"ids\"])\n",
    "\n",
    "print(\"Post solo nel testo:\", set_txt - set_img)\n",
    "print(\"Post solo nelle immagini:\", set_img - set_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587870e3-95a0-4e71-b30f-75156c5c7633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4]\n",
      "shape: torch.Size([5000])\n",
      "Unique classes: [0 1 2]\n",
      "shape: torch.Size([5000])\n",
      "Unique classes: [0 1]\n",
      "shape: torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "\n",
    "# 5 classes\n",
    "y_df = con.execute(\"\"\"\n",
    "    SELECT post_id, er_bins, er_bins3, er_bins2\n",
    "    FROM clip_full_sample\n",
    "    WHERE split = 'validation'\"\"\").df()\n",
    "\n",
    "\n",
    "y_va_aligned5 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_va, \"er_bins\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"very_low\": 0,\n",
    "    \"low\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"high\": 3,\n",
    "    \"very_high\": 4\n",
    "}\n",
    "y_va_int = np.array([class_to_int[c] for c in y_va_aligned5])\n",
    "y_va_5 = torch.tensor(y_va_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_va_int))\n",
    "print(\"shape:\", y_va_5.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_5.shape)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", y_va_5)\n",
    "\n",
    "# 3 classes\n",
    "\n",
    "y_va_aligned3 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_va, \"er_bins3\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"medium\": 1,\n",
    "    \"high\": 2\n",
    "}\n",
    "y_va_int = np.array([class_to_int[c] for c in y_va_aligned3])\n",
    "y_va_3 = torch.tensor(y_va_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_va_int))\n",
    "print(\"shape:\", y_va_3.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_3.shape)\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_va_3.npy\", y_va_3)\n",
    "\n",
    "y_va_aligned2 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_va, \"er_bins2\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"high\": 1\n",
    "}\n",
    "y_va_int = np.array([class_to_int[c] for c in y_va_aligned2])\n",
    "y_va_2 = torch.tensor(y_va_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_va_int))\n",
    "print(\"shape:\", y_va_2.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_2.shape)\n",
    "\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_va_2.npy\", y_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f390584-8312-435a-a983-f53acfd41c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_text = 512 # token text\n",
    "dim_img  = 768 # patch images\n",
    "dim = dim_text # common space must be 512\n",
    "\n",
    "# Projection to get the image to a 512 dimension\n",
    "img_proj = nn.Linear(dim_img, dim)\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "# Attention layers text to image and image to text\n",
    "attn_t2i = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "attn_i2t = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "# Feed-forward operations for text and images\n",
    "ff_t = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "ff_i = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "# Layer norms\n",
    "norm_t1 = nn.LayerNorm(dim)\n",
    "norm_t2 = nn.LayerNorm(dim)\n",
    "norm_i1 = nn.LayerNorm(dim)\n",
    "norm_i2 = nn.LayerNorm(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8705bb91-7c40-436e-a54e-e33c2ce13dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_va = torch.tensor(txt_tokens_va, dtype=torch.float32)\n",
    "img_va = torch.tensor(img_tokens_post_va, dtype=torch.float32)\n",
    "\n",
    "fused_val = generate_fused_features(txt_va, img_va, batch_size=32)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_val.npy\", fused_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766bf88c-48fa-4a04-875f-251bad7dd4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd612d-109d-42d3-9faa-ff68db25d1a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CROSS-ATTENTION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9550c1c-6790-4870-9f9e-4bde9b23d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_npz_te  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_test_ids_y.npz\"\n",
    "image_npz_te = \"D:/dataset/clip_cross_attention_emb/clip_vit_b32_IMG_TOKENS_test_ids_y.npz\"\n",
    "\n",
    "t_te = np.load(text_npz_te, allow_pickle = True)\n",
    "i_te = np.load(image_npz_te, allow_pickle = True)\n",
    "\n",
    "ids_txt_te = t_te[\"ids\"]\n",
    "txt_tokens_te = t_te[\"embeddings\"]\n",
    "\n",
    "ids_img_te = i_te[\"ids\"]\n",
    "img_tokens_te = i_te[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a3b8bf-c4de-4a76-bce4-35b294bcd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tokens_post_te = aggregate_image_tokens_per_post(ids_txt_te, ids_img_te, img_tokens_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3323cdcc-643d-41e3-b404-7ac9094cf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 77, 512) (6933, 50, 768) (5000, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "print(txt_tokens_te.shape, img_tokens_te.shape, img_tokens_post_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3f18e79-bb73-476b-81c1-873f2e37922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post solo nel testo: set()\n",
      "Post solo nelle immagini: set()\n"
     ]
    }
   ],
   "source": [
    "set_txt = set(t_te[\"ids\"])\n",
    "set_img = set(i_te[\"ids\"])\n",
    "\n",
    "print(\"Post solo nel testo:\", set_txt - set_img)\n",
    "print(\"Post solo nelle immagini:\", set_img - set_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ffad56e-aab1-4fb5-8b01-35b2d5f820e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2 3 4]\n",
      "shape: torch.Size([5000])\n",
      "Unique classes: [0 1 2]\n",
      "shape: torch.Size([5000])\n",
      "Unique classes: [0 1]\n",
      "shape: torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "txt_te = torch.tensor(txt_tokens_te, dtype=torch.float32)\n",
    "img_te = torch.tensor(img_tokens_post_te, dtype=torch.float32)\n",
    "\n",
    "# target\n",
    "\n",
    "# 5 classes\n",
    "y_df = con.execute(\"\"\"\n",
    "    SELECT post_id, er_bins, er_bins3, er_bins2\n",
    "    FROM clip_full_sample\n",
    "    WHERE split = 'test'\"\"\").df()\n",
    "\n",
    "\n",
    "y_te_aligned5 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_te, \"er_bins\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"very_low\": 0,\n",
    "    \"low\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"high\": 3,\n",
    "    \"very_high\": 4\n",
    "}\n",
    "y_te_int = np.array([class_to_int[c] for c in y_te_aligned5])\n",
    "y_te_5 = torch.tensor(y_te_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_te_int))\n",
    "print(\"shape:\", y_te_5.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_5.shape)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_te_5.npy\", y_te_5)\n",
    "\n",
    "# 3 classes\n",
    "\n",
    "y_te_aligned3 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_te, \"er_bins3\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"medium\": 1,\n",
    "    \"high\": 2\n",
    "}\n",
    "y_te_int = np.array([class_to_int[c] for c in y_te_aligned3])\n",
    "y_te_3 = torch.tensor(y_te_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_te_int))\n",
    "print(\"shape:\", y_te_3.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_3.shape)\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_te_3.npy\", y_te_3)\n",
    "\n",
    "y_te_aligned2 = (\n",
    "    y_df.set_index(\"post_id\")\n",
    "        .loc[ids_txt_te, \"er_bins2\"]\n",
    "        .to_numpy()\n",
    ")\n",
    "\n",
    "# Since I am using a pytorch tensor I need the target into numerical variable\n",
    "class_to_int = {\n",
    "    \"low\": 0,\n",
    "    \"high\": 1\n",
    "}\n",
    "y_te_int = np.array([class_to_int[c] for c in y_te_aligned2])\n",
    "y_te_2 = torch.tensor(y_te_int, dtype=torch.long)\n",
    "\n",
    "print(\"Unique classes:\", np.unique(y_te_int))\n",
    "print(\"shape:\", y_te_2.shape)\n",
    "# print(txt_va.shape, img_va.shape, y_va_2.shape)\n",
    "\n",
    "\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/y_te_2.npy\", y_te_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e06c4f-f052-4b2e-b81f-8a7cc57939bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_text = 512 # token text\n",
    "dim_img  = 768 # patch images\n",
    "dim = dim_text # common space must be 512\n",
    "\n",
    "# Projection to get the image to a 512 dimension\n",
    "img_proj = nn.Linear(dim_img, dim)\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "# Attention layers text to image and image to text\n",
    "attn_t2i = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "attn_i2t = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "# Feed-forward operations for text and images\n",
    "ff_t = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "ff_i = nn.Sequential(\n",
    "    nn.Linear(dim, 4*dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*dim, dim)\n",
    ")\n",
    "\n",
    "# Layer norms\n",
    "norm_t1 = nn.LayerNorm(dim)\n",
    "norm_t2 = nn.LayerNorm(dim)\n",
    "norm_i1 = nn.LayerNorm(dim)\n",
    "norm_i2 = nn.LayerNorm(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f279f7f-7897-4ecf-9cb9-64eb3f496bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_test = generate_fused_features(txt_te, img_te, batch_size=32)\n",
    "np.save(\"D:/dataset/clip_cross_attention_emb/fused_test.npy\", fused_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518bb2d0-3db6-46c9-b7df-27b1d573f5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9b020-3e8a-476d-9e2a-11613ed3d068",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "874e3b9f-61c9-4b4b-9804-2f5dcb511aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_npz_tr  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_train_ids_y.npz\"\n",
    "t_tr = np.load(text_npz_tr, allow_pickle = True)\n",
    "ids_txt_tr = t_tr[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07912f87-db40-427b-b20c-935aa6461079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_txt_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e081050-2e33-48e6-904e-13ec97aaa9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA TRAIN\n",
    "meta_train_final = pd.read_csv(\"D:/dataset/meta_classification/meta_train_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_train_final = meta_train_final[meta_train_final[\"post_id\"].isin(ids_txt_tr)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_train_final = meta_train_final.set_index(\"post_id\").loc[ids_txt_tr].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_train_final[\"post_id\"].to_numpy() == ids_txt_tr).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_train = meta_train_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_train = X_meta_train.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_train = np.load(\"D:/dataset/clip_cross_attention_emb/fused_train.npy\", allow_pickle = True)\n",
    "X_tr = np.hstack([fused_train, X_meta_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f174c0c6-d131-4c8e-b7f0-bb1896fee1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24993, 1052)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ef75f1-dd8b-463b-9c31-b28a088f5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89dd66db-758a-4781-84e8-7a8caf3af272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA VALIDATION\n",
    "text_npz_va  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_validation_ids_y.npz\"\n",
    "t_va = np.load(text_npz_va, allow_pickle = True)\n",
    "ids_txt_va = t_va[\"ids\"]\n",
    "\n",
    "meta_val_final = pd.read_csv(\"D:/dataset/meta_classification/meta_val_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_val_final = meta_val_final[meta_val_final[\"post_id\"].isin(ids_txt_va)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_val_final = meta_val_final.set_index(\"post_id\").loc[ids_txt_va].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_val_final[\"post_id\"].to_numpy() == ids_txt_va).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_val = meta_val_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_val = X_meta_val.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_val = np.load(\"D:/dataset/clip_cross_attention_emb/fused_val.npy\", allow_pickle = True)\n",
    "X_va = np.hstack([fused_val, X_meta_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6840d809-89ac-45d2-945f-7523c74579e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1052)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_va.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "801a8570-b22f-4944-abda-d4ab1b0c31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f31fe56-2aba-4124-8539-26c3c57fef62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned\n"
     ]
    }
   ],
   "source": [
    "# METADATA TEST\n",
    "\n",
    "text_npz_te  = \"D:/dataset/clip_cross_attention_emb/clip-vit-base-patch32_TOKENS_test_ids_y.npz\"\n",
    "t_te = np.load(text_npz_te, allow_pickle = True)\n",
    "ids_txt_te = t_te[\"ids\"]\n",
    "\n",
    "meta_test_final = pd.read_csv(\"D:/dataset/meta_classification/meta_test_final.csv\")\n",
    "# Filtra dati che sono in ids_txt_tr\n",
    "meta_test_final = meta_test_final[meta_test_final[\"post_id\"].isin(ids_txt_te)]\n",
    "# Riordina seguento l'ordine degli embeddings\n",
    "meta_test_final = meta_test_final.set_index(\"post_id\").loc[ids_txt_te].reset_index()\n",
    "# Check allineamento\n",
    "assert (meta_test_final[\"post_id\"].to_numpy() == ids_txt_te).all()\n",
    "print(\"Aligned\")\n",
    "# Remove post_id\n",
    "X_meta_test = meta_test_final.drop([\"post_id\"], axis=1)\n",
    "X_meta_test = X_meta_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "fused_test = np.load(\"D:/dataset/clip_cross_attention_emb/fused_test.npy\", allow_pickle = True)\n",
    "X_te = np.hstack([fused_test, X_meta_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8a88adc-50c9-40f1-bb89-cf1ad4cea23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1052)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5583ef8-c353-467b-9abb-c7903ef13ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_cross_attention_emb/X_test.npy\", X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f0328-750f-47ed-a0eb-d280ea516fc6",
   "metadata": {},
   "source": [
    "# CLASSIFICATION 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186201a3-4a5f-4e49-8004-f7148370b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70bd1df-da95-4000-9862-8cc972df0674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 1052) (24993,) (5000, 1052) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8426c239-6351-4b8a-8187-b6b2c1b2994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.1647556846408762 | accuracy (val): 0.2346\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.16843089580730625 | accuracy (val): 0.2324\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.15993931310424248 | accuracy (val): 0.2416\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.15375959452643725 | accuracy (val): 0.2328\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.14564667869311454 | accuracy (val): 0.2426\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.15169951567450124 | accuracy (val): 0.2472\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.15987081430319183 | accuracy (val): 0.2584\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.15535904820075885 | accuracy (val): 0.258\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "Validation macro-F1: 0.16843089580730625\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "1  0.00001     balanced      0.168431        0.2324\n",
      "0  0.00001         None      0.164756        0.2346\n",
      "2  0.00010         None      0.159939        0.2416\n",
      "6  0.01000         None      0.159871        0.2584\n",
      "7  0.01000     balanced      0.155359        0.2580\n",
      "3  0.00010     balanced      0.153760        0.2328\n",
      "5  0.00100     balanced      0.151700        0.2472\n",
      "4  0.00100         None      0.145647        0.2426\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f096a815-c7d8-474a-8eae-1badc53f2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.0672 | accuracy (val): 0.1978\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.0672 | accuracy (val): 0.1978\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.0672 | accuracy (val): 0.1978\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.0672 | accuracy (val): 0.1978\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.06716980704235488\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09       0.06717        0.1978\n",
      "1   1.000000e-08       0.06717        0.1978\n",
      "2   1.000000e-07       0.06717        0.1978\n",
      "3   1.000000e-06       0.06717        0.1978\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03835789-cbd5-4c3d-9925-fce5b3314813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2559 | accuracy (val): 0.2606\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2667 | accuracy (val): 0.2688\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2772 | accuracy (val): 0.2810\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2714 | accuracy (val): 0.2690\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2675 | accuracy (val): 0.2734\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2671 | accuracy (val): 0.2776\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2272 | accuracy (val): 0.2484\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2326 | accuracy (val): 0.2472\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2333 | accuracy (val): 0.2544\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2420 | accuracy (val): 0.2572\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2519 | accuracy (val): 0.2662\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2337 | accuracy (val): 0.2564\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2359 | accuracy (val): 0.2460\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2470 | accuracy (val): 0.2616\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2545 | accuracy (val): 0.2672\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2554 | accuracy (val): 0.2664\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2573 | accuracy (val): 0.2702\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2483 | accuracy (val): 0.2686\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2292 | accuracy (val): 0.2358\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2420 | accuracy (val): 0.2502\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2385 | accuracy (val): 0.2512\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2246 | accuracy (val): 0.2358\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2317 | accuracy (val): 0.2446\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2329 | accuracy (val): 0.2492\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2497 | accuracy (val): 0.2552\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2608 | accuracy (val): 0.2658\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2601 | accuracy (val): 0.2688\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2487 | accuracy (val): 0.2586\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2546 | accuracy (val): 0.2690\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2570 | accuracy (val): 0.2726\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2301 | accuracy (val): 0.2420\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2277 | accuracy (val): 0.2396\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2263 | accuracy (val): 0.2424\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.2357 | accuracy (val): 0.2432\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2454 | accuracy (val): 0.2562\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2384 | accuracy (val): 0.2524\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "Validation macro-F1: 0.2771693198753883\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "2             80          8                 2         0.05      0.277169   \n",
      "3             30          8                 5         0.05      0.271371   \n",
      "4             50          8                 5         0.05      0.267511   \n",
      "5             80          8                 5         0.05      0.267124   \n",
      "1             50          8                 2         0.05      0.266657   \n",
      "25            50         12                 2         0.05      0.260792   \n",
      "26            80         12                 2         0.05      0.260111   \n",
      "16            50         10                 5         0.05      0.257323   \n",
      "29            80         12                 5         0.05      0.257016   \n",
      "0             30          8                 2         0.05      0.255932   \n",
      "15            30         10                 5         0.05      0.255358   \n",
      "28            50         12                 5         0.05      0.254569   \n",
      "14            80         10                 2         0.05      0.254536   \n",
      "10            50          8                 5         sqrt      0.251938   \n",
      "24            30         12                 2         0.05      0.249697   \n",
      "27            30         12                 5         0.05      0.248722   \n",
      "17            80         10                 5         0.05      0.248314   \n",
      "13            50         10                 2         0.05      0.246972   \n",
      "34            50         12                 5         sqrt      0.245363   \n",
      "19            50         10                 2         sqrt      0.242044   \n",
      "9             30          8                 5         sqrt      0.241990   \n",
      "20            80         10                 2         sqrt      0.238513   \n",
      "35            80         12                 5         sqrt      0.238395   \n",
      "12            30         10                 2         0.05      0.235875   \n",
      "33            30         12                 5         sqrt      0.235728   \n",
      "11            80          8                 5         sqrt      0.233681   \n",
      "8             80          8                 2         sqrt      0.233345   \n",
      "23            80         10                 5         sqrt      0.232922   \n",
      "7             50          8                 2         sqrt      0.232575   \n",
      "22            50         10                 5         sqrt      0.231718   \n",
      "30            30         12                 2         sqrt      0.230128   \n",
      "18            30         10                 2         sqrt      0.229151   \n",
      "31            50         12                 2         sqrt      0.227715   \n",
      "6             30          8                 2         sqrt      0.227166   \n",
      "32            80         12                 2         sqrt      0.226297   \n",
      "21            30         10                 5         sqrt      0.224588   \n",
      "\n",
      "    val_accuracy  \n",
      "2         0.2810  \n",
      "3         0.2690  \n",
      "4         0.2734  \n",
      "5         0.2776  \n",
      "1         0.2688  \n",
      "25        0.2658  \n",
      "26        0.2688  \n",
      "16        0.2702  \n",
      "29        0.2726  \n",
      "0         0.2606  \n",
      "15        0.2664  \n",
      "28        0.2690  \n",
      "14        0.2672  \n",
      "10        0.2662  \n",
      "24        0.2552  \n",
      "27        0.2586  \n",
      "17        0.2686  \n",
      "13        0.2616  \n",
      "34        0.2562  \n",
      "19        0.2502  \n",
      "9         0.2572  \n",
      "20        0.2512  \n",
      "35        0.2524  \n",
      "12        0.2460  \n",
      "33        0.2432  \n",
      "11        0.2564  \n",
      "8         0.2544  \n",
      "23        0.2492  \n",
      "7         0.2472  \n",
      "22        0.2446  \n",
      "30        0.2420  \n",
      "18        0.2358  \n",
      "31        0.2396  \n",
      "6         0.2484  \n",
      "32        0.2424  \n",
      "21        0.2358  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n",
    "\n",
    "# Among these the best is \n",
    "# Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
    "# macro-F1 (val): 0.2244 | accuracy (val): 0.2418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac7132a-1670-4f17-a36d-1530ede2bff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2647 | accuracy (val): 0.2956\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2554 | accuracy (val): 0.2912\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2637 | accuracy (val): 0.2902\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2620 | accuracy (val): 0.2898\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2602 | accuracy (val): 0.2970\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2448 | accuracy (val): 0.2876\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2523 | accuracy (val): 0.2800\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2475 | accuracy (val): 0.2768\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.26465533455930046\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "0               0.5      0            0.1          4           100   \n",
      "2               0.5      0            0.1          6           100   \n",
      "3               0.5      0            0.1          6           150   \n",
      "4               0.5      1            0.1          4           100   \n",
      "1               0.5      0            0.1          4           150   \n",
      "6               0.5      1            0.1          6           100   \n",
      "7               0.5      1            0.1          6           150   \n",
      "5               0.5      1            0.1          4           150   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "0           1        0.8      0.264655        0.2956  \n",
      "2           1        0.8      0.263670        0.2902  \n",
      "3           1        0.8      0.262015        0.2898  \n",
      "4           1        0.8      0.260209        0.2970  \n",
      "1           1        0.8      0.255385        0.2912  \n",
      "6           1        0.8      0.252346        0.2800  \n",
      "7           1        0.8      0.247457        0.2768  \n",
      "5           1        0.8      0.244795        0.2876  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5], \n",
    "    \"gamma\": [0, 1], \n",
    "    \"reg_lambda\": [1], \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241ee7e-572c-4bdd-830b-cf859ad7438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE ON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85cca0a-af44-474f-ac29-1bb845bdb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/clip_cross_attention_emb/X_test.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_5.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_5.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/clip_cross_attention_emb/y_te_5.npy\", allow_pickle = True)\n",
    "\n",
    "X_trva = np.concatenate((X_tr, X_va), axis = 0).astype(np.float32)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344bc7f1-3a8f-4cdd-960c-2dfe6545b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (train): 0.1510 | accuracy (train): 0.2670\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=8, max_features=0.05, min_samples_leaf=2,\n",
      "                       n_estimators=80, n_jobs=-1, random_state=42)\n",
      "macro-F1 (train): 0.2883 | accuracy (train): 0.2968\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=0,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (train): 0.3087 | accuracy (train): 0.3252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_trva_enc = le.fit_transform(y_trva)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=8, max_features=0.05, min_samples_leaf=2, n_estimators=80, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.1, max_depth= 4, n_estimators= 100, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_trva_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_trva, y_trva_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_trva, y_trva)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (train): {macro_f1:.4f} | accuracy (train): {acc:.4f}\")\n",
    "\n",
    "del X_trva, X_te, y_trva, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cbfe9b-7f08-4776-a9ea-f2f6ce5a0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2303 | accuracy (test): 0.2380\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 1e-05,\n",
    "        average = True,\n",
    "        class_weight = 'balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_trva, y_trva)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0b40f-0410-4355-ba7b-455f677d58bc",
   "metadata": {},
   "source": [
    "# 3 classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359b2a7-42cf-4c79-80d9-0a76759e1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b691328-4969-4938-b98f-09156afb3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_3.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_3.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dc0552-5fa4-45f6-a665-6145069ee3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 1052) (24993,) (5000, 1052) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f23066-0442-4ff2-ad1c-05176d372078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.35353245107982706 | accuracy (val): 0.3848\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.3397182866783806 | accuracy (val): 0.3856\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.3364996891219681 | accuracy (val): 0.3854\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.33820837381647556 | accuracy (val): 0.3962\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.32146032926155677 | accuracy (val): 0.3856\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.30364638333081906 | accuracy (val): 0.3818\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.30429447029801726 | accuracy (val): 0.3936\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.3227482384757346 | accuracy (val): 0.4142\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 1e-05, 'class_weight': None}\n",
      "Validation macro-F1: 0.35353245107982706\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "0  0.00001         None      0.353532        0.3848\n",
      "1  0.00001     balanced      0.339718        0.3856\n",
      "3  0.00010     balanced      0.338208        0.3962\n",
      "2  0.00010         None      0.336500        0.3854\n",
      "7  0.01000     balanced      0.322748        0.4142\n",
      "4  0.00100         None      0.321460        0.3856\n",
      "6  0.01000         None      0.304294        0.3936\n",
      "5  0.00100     balanced      0.303646        0.3818\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2ec9de-a6eb-40c8-b4ef-cd1af58feca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.1687 | accuracy (val): 0.3360\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.1687 | accuracy (val): 0.3360\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.1687 | accuracy (val): 0.3360\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.1687 | accuracy (val): 0.3360\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.1686771998419683\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09      0.168677         0.336\n",
      "1   1.000000e-08      0.168677         0.336\n",
      "2   1.000000e-07      0.168677         0.336\n",
      "3   1.000000e-06      0.168677         0.336\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c9771a-7087-42d4-91f5-030e82b27611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4088 | accuracy (val): 0.4200\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4178 | accuracy (val): 0.4306\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4119 | accuracy (val): 0.4352\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4025 | accuracy (val): 0.4264\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4146 | accuracy (val): 0.4390\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.3947 | accuracy (val): 0.4254\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4169 | accuracy (val): 0.4242\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4284 | accuracy (val): 0.4456\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4268 | accuracy (val): 0.4472\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4077 | accuracy (val): 0.4192\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4189 | accuracy (val): 0.4406\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4209 | accuracy (val): 0.4382\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4186 | accuracy (val): 0.4250\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4234 | accuracy (val): 0.4332\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4274 | accuracy (val): 0.4362\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.4191 | accuracy (val): 0.4286\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4219 | accuracy (val): 0.4344\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4138 | accuracy (val): 0.4296\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3887 | accuracy (val): 0.3960\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4050 | accuracy (val): 0.4096\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.3959 | accuracy (val): 0.4072\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3640 | accuracy (val): 0.3790\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.3760 | accuracy (val): 0.3950\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.3703 | accuracy (val): 0.4004\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3771 | accuracy (val): 0.3888\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.3949 | accuracy (val): 0.4058\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4072 | accuracy (val): 0.4174\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3955 | accuracy (val): 0.4014\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.4085 | accuracy (val): 0.4182\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4116 | accuracy (val): 0.4304\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3653 | accuracy (val): 0.3816\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.3758 | accuracy (val): 0.3926\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.3985 | accuracy (val): 0.4130\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.3861 | accuracy (val): 0.3932\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.3912 | accuracy (val): 0.4000\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.4042 | accuracy (val): 0.4208\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Validation macro-F1: 0.42841792978570536\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "7             50          8                 2         sqrt      0.428418   \n",
      "14            80         10                 2         0.05      0.427396   \n",
      "8             80          8                 2         sqrt      0.426758   \n",
      "13            50         10                 2         0.05      0.423359   \n",
      "16            50         10                 5         0.05      0.421946   \n",
      "11            80          8                 5         sqrt      0.420898   \n",
      "15            30         10                 5         0.05      0.419105   \n",
      "10            50          8                 5         sqrt      0.418948   \n",
      "12            30         10                 2         0.05      0.418627   \n",
      "1             50          8                 2         0.05      0.417777   \n",
      "6             30          8                 2         sqrt      0.416851   \n",
      "4             50          8                 5         0.05      0.414571   \n",
      "17            80         10                 5         0.05      0.413816   \n",
      "2             80          8                 2         0.05      0.411921   \n",
      "29            80         12                 5         0.05      0.411552   \n",
      "0             30          8                 2         0.05      0.408794   \n",
      "28            50         12                 5         0.05      0.408513   \n",
      "9             30          8                 5         sqrt      0.407739   \n",
      "26            80         12                 2         0.05      0.407160   \n",
      "19            50         10                 2         sqrt      0.405015   \n",
      "35            80         12                 5         sqrt      0.404156   \n",
      "3             30          8                 5         0.05      0.402479   \n",
      "32            80         12                 2         sqrt      0.398473   \n",
      "20            80         10                 2         sqrt      0.395858   \n",
      "27            30         12                 5         0.05      0.395494   \n",
      "25            50         12                 2         0.05      0.394881   \n",
      "5             80          8                 5         0.05      0.394685   \n",
      "34            50         12                 5         sqrt      0.391214   \n",
      "18            30         10                 2         sqrt      0.388673   \n",
      "33            30         12                 5         sqrt      0.386081   \n",
      "24            30         12                 2         0.05      0.377062   \n",
      "22            50         10                 5         sqrt      0.376019   \n",
      "31            50         12                 2         sqrt      0.375761   \n",
      "23            80         10                 5         sqrt      0.370251   \n",
      "30            30         12                 2         sqrt      0.365319   \n",
      "21            30         10                 5         sqrt      0.363978   \n",
      "\n",
      "    val_accuracy  \n",
      "7         0.4456  \n",
      "14        0.4362  \n",
      "8         0.4472  \n",
      "13        0.4332  \n",
      "16        0.4344  \n",
      "11        0.4382  \n",
      "15        0.4286  \n",
      "10        0.4406  \n",
      "12        0.4250  \n",
      "1         0.4306  \n",
      "6         0.4242  \n",
      "4         0.4390  \n",
      "17        0.4296  \n",
      "2         0.4352  \n",
      "29        0.4304  \n",
      "0         0.4200  \n",
      "28        0.4182  \n",
      "9         0.4192  \n",
      "26        0.4174  \n",
      "19        0.4096  \n",
      "35        0.4208  \n",
      "3         0.4264  \n",
      "32        0.4130  \n",
      "20        0.4072  \n",
      "27        0.4014  \n",
      "25        0.4058  \n",
      "5         0.4254  \n",
      "34        0.4000  \n",
      "18        0.3960  \n",
      "33        0.3932  \n",
      "24        0.3888  \n",
      "22        0.3950  \n",
      "31        0.3926  \n",
      "23        0.4004  \n",
      "30        0.3816  \n",
      "21        0.3790  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n",
    "\n",
    "# Among these the best is \n",
    "# Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
    "# macro-F1 (val): 0.2244 | accuracy (val): 0.2418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88c1788-e663-44e1-b13d-f9b37a0dc9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.4181 | accuracy (val): 0.4588\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.3873 | accuracy (val): 0.4478\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.4160 | accuracy (val): 0.4666\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.3918 | accuracy (val): 0.4514\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.4229 | accuracy (val): 0.4638\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.3948 | accuracy (val): 0.4524\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.4233 | accuracy (val): 0.4712\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.4097 | accuracy (val): 0.4546\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.4233126177538175\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "6               0.5      1            0.1          6           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "0               0.5      0            0.1          4           100   \n",
      "2               0.5      0            0.1          6           100   \n",
      "7               0.5      1            0.1          6           150   \n",
      "5               0.5      1            0.1          4           150   \n",
      "3               0.5      0            0.1          6           150   \n",
      "1               0.5      0            0.1          4           150   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "6           1        0.8      0.423313        0.4712  \n",
      "4           1        0.8      0.422906        0.4638  \n",
      "0           1        0.8      0.418146        0.4588  \n",
      "2           1        0.8      0.416026        0.4666  \n",
      "7           1        0.8      0.409682        0.4546  \n",
      "5           1        0.8      0.394841        0.4524  \n",
      "3           1        0.8      0.391814        0.4514  \n",
      "1           1        0.8      0.387287        0.4478  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5], \n",
    "    \"gamma\": [0, 1], \n",
    "    \"reg_lambda\": [1], \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d27a3547-722c-438d-b51d-50c4e370c012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERFORMANCE ON TEST\n",
    "del X_trva, X_te, y_trva, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb26954f-37eb-4add-9fe5-9e92082b2d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/clip_cross_attention_emb/X_test.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_3.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_3.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/clip_cross_attention_emb/y_te_3.npy\", allow_pickle = True)\n",
    "\n",
    "X_trva = np.concatenate((X_tr, X_va), axis = 0).astype(np.float32)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6a82ad3-e710-4a24-a582-1b3b73bcbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (train): 0.3145 | accuracy (train): 0.4118\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=8, min_samples_leaf=2, n_estimators=50,\n",
      "                       n_jobs=-1, random_state=42)\n",
      "macro-F1 (train): 0.4504 | accuracy (train): 0.4580\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=1,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=-1, num_class=3, ...)\n",
      "macro-F1 (train): 0.4458 | accuracy (train): 0.4910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_trva_enc = le.fit_transform(y_trva)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=8, max_features='sqrt', min_samples_leaf=2, n_estimators=50, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 1, learning_rate = 0.1, max_depth= 6, n_estimators= 100, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_trva_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_trva, y_trva_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_trva, y_trva)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (train): {macro_f1:.4f} | accuracy (train): {acc:.4f}\")\n",
    "\n",
    "del X_trva, X_te, y_trva, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44862e31-dbe2-453f-ac4f-6ea21a6cc6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.3278 | accuracy (test): 0.3708\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 1e-05,\n",
    "        average = True,\n",
    "        class_weight = None,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_trva, y_trva)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8962062-2a95-42ec-873f-1620a05287af",
   "metadata": {},
   "source": [
    "# 2 classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb23fa6-6003-488e-b25d-5f2261c7c9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724daa07-999d-48f8-b542-3ac52fe33cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_2.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_2.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9ebaf6-0e2f-4d2a-8a1f-eef714c01a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24993, 1052) (24993,) (5000, 1052) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_va.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ffbca2-0ade-4b20-8edc-fe84fbdca452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.5744943848469726 | accuracy (val): 0.5748\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.5803967103102088 | accuracy (val): 0.5804\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.5861664794848382 | accuracy (val): 0.5862\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.5776986230489807 | accuracy (val): 0.5786\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.5978638527859992 | accuracy (val): 0.598\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.5916889649859678 | accuracy (val): 0.592\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.6251065140966521 | accuracy (val): 0.6348\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.6309103858967295 | accuracy (val): 0.6374\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "Validation macro-F1: 0.6309103858967295\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "7  0.01000     balanced      0.630910        0.6374\n",
      "6  0.01000         None      0.625107        0.6348\n",
      "4  0.00100         None      0.597864        0.5980\n",
      "5  0.00100     balanced      0.591689        0.5920\n",
      "2  0.00010         None      0.586166        0.5862\n",
      "1  0.00001     balanced      0.580397        0.5804\n",
      "3  0.00010     balanced      0.577699        0.5786\n",
      "0  0.00001         None      0.574494        0.5748\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea46ac2e-9826-4733-bb38-30c07ffad5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.3333 | accuracy (val): 0.4968\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.3333 | accuracy (val): 0.4968\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.3333 | accuracy (val): 0.4968\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.3333 | accuracy (val): 0.4968\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.33331298295061895\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09      0.333313        0.4968\n",
      "1   1.000000e-08      0.333313        0.4968\n",
      "2   1.000000e-07      0.333313        0.4968\n",
      "3   1.000000e-06      0.333313        0.4968\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e5ed74-c560-4994-8ee8-9943794f244f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5820 | accuracy (val): 0.5972\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5996 | accuracy (val): 0.6120\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5971 | accuracy (val): 0.6152\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5830 | accuracy (val): 0.6016\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5925 | accuracy (val): 0.6124\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5852 | accuracy (val): 0.6082\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5511 | accuracy (val): 0.5690\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5802 | accuracy (val): 0.5938\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5784 | accuracy (val): 0.5964\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5761 | accuracy (val): 0.5776\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5981 | accuracy (val): 0.6074\n",
      "\n",
      "Combination: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5762 | accuracy (val): 0.5966\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5668 | accuracy (val): 0.5760\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5958 | accuracy (val): 0.6042\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5901 | accuracy (val): 0.6020\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5645 | accuracy (val): 0.5806\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5832 | accuracy (val): 0.5988\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5818 | accuracy (val): 0.6032\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5515 | accuracy (val): 0.5648\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5760 | accuracy (val): 0.5884\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5927 | accuracy (val): 0.6054\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5863 | accuracy (val): 0.5890\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5965 | accuracy (val): 0.6066\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5813 | accuracy (val): 0.5976\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5206 | accuracy (val): 0.5426\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5462 | accuracy (val): 0.5658\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5454 | accuracy (val): 0.5690\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5250 | accuracy (val): 0.5554\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5290 | accuracy (val): 0.5642\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5301 | accuracy (val): 0.5696\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5380 | accuracy (val): 0.5504\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5496 | accuracy (val): 0.5616\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5578 | accuracy (val): 0.5736\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "macro-F1 (val): 0.5659 | accuracy (val): 0.5704\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.5624 | accuracy (val): 0.5750\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.5662 | accuracy (val): 0.5854\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 8, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Validation macro-F1: 0.5995971204015587\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "1             50          8                 2         0.05      0.599597   \n",
      "10            50          8                 5         sqrt      0.598090   \n",
      "2             80          8                 2         0.05      0.597092   \n",
      "22            50         10                 5         sqrt      0.596502   \n",
      "13            50         10                 2         0.05      0.595843   \n",
      "20            80         10                 2         sqrt      0.592697   \n",
      "4             50          8                 5         0.05      0.592535   \n",
      "14            80         10                 2         0.05      0.590098   \n",
      "21            30         10                 5         sqrt      0.586339   \n",
      "5             80          8                 5         0.05      0.585215   \n",
      "16            50         10                 5         0.05      0.583244   \n",
      "3             30          8                 5         0.05      0.583000   \n",
      "0             30          8                 2         0.05      0.581982   \n",
      "17            80         10                 5         0.05      0.581842   \n",
      "23            80         10                 5         sqrt      0.581318   \n",
      "7             50          8                 2         sqrt      0.580167   \n",
      "8             80          8                 2         sqrt      0.578368   \n",
      "11            80          8                 5         sqrt      0.576200   \n",
      "9             30          8                 5         sqrt      0.576074   \n",
      "19            50         10                 2         sqrt      0.576032   \n",
      "12            30         10                 2         0.05      0.566765   \n",
      "35            80         12                 5         sqrt      0.566234   \n",
      "33            30         12                 5         sqrt      0.565883   \n",
      "15            30         10                 5         0.05      0.564513   \n",
      "34            50         12                 5         sqrt      0.562444   \n",
      "32            80         12                 2         sqrt      0.557839   \n",
      "18            30         10                 2         sqrt      0.551533   \n",
      "6             30          8                 2         sqrt      0.551079   \n",
      "31            50         12                 2         sqrt      0.549604   \n",
      "25            50         12                 2         0.05      0.546205   \n",
      "26            80         12                 2         0.05      0.545410   \n",
      "30            30         12                 2         sqrt      0.537973   \n",
      "29            80         12                 5         0.05      0.530080   \n",
      "28            50         12                 5         0.05      0.528993   \n",
      "27            30         12                 5         0.05      0.524995   \n",
      "24            30         12                 2         0.05      0.520605   \n",
      "\n",
      "    val_accuracy  \n",
      "1         0.6120  \n",
      "10        0.6074  \n",
      "2         0.6152  \n",
      "22        0.6066  \n",
      "13        0.6042  \n",
      "20        0.6054  \n",
      "4         0.6124  \n",
      "14        0.6020  \n",
      "21        0.5890  \n",
      "5         0.6082  \n",
      "16        0.5988  \n",
      "3         0.6016  \n",
      "0         0.5972  \n",
      "17        0.6032  \n",
      "23        0.5976  \n",
      "7         0.5938  \n",
      "8         0.5964  \n",
      "11        0.5966  \n",
      "9         0.5776  \n",
      "19        0.5884  \n",
      "12        0.5760  \n",
      "35        0.5854  \n",
      "33        0.5704  \n",
      "15        0.5806  \n",
      "34        0.5750  \n",
      "32        0.5736  \n",
      "18        0.5648  \n",
      "6         0.5690  \n",
      "31        0.5616  \n",
      "25        0.5658  \n",
      "26        0.5690  \n",
      "30        0.5504  \n",
      "29        0.5696  \n",
      "28        0.5642  \n",
      "27        0.5554  \n",
      "24        0.5426  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [30, 50, 80],\n",
    "    \"max_depth\": [8, 10, 12],\n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n",
    "\n",
    "# Among these the best is \n",
    "# Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
    "# macro-F1 (val): 0.2244 | accuracy (val): 0.2418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0016fe5-4eee-4d66-b72f-fa0b05ef6825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.6058 | accuracy (val): 0.6248\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.6121 | accuracy (val): 0.6280\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.5836 | accuracy (val): 0.6106\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.5749 | accuracy (val): 0.6034\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.6049 | accuracy (val): 0.6244\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.6111 | accuracy (val): 0.6284\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.5866 | accuracy (val): 0.6112\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.5828 | accuracy (val): 0.6080\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.6121097824411358\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "1               0.5      0            0.1          4           150   \n",
      "5               0.5      1            0.1          4           150   \n",
      "0               0.5      0            0.1          4           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "6               0.5      1            0.1          6           100   \n",
      "2               0.5      0            0.1          6           100   \n",
      "7               0.5      1            0.1          6           150   \n",
      "3               0.5      0            0.1          6           150   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "1           1        0.8      0.612110        0.6280  \n",
      "5           1        0.8      0.611051        0.6284  \n",
      "0           1        0.8      0.605790        0.6248  \n",
      "4           1        0.8      0.604855        0.6244  \n",
      "6           1        0.8      0.586587        0.6112  \n",
      "2           1        0.8      0.583609        0.6106  \n",
      "7           1        0.8      0.582837        0.6080  \n",
      "3           1        0.8      0.574883        0.6034  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5], \n",
    "    \"gamma\": [0, 1], \n",
    "    \"reg_lambda\": [1], \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf3a88f-4e3c-465a-98a6-212729ac39dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_tr, y_tr, X_va, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3ba5de-f335-4891-b3f1-a8603ef57f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERFORMANCE ON TEST\n",
    "del X_trva, y_trva, X_te, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce980edf-468a-4519-b493-148b669fccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_cross_attention_emb/X_train.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_va = np.load(\"D:/dataset/clip_cross_attention_emb/X_val.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/clip_cross_attention_emb/X_test.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_cross_attention_emb/y_tr_2.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_cross_attention_emb/y_va_2.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/clip_cross_attention_emb/y_te_2.npy\", allow_pickle = True)\n",
    "\n",
    "X_trva = np.concatenate((X_tr, X_va), axis = 0).astype(np.float32)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "del X_tr, X_va, y_tr, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0092661b-e1a3-4a85-bbae-2ad509681b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (train): 0.5414 | accuracy (train): 0.5784\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=8, max_features=0.05, min_samples_leaf=2,\n",
      "                       n_estimators=50, n_jobs=-1, random_state=42)\n",
      "macro-F1 (train): 0.6331 | accuracy (train): 0.6372\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=0,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=150, n_jobs=-1, num_class=2, ...)\n",
      "macro-F1 (train): 0.6580 | accuracy (train): 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_trva_enc = le.fit_transform(y_trva)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=8, max_features=0.05, min_samples_leaf=2, n_estimators=50, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.1, max_depth= 4, n_estimators= 150, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_trva_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_trva, y_trva_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_trva, y_trva)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (train): {macro_f1:.4f} | accuracy (train): {acc:.4f}\")\n",
    "\n",
    "del X_trva, X_te, y_trva, y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db4360c-885b-40b8-a4c6-fd9ff258df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.6206 | accuracy (test): 0.6242\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 0.01,\n",
    "        average = True,\n",
    "        class_weight = 'balanced',\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_trva, y_trva)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d153e-d45a-40e5-81a2-4f748b0233e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
