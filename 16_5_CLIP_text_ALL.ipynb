{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b0f626-3dc4-45e2-b38c-4cc7bc4c326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import duckdb, torch, time, os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz, hstack, save_npz\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc, os, time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1c4b2-e5c0-42ea-a775-0a85548d86aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d4abb1-de34-4cef-868b-c0859e93e398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ef4ec5cefa4dc89b7e8cc10371be90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campione estratto da DuckDB: 5000 righe\n"
     ]
    }
   ],
   "source": [
    "# Check which model to use, depending on performance and time required on a sample set\n",
    "\n",
    "TEXT_COLUMN = \"caption_bert_clip\"\n",
    "TARGET_COLUMN = \"er_bins\"\n",
    "TABLE_NAME = \"md1718\"\n",
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "\n",
    "SAMPLE_SIZE = 5000 \n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT\n",
    "        {TEXT_COLUMN} AS {TEXT_COLUMN},\n",
    "        {TARGET_COLUMN} AS {TARGET_COLUMN}\n",
    "    FROM {TABLE_NAME}\n",
    "    WHERE {TEXT_COLUMN} IS NOT NULL\n",
    "      AND {TARGET_COLUMN} IS NOT NULL\n",
    "    ORDER BY random()\n",
    "    LIMIT {SAMPLE_SIZE}\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).df()\n",
    "\n",
    "print(f\"Campione estratto da DuckDB: {len(df)} righe\")\n",
    "\n",
    "texts = df[TEXT_COLUMN].astype(str).tolist()\n",
    "\n",
    "# Numeric target for XGBoost\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[TARGET_COLUMN])\n",
    "\n",
    "# Fixed split on indexes to ensure the same are used\n",
    "indices = np.arange(len(df))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f48442-2673-4d6c-99f0-a7ba0bbdca26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Upload CLIP model: clip-vit-b32 (openai/clip-vit-base-patch32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440d0dabde294dde83c198331563fa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimox\\anaconda3\\envs\\clip_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mimox\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time CLIP embedding extraction: 252.89 s\n",
      " Shape embeddings: (5000, 512)\n",
      "\n",
      " Upload CLIP model: clip-vit-b16 (openai/clip-vit-base-patch16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d531157cab4417b6c6c36d47f0f436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimox\\anaconda3\\envs\\clip_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mimox\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59c728c28824e3eb27f221dd72fa6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecf360ecfbf4b9fb43648d03915a5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a254b434634ea58972944ca1efc4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145e099889b34e92a44eaeecb27d6e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096221e76a794b18bf3798723a49d9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef94c10544b4b04a180633cab94e87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85e886e72b84194b901fcc775387c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2411e4eaa57e46808f94c1a9c76b0b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time CLIP embedding extraction: 256.74 s\n",
      " Shape embeddings: (5000, 512)\n",
      "\n",
      " Upload CLIP model: clip-vit-l14 (openai/clip-vit-large-patch14)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d6574fa3664e3dbc2044ed6f51fd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimox\\anaconda3\\envs\\clip_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mimox\\.cache\\huggingface\\hub\\models--openai--clip-vit-large-patch14. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f057f0f8967844808f0c86c30b9ce760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a743438428684c41870c2b2a90d35d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ea999afe814cea815e47868cbc77f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61363bf9f013426eb7740396c06c69d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb79cbca2a6e468e902aa4b8e6c847ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1dd098334e40cc9f37d8a9ae53675a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e392630a61e492488afa8b9925ae316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time CLIP embedding extraction: 536.35 s\n",
      " Shape embeddings: (5000, 768)\n",
      "\n",
      " FINAL RESULTS\n",
      "     Encoder  Embedding Time (s) Classifier  Train Time (s)  Inference Time (s)  Accuracy  F1-macro\n",
      "clip-vit-b16          256.735876    XGBoost       86.038053            0.011756     0.248  0.247757\n",
      "clip-vit-b32          252.894578    XGBoost       85.060797            0.015024     0.247  0.245733\n",
      "clip-vit-b32          252.894578  LinearSVM        3.211219            0.015683     0.246  0.244110\n",
      "clip-vit-l14          536.346948    XGBoost      140.691305            0.017699     0.242  0.241942\n",
      "clip-vit-b16          256.735876  LinearSVM        3.098709            0.004016     0.241  0.237043\n",
      "clip-vit-l14          536.346948  LinearSVM        4.337271            0.004008     0.238  0.236431\n",
      "clip-vit-l14          536.346948 NaiveBayes        0.023740            0.058707     0.239  0.233847\n",
      "clip-vit-b16          256.735876 NaiveBayes        0.021583            0.016173     0.234  0.226761\n",
      "clip-vit-b32          252.894578 NaiveBayes        0.033280            0.039093     0.227  0.221690\n"
     ]
    }
   ],
   "source": [
    "# List of encoders to test\n",
    "# Modelli CLIP da testare (puoi aggiungerne altri)\n",
    "ENCODERS = {\n",
    "    \"clip-vit-b32\": \"openai/clip-vit-base-patch32\",\n",
    "    \"clip-vit-b16\": \"openai/clip-vit-base-patch16\",\n",
    "    \"clip-vit-l14\": \"openai/clip-vit-large-patch14\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to extract embeddings\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def extract_clip_text_embeddings(model_name, model_path, texts, batch_size=32):\n",
    "    print(f\"\\n Upload CLIP model: {model_name} ({model_path})\")\n",
    "\n",
    "    # Carico modello e processor\n",
    "    model = CLIPModel.from_pretrained(model_path)\n",
    "    processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[start:start + batch_size]\n",
    "\n",
    "            # Tokenizzazione\n",
    "            inputs = processor(\n",
    "                text=batch_texts,\n",
    "                images=None,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            # Sposto su device\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "            # Estrazione embedding testuali\n",
    "            text_features = model.get_text_features(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # Normalizzazione L2 (tipica per CLIP)\n",
    "            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "            all_embeddings.append(text_features.cpu().numpy())\n",
    "\n",
    "    end_time = time.time()\n",
    "    extraction_time = end_time - start_time\n",
    "\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "    print(f\" Time CLIP embedding extraction: {extraction_time:.2f} s\")\n",
    "    print(f\" Shape embeddings: {embeddings.shape}\")\n",
    "\n",
    "    return embeddings, extraction_time\n",
    "\n",
    "# Function classify ER based on the extracted embeddings\n",
    "def benchmark_classifiers(X, y, train_idx, test_idx):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1) Linear SVM\n",
    "    svm = LinearSVC(random_state=RANDOM_STATE)\n",
    "    start_train = time.time()\n",
    "    svm.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    svm_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    svm_infer_time = end_pred - start_pred\n",
    "\n",
    "    svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "    svm_f1 = f1_score(y_test, y_pred_svm, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"LinearSVM\",\n",
    "        \"Train Time (s)\": svm_train_time,\n",
    "        \"Inference Time (s)\": svm_infer_time,\n",
    "        \"Accuracy\": svm_acc,\n",
    "        \"F1-macro\": svm_f1\n",
    "    })\n",
    "\n",
    "    # 2) Naive Bayes (GaussianNB)\n",
    "    nb = GaussianNB()\n",
    "    start_train = time.time()\n",
    "    nb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    nb_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_nb = nb.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    nb_infer_time = end_pred - start_pred\n",
    "\n",
    "    nb_acc = accuracy_score(y_test, y_pred_nb)\n",
    "    nb_f1 = f1_score(y_test, y_pred_nb, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"NaiveBayes\",\n",
    "        \"Train Time (s)\": nb_train_time,\n",
    "        \"Inference Time (s)\": nb_infer_time,\n",
    "        \"Accuracy\": nb_acc,\n",
    "        \"F1-macro\": nb_f1\n",
    "    })\n",
    "\n",
    "    # 3) XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"multi:softmax\" if len(np.unique(y)) > 2 else \"binary:logistic\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"auto\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    xgb_train_time = end_train - start_train\n",
    "\n",
    "    start_pred = time.time()\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    end_pred = time.time()\n",
    "    xgb_infer_time = end_pred - start_pred\n",
    "\n",
    "    xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "    xgb_f1 = f1_score(y_test, y_pred_xgb, average=\"macro\")\n",
    "\n",
    "    results.append({\n",
    "        \"Classifier\": \"XGBoost\",\n",
    "        \"Train Time (s)\": xgb_train_time,\n",
    "        \"Inference Time (s)\": xgb_infer_time,\n",
    "        \"Accuracy\": xgb_acc,\n",
    "        \"F1-macro\": xgb_f1\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for enc_name, enc_path in ENCODERS.items():\n",
    "    # 1) embedding + time\n",
    "    X, emb_time = extract_clip_text_embeddings(enc_name, enc_path, texts)\n",
    "\n",
    "    # 2) classification + time\n",
    "    clf_results = benchmark_classifiers(X, y, train_idx, test_idx)\n",
    "\n",
    "    # 3) encoder info\n",
    "    for r in clf_results:\n",
    "        r_with_enc = {\n",
    "            \"Encoder\": enc_name,\n",
    "            \"Embedding Time (s)\": emb_time,\n",
    "            **r\n",
    "        }\n",
    "        all_results.append(r_with_enc)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values([\"F1-macro\", \"Accuracy\"], ascending=False)\n",
    "\n",
    "print(\"\\n FINAL RESULTS\")\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(\"classification_embedding_benchmark_results.csv\", index=False)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92567d02-b4f0-4555-891c-882f906911d4",
   "metadata": {},
   "source": [
    "# EMBEDDINGS EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c850fd2-1ac0-4f12-825a-25a4b8655d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5723498-9361-42b2-96bd-1de9b61efbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name):\n",
    "    print(f\"Loading {split_name}...\")\n",
    "    df = con.sql(f\"\"\"\n",
    "        SELECT post_id, caption_bert_clip, er_bins\n",
    "        FROM md1718\n",
    "        WHERE split = '{split_name}'\n",
    "    \"\"\").df()\n",
    "    ids = df[\"post_id\"].to_numpy()\n",
    "    texts = df[\"caption_bert_clip\"].tolist()\n",
    "    y = df[\"er_bins\"]\n",
    "    del df; gc.collect()\n",
    "    print(f\"{split_name} done.\")\n",
    "    return ids, texts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95968f1e-1831-4e49-944b-ca1804a2aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e898f29ec93d44b8b9d2c5fba1154ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done.\n",
      "Loading validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9d9450904241afa6bba2e987168dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation done.\n",
      "Loading test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93102dc243c1479d91c53e9641de7d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done.\n"
     ]
    }
   ],
   "source": [
    "train_ids, Xtr_text, y_tr = load_split(\"train\")\n",
    "val_ids, Xva_text, y_val  = load_split(\"validation\")\n",
    "test_ids, Xte_text, y_te  = load_split(\"test\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667abdc3-53a7-47e5-b0cf-161753e9cfbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_name)\n",
    "model = CLIPModel.from_pretrained(clip_name)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3826ff-ea17-448b-afc8-b587d251cb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a829eaf4-5a36-4d79-92be-6eb94dbcbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_clip(texts, bs=64):\n",
    "    all_emb = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch = texts[i:i+bs]\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            # Textual embeddings\n",
    "            text_feat = model.get_text_features(\n",
    "                input_ids=inputs[\"input_ids\"], # token ids in CLIP vocabulary\n",
    "                attention_mask=inputs[\"attention_mask\"] # 1 valid token, 0 otherwise\n",
    "            )\n",
    "\n",
    "            # L2-normalization (important for CLIP)\n",
    "            text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            all_emb.append(text_feat.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4270b937-04e1-4173-8aae-a0d7add82514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU suggestions\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(8)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "emb_dir = Path(\"D:/dataset/clip_text_emb_ALL\")\n",
    "emb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save shards on memory and skips the ones already executed\n",
    "def embed_sharded_cached(texts, ids, y, split_name, shard=5000):\n",
    "    prefix = f\"{clip_name.split('/')[-1]}_{split_name}\"\n",
    "    out_files = []\n",
    "\n",
    "    # Check for correct lengths\n",
    "    n = len(texts)\n",
    "    assert len(ids) == n, f\"ids len {len(ids)} != texts len {n}\"\n",
    "    assert len(y) == n,   f\"y len {len(y)} != texts len {n}\"\n",
    "\n",
    "    for i in range(0, n, shard):\n",
    "        f = emb_dir / f\"{prefix}_{i:07d}.npy\"\n",
    "        if f.exists():\n",
    "            print(f\"[skip] {f.name}\")\n",
    "            out_files.append(f)\n",
    "            continue\n",
    "\n",
    "        part = texts[i:i+shard]\n",
    "        t0 = time.time()\n",
    "        E = embed_clip(part, bs=64).astype(\"float32\")\n",
    "        np.save(f, E)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[saved] {f.name}  [{i+len(part)}/{n}]  ({len(part)} cap in {dt:.1f}s)\")\n",
    "\n",
    "        out_files.append(f)\n",
    "\n",
    "    # Concatenate shards and save ALL\n",
    "    arrays = [np.load(f) for f in out_files]\n",
    "    E_all = np.vstack(arrays).astype(\"float32\")\n",
    "    np.save(emb_dir / f\"{prefix}_ALL.npy\", E_all)\n",
    "\n",
    "    # Safety check\n",
    "    assert E_all.shape[0] == n, \\\n",
    "        f\"Embeddings rows {E_all.shape[0]} != texts len {n}\"\n",
    "\n",
    "    npz_path = emb_dir / f\"{prefix}_ids_y.npz\"\n",
    "    np.savez(\n",
    "        npz_path,\n",
    "        ids=np.asarray(ids),\n",
    "        embeddings=E_all,\n",
    "        y=np.asarray(y)\n",
    "    )\n",
    "    print(f\"[done] Saved aligned ids + embeddings + y → {npz_path.name}\")\n",
    "\n",
    "    return E_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13718591-609b-475f-ba6f-ae357ccc383a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train…\n",
      "[saved] clip-vit-base-patch32_train_0000000.npy  [10000/773497]  (10000 cap in 385.7s)\n",
      "[saved] clip-vit-base-patch32_train_0010000.npy  [20000/773497]  (10000 cap in 414.3s)\n",
      "[saved] clip-vit-base-patch32_train_0020000.npy  [30000/773497]  (10000 cap in 404.4s)\n",
      "[saved] clip-vit-base-patch32_train_0030000.npy  [40000/773497]  (10000 cap in 421.9s)\n",
      "[saved] clip-vit-base-patch32_train_0040000.npy  [50000/773497]  (10000 cap in 422.4s)\n",
      "[saved] clip-vit-base-patch32_train_0050000.npy  [60000/773497]  (10000 cap in 412.0s)\n",
      "[saved] clip-vit-base-patch32_train_0060000.npy  [70000/773497]  (10000 cap in 419.1s)\n",
      "[saved] clip-vit-base-patch32_train_0070000.npy  [80000/773497]  (10000 cap in 400.1s)\n",
      "[saved] clip-vit-base-patch32_train_0080000.npy  [90000/773497]  (10000 cap in 427.6s)\n",
      "[saved] clip-vit-base-patch32_train_0090000.npy  [100000/773497]  (10000 cap in 407.4s)\n",
      "[saved] clip-vit-base-patch32_train_0100000.npy  [110000/773497]  (10000 cap in 424.1s)\n",
      "[saved] clip-vit-base-patch32_train_0110000.npy  [120000/773497]  (10000 cap in 414.1s)\n",
      "[saved] clip-vit-base-patch32_train_0120000.npy  [130000/773497]  (10000 cap in 408.4s)\n",
      "[saved] clip-vit-base-patch32_train_0130000.npy  [140000/773497]  (10000 cap in 407.9s)\n",
      "[saved] clip-vit-base-patch32_train_0140000.npy  [150000/773497]  (10000 cap in 410.1s)\n",
      "[saved] clip-vit-base-patch32_train_0150000.npy  [160000/773497]  (10000 cap in 408.0s)\n",
      "[saved] clip-vit-base-patch32_train_0160000.npy  [170000/773497]  (10000 cap in 405.8s)\n",
      "[saved] clip-vit-base-patch32_train_0170000.npy  [180000/773497]  (10000 cap in 424.9s)\n",
      "[saved] clip-vit-base-patch32_train_0180000.npy  [190000/773497]  (10000 cap in 416.2s)\n",
      "[saved] clip-vit-base-patch32_train_0190000.npy  [200000/773497]  (10000 cap in 416.8s)\n",
      "[saved] clip-vit-base-patch32_train_0200000.npy  [210000/773497]  (10000 cap in 406.8s)\n",
      "[saved] clip-vit-base-patch32_train_0210000.npy  [220000/773497]  (10000 cap in 410.0s)\n",
      "[saved] clip-vit-base-patch32_train_0220000.npy  [230000/773497]  (10000 cap in 393.1s)\n",
      "[saved] clip-vit-base-patch32_train_0230000.npy  [240000/773497]  (10000 cap in 413.4s)\n",
      "[saved] clip-vit-base-patch32_train_0240000.npy  [250000/773497]  (10000 cap in 403.4s)\n",
      "[saved] clip-vit-base-patch32_train_0250000.npy  [260000/773497]  (10000 cap in 406.1s)\n",
      "[saved] clip-vit-base-patch32_train_0260000.npy  [270000/773497]  (10000 cap in 409.6s)\n",
      "[saved] clip-vit-base-patch32_train_0270000.npy  [280000/773497]  (10000 cap in 406.1s)\n",
      "[saved] clip-vit-base-patch32_train_0280000.npy  [290000/773497]  (10000 cap in 415.5s)\n",
      "[saved] clip-vit-base-patch32_train_0290000.npy  [300000/773497]  (10000 cap in 415.6s)\n",
      "[saved] clip-vit-base-patch32_train_0300000.npy  [310000/773497]  (10000 cap in 418.9s)\n",
      "[saved] clip-vit-base-patch32_train_0310000.npy  [320000/773497]  (10000 cap in 416.6s)\n",
      "[saved] clip-vit-base-patch32_train_0320000.npy  [330000/773497]  (10000 cap in 410.7s)\n",
      "[saved] clip-vit-base-patch32_train_0330000.npy  [340000/773497]  (10000 cap in 411.2s)\n",
      "[saved] clip-vit-base-patch32_train_0340000.npy  [350000/773497]  (10000 cap in 414.0s)\n",
      "[saved] clip-vit-base-patch32_train_0350000.npy  [360000/773497]  (10000 cap in 409.2s)\n",
      "[saved] clip-vit-base-patch32_train_0360000.npy  [370000/773497]  (10000 cap in 402.1s)\n",
      "[saved] clip-vit-base-patch32_train_0370000.npy  [380000/773497]  (10000 cap in 407.6s)\n",
      "[saved] clip-vit-base-patch32_train_0380000.npy  [390000/773497]  (10000 cap in 412.3s)\n",
      "[saved] clip-vit-base-patch32_train_0390000.npy  [400000/773497]  (10000 cap in 412.4s)\n",
      "[saved] clip-vit-base-patch32_train_0400000.npy  [410000/773497]  (10000 cap in 396.7s)\n",
      "[saved] clip-vit-base-patch32_train_0410000.npy  [420000/773497]  (10000 cap in 405.6s)\n",
      "[saved] clip-vit-base-patch32_train_0420000.npy  [430000/773497]  (10000 cap in 406.4s)\n",
      "[saved] clip-vit-base-patch32_train_0430000.npy  [440000/773497]  (10000 cap in 409.3s)\n",
      "[saved] clip-vit-base-patch32_train_0440000.npy  [450000/773497]  (10000 cap in 413.5s)\n",
      "[saved] clip-vit-base-patch32_train_0450000.npy  [460000/773497]  (10000 cap in 411.9s)\n",
      "[saved] clip-vit-base-patch32_train_0460000.npy  [470000/773497]  (10000 cap in 409.0s)\n",
      "[saved] clip-vit-base-patch32_train_0470000.npy  [480000/773497]  (10000 cap in 405.6s)\n",
      "[saved] clip-vit-base-patch32_train_0480000.npy  [490000/773497]  (10000 cap in 413.0s)\n",
      "[saved] clip-vit-base-patch32_train_0490000.npy  [500000/773497]  (10000 cap in 420.3s)\n",
      "[saved] clip-vit-base-patch32_train_0500000.npy  [510000/773497]  (10000 cap in 439.4s)\n",
      "[saved] clip-vit-base-patch32_train_0510000.npy  [520000/773497]  (10000 cap in 432.6s)\n",
      "[saved] clip-vit-base-patch32_train_0520000.npy  [530000/773497]  (10000 cap in 415.0s)\n",
      "[saved] clip-vit-base-patch32_train_0530000.npy  [540000/773497]  (10000 cap in 402.2s)\n",
      "[saved] clip-vit-base-patch32_train_0540000.npy  [550000/773497]  (10000 cap in 420.9s)\n",
      "[saved] clip-vit-base-patch32_train_0550000.npy  [560000/773497]  (10000 cap in 407.2s)\n",
      "[saved] clip-vit-base-patch32_train_0560000.npy  [570000/773497]  (10000 cap in 416.0s)\n",
      "[saved] clip-vit-base-patch32_train_0570000.npy  [580000/773497]  (10000 cap in 392.8s)\n",
      "[saved] clip-vit-base-patch32_train_0580000.npy  [590000/773497]  (10000 cap in 406.3s)\n",
      "[saved] clip-vit-base-patch32_train_0590000.npy  [600000/773497]  (10000 cap in 423.8s)\n",
      "[saved] clip-vit-base-patch32_train_0600000.npy  [610000/773497]  (10000 cap in 406.9s)\n",
      "[saved] clip-vit-base-patch32_train_0610000.npy  [620000/773497]  (10000 cap in 425.6s)\n",
      "[saved] clip-vit-base-patch32_train_0620000.npy  [630000/773497]  (10000 cap in 427.6s)\n",
      "[saved] clip-vit-base-patch32_train_0630000.npy  [640000/773497]  (10000 cap in 410.0s)\n",
      "[saved] clip-vit-base-patch32_train_0640000.npy  [650000/773497]  (10000 cap in 404.9s)\n",
      "[saved] clip-vit-base-patch32_train_0650000.npy  [660000/773497]  (10000 cap in 393.3s)\n",
      "[saved] clip-vit-base-patch32_train_0660000.npy  [670000/773497]  (10000 cap in 405.7s)\n",
      "[saved] clip-vit-base-patch32_train_0670000.npy  [680000/773497]  (10000 cap in 408.7s)\n",
      "[saved] clip-vit-base-patch32_train_0680000.npy  [690000/773497]  (10000 cap in 406.7s)\n",
      "[saved] clip-vit-base-patch32_train_0690000.npy  [700000/773497]  (10000 cap in 404.7s)\n",
      "[saved] clip-vit-base-patch32_train_0700000.npy  [710000/773497]  (10000 cap in 416.2s)\n",
      "[saved] clip-vit-base-patch32_train_0710000.npy  [720000/773497]  (10000 cap in 418.2s)\n",
      "[saved] clip-vit-base-patch32_train_0720000.npy  [730000/773497]  (10000 cap in 418.5s)\n",
      "[saved] clip-vit-base-patch32_train_0730000.npy  [740000/773497]  (10000 cap in 412.9s)\n",
      "[saved] clip-vit-base-patch32_train_0740000.npy  [750000/773497]  (10000 cap in 422.4s)\n",
      "[saved] clip-vit-base-patch32_train_0750000.npy  [760000/773497]  (10000 cap in 425.3s)\n",
      "[saved] clip-vit-base-patch32_train_0760000.npy  [770000/773497]  (10000 cap in 413.4s)\n",
      "[saved] clip-vit-base-patch32_train_0770000.npy  [773497/773497]  (3497 cap in 146.6s)\n",
      "[done] Saved aligned ids + embeddings + y → clip-vit-base-patch32_train_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Train…\")\n",
    "E_tr = embed_sharded_cached(Xtr_text, train_ids, y_tr, \"train\", shard=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fffee27-349a-46e4-848d-8b349ec6eee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val…\n",
      "[saved] clip-vit-base-patch32_val_0000000.npy  [20000/412325]  (20000 cap in 784.3s)\n",
      "[saved] clip-vit-base-patch32_val_0020000.npy  [40000/412325]  (20000 cap in 796.0s)\n",
      "[saved] clip-vit-base-patch32_val_0040000.npy  [60000/412325]  (20000 cap in 819.0s)\n",
      "[saved] clip-vit-base-patch32_val_0060000.npy  [80000/412325]  (20000 cap in 811.2s)\n",
      "[saved] clip-vit-base-patch32_val_0080000.npy  [100000/412325]  (20000 cap in 816.7s)\n",
      "[saved] clip-vit-base-patch32_val_0100000.npy  [120000/412325]  (20000 cap in 797.7s)\n",
      "[saved] clip-vit-base-patch32_val_0120000.npy  [140000/412325]  (20000 cap in 833.4s)\n",
      "[saved] clip-vit-base-patch32_val_0140000.npy  [160000/412325]  (20000 cap in 828.6s)\n",
      "[saved] clip-vit-base-patch32_val_0160000.npy  [180000/412325]  (20000 cap in 794.1s)\n",
      "[saved] clip-vit-base-patch32_val_0180000.npy  [200000/412325]  (20000 cap in 820.0s)\n",
      "[saved] clip-vit-base-patch32_val_0200000.npy  [220000/412325]  (20000 cap in 797.7s)\n",
      "[saved] clip-vit-base-patch32_val_0220000.npy  [240000/412325]  (20000 cap in 838.6s)\n",
      "[saved] clip-vit-base-patch32_val_0240000.npy  [260000/412325]  (20000 cap in 807.3s)\n",
      "[saved] clip-vit-base-patch32_val_0260000.npy  [280000/412325]  (20000 cap in 823.7s)\n",
      "[saved] clip-vit-base-patch32_val_0280000.npy  [300000/412325]  (20000 cap in 822.3s)\n",
      "[saved] clip-vit-base-patch32_val_0300000.npy  [320000/412325]  (20000 cap in 825.1s)\n",
      "[saved] clip-vit-base-patch32_val_0320000.npy  [340000/412325]  (20000 cap in 830.6s)\n",
      "[saved] clip-vit-base-patch32_val_0340000.npy  [360000/412325]  (20000 cap in 821.9s)\n",
      "[saved] clip-vit-base-patch32_val_0360000.npy  [380000/412325]  (20000 cap in 832.6s)\n",
      "[saved] clip-vit-base-patch32_val_0380000.npy  [400000/412325]  (20000 cap in 845.1s)\n",
      "[saved] clip-vit-base-patch32_val_0400000.npy  [412325/412325]  (12325 cap in 509.8s)\n",
      "[done] Saved aligned ids + embeddings + y → clip-vit-base-patch32_val_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Val…\")\n",
    "E_va = embed_sharded_cached(Xva_text, val_ids, y_val, \"val\",   shard=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce028b1-3ff5-46d6-8e3d-221da6c8392d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test…\n",
      "[saved] clip-vit-base-patch32_test_0000000.npy  [20000/423604]  (20000 cap in 779.3s)\n",
      "[saved] clip-vit-base-patch32_test_0020000.npy  [40000/423604]  (20000 cap in 821.8s)\n",
      "[saved] clip-vit-base-patch32_test_0040000.npy  [60000/423604]  (20000 cap in 818.5s)\n",
      "[saved] clip-vit-base-patch32_test_0060000.npy  [80000/423604]  (20000 cap in 826.2s)\n",
      "[saved] clip-vit-base-patch32_test_0080000.npy  [100000/423604]  (20000 cap in 832.0s)\n",
      "[saved] clip-vit-base-patch32_test_0100000.npy  [120000/423604]  (20000 cap in 827.2s)\n",
      "[saved] clip-vit-base-patch32_test_0120000.npy  [140000/423604]  (20000 cap in 834.9s)\n",
      "[saved] clip-vit-base-patch32_test_0140000.npy  [160000/423604]  (20000 cap in 830.8s)\n",
      "[saved] clip-vit-base-patch32_test_0160000.npy  [180000/423604]  (20000 cap in 825.9s)\n",
      "[saved] clip-vit-base-patch32_test_0180000.npy  [200000/423604]  (20000 cap in 838.8s)\n",
      "[saved] clip-vit-base-patch32_test_0200000.npy  [220000/423604]  (20000 cap in 797.9s)\n",
      "[saved] clip-vit-base-patch32_test_0220000.npy  [240000/423604]  (20000 cap in 813.1s)\n",
      "[saved] clip-vit-base-patch32_test_0240000.npy  [260000/423604]  (20000 cap in 826.5s)\n",
      "[saved] clip-vit-base-patch32_test_0260000.npy  [280000/423604]  (20000 cap in 822.5s)\n",
      "[saved] clip-vit-base-patch32_test_0280000.npy  [300000/423604]  (20000 cap in 828.4s)\n",
      "[saved] clip-vit-base-patch32_test_0300000.npy  [320000/423604]  (20000 cap in 830.6s)\n",
      "[saved] clip-vit-base-patch32_test_0320000.npy  [340000/423604]  (20000 cap in 813.4s)\n",
      "[saved] clip-vit-base-patch32_test_0340000.npy  [360000/423604]  (20000 cap in 790.6s)\n",
      "[saved] clip-vit-base-patch32_test_0360000.npy  [380000/423604]  (20000 cap in 822.9s)\n",
      "[saved] clip-vit-base-patch32_test_0380000.npy  [400000/423604]  (20000 cap in 804.6s)\n",
      "[saved] clip-vit-base-patch32_test_0400000.npy  [420000/423604]  (20000 cap in 833.8s)\n",
      "[saved] clip-vit-base-patch32_test_0420000.npy  [423604/423604]  (3604 cap in 150.5s)\n",
      "[done] Saved aligned ids + embeddings + y → clip-vit-base-patch32_test_ids_y.npz\n"
     ]
    }
   ],
   "source": [
    "print(\"Test…\")\n",
    "E_te = embed_sharded_cached(Xte_text, test_ids, y_te, \"test\",  shard=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0481828-454c-4ebb-9d73-320a86eaf2b9",
   "metadata": {},
   "source": [
    "# LOAD DATA FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4ac7a6-c30c-4eb7-b7f7-7574ddde5221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773497, 512) 773497 773497\n"
     ]
    }
   ],
   "source": [
    "train = np.load(\"D:/dataset/clip_text_emb_ALL/clip-vit-base-patch32_train_ids_y.npz\", allow_pickle = True)\n",
    "\n",
    "X_tr = train[\"embeddings\"]\n",
    "y_tr = train[\"y\"]\n",
    "ids_tr = train[\"ids\"]\n",
    "\n",
    "print(X_tr.shape, len(y_tr), len(ids_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aedb603-fac1-4792-b028-67deceffc477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412325, 512) 412325 412325\n"
     ]
    }
   ],
   "source": [
    "val = np.load(\"D:/dataset/clip_text_emb_ALL/clip-vit-base-patch32_val_ids_y.npz\", allow_pickle = True)\n",
    "\n",
    "X_va = val[\"embeddings\"]\n",
    "y_va = val[\"y\"]\n",
    "ids_va = val[\"ids\"]\n",
    "\n",
    "print(X_va.shape, len(y_va), len(ids_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0c7151-b528-4982-ad18-a846a99b28e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.25431075918002505 | accuracy (val): 0.25624446734978473\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.25232602965048356 | accuracy (val): 0.26416782877584427\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.2551364166993123 | accuracy (val): 0.25695264657733585\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2520026516181246 | accuracy (val): 0.26392530164312134\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.2527830287365438 | accuracy (val): 0.25449584672285214\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2521891009621885 | accuracy (val): 0.26207481962044504\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.24961141127044825 | accuracy (val): 0.25228157400109136\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.25216087584441305 | accuracy (val): 0.258791002243376\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 0.0001, 'class_weight': None}\n",
      "Validation macro-F1: 0.2551364166993123\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "2  0.00010         None      0.255136      0.256953\n",
      "0  0.00001         None      0.254311      0.256244\n",
      "4  0.00100         None      0.252783      0.254496\n",
      "1  0.00001     balanced      0.252326      0.264168\n",
      "5  0.00100     balanced      0.252189      0.262075\n",
      "7  0.01000     balanced      0.252161      0.258791\n",
      "3  0.00010     balanced      0.252003      0.263925\n",
      "6  0.01000         None      0.249611      0.252282\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c3cb70-c318-402b-8964-08e4d8bed391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.2097 | accuracy (val): 0.2442\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.2097 | accuracy (val): 0.2442\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.2097 | accuracy (val): 0.2442\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.2097 | accuracy (val): 0.2442\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-06}\n",
      "Validation macro-F1: 0.20967730015944536\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "3   1.000000e-06      0.209677      0.244176\n",
      "0   1.000000e-09      0.209675      0.244174\n",
      "1   1.000000e-08      0.209675      0.244174\n",
      "2   1.000000e-07      0.209675      0.244174\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "# Metto i risultati in un DataFrame per ispezionarli meglio\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deff276a-0186-4c66-9e3a-2e0ea06df787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2063 | accuracy (val): 0.2310\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2058 | accuracy (val): 0.2316\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2076 | accuracy (val): 0.2313\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2059 | accuracy (val): 0.2316\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2056 | accuracy (val): 0.2298\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2038 | accuracy (val): 0.2303\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2013 | accuracy (val): 0.2288\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2058 | accuracy (val): 0.2317\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2186 | accuracy (val): 0.2335\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2174 | accuracy (val): 0.2346\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2179 | accuracy (val): 0.2335\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2177 | accuracy (val): 0.2352\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2177 | accuracy (val): 0.2336\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2167 | accuracy (val): 0.2349\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2164 | accuracy (val): 0.2325\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2155 | accuracy (val): 0.2340\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Validation macro-F1: 0.21861106450834775\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "8             50         12                 2         0.05      0.218611   \n",
      "10            50         12                 5         0.05      0.217873   \n",
      "12            50         12                 2         sqrt      0.217726   \n",
      "11            80         12                 5         0.05      0.217667   \n",
      "9             80         12                 2         0.05      0.217395   \n",
      "13            80         12                 2         sqrt      0.216741   \n",
      "14            50         12                 5         sqrt      0.216405   \n",
      "15            80         12                 5         sqrt      0.215508   \n",
      "2             50         10                 5         0.05      0.207619   \n",
      "0             50         10                 2         0.05      0.206274   \n",
      "3             80         10                 5         0.05      0.205852   \n",
      "1             80         10                 2         0.05      0.205834   \n",
      "7             80         10                 5         sqrt      0.205802   \n",
      "4             50         10                 2         sqrt      0.205587   \n",
      "5             80         10                 2         sqrt      0.203847   \n",
      "6             50         10                 5         sqrt      0.201319   \n",
      "\n",
      "    val_accuracy  \n",
      "8       0.233510  \n",
      "10      0.233505  \n",
      "12      0.233629  \n",
      "11      0.235242  \n",
      "9       0.234606  \n",
      "13      0.234936  \n",
      "14      0.232542  \n",
      "15      0.234000  \n",
      "2       0.231347  \n",
      "0       0.231046  \n",
      "3       0.231594  \n",
      "1       0.231599  \n",
      "7       0.231676  \n",
      "4       0.229843  \n",
      "5       0.230275  \n",
      "6       0.228776  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 80],\n",
    "    \"max_depth\": [10, 12], \n",
    "    \"min_samples_leaf\": [2, 5],\n",
    "    \"max_features\": [0.05, \"sqrt\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8dfb430-86e0-435a-a500-9393d1227359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2460 | accuracy (val): 0.2511\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2505 | accuracy (val): 0.2550\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2527 | accuracy (val): 0.2563\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2549 | accuracy (val): 0.2581\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2460 | accuracy (val): 0.2511\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2505 | accuracy (val): 0.2550\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2521 | accuracy (val): 0.2556\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2549 | accuracy (val): 0.2580\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.25492569040039337\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "3               0.5      0            0.1          6           150   \n",
      "7               0.5      1            0.1          6           150   \n",
      "2               0.5      0            0.1          6           100   \n",
      "6               0.5      1            0.1          6           100   \n",
      "5               0.5      1            0.1          4           150   \n",
      "1               0.5      0            0.1          4           150   \n",
      "0               0.5      0            0.1          4           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "3           1        0.8      0.254926      0.258080  \n",
      "7           1        0.8      0.254904      0.258022  \n",
      "2           1        0.8      0.252726      0.256271  \n",
      "6           1        0.8      0.252119      0.255551  \n",
      "5           1        0.8      0.250478      0.254976  \n",
      "1           1        0.8      0.250478      0.254976  \n",
      "0           1        0.8      0.245957      0.251093  \n",
      "4           1        0.8      0.245957      0.251093  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150], \n",
    "    \"max_depth\": [4, 6], \n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8], \n",
    "    \"colsample_bytree\": [0.5],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"reg_lambda\": [1],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308e624-c180-404b-a2c6-948d8f1ffe26",
   "metadata": {},
   "source": [
    "# PERFORMANCE SUL TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f075940-3bc6-4114-a93a-c1744aa6f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"D:/dataset/clip_text_emb_ALL/clip-vit-base-patch32_train_ids_y.npz\", allow_pickle = True)\n",
    "\n",
    "X_tr = train[\"embeddings\"]\n",
    "y_tr = train[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32af0aee-4df6-44d5-85ab-312e3b73314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load(\"D:/dataset/clip_text_emb_ALL/clip-vit-base-patch32_val_ids_y.npz\", allow_pickle = True)\n",
    "\n",
    "X_va = val[\"embeddings\"]\n",
    "y_va = val[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deff258c-5395-44e2-a0b9-ebd484dc6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trva = np.concatenate((X_tr, X_va), axis = 0)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "del X_tr, y_tr, X_va, y_va\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5784c74d-0f4b-404a-b82f-af8fd69c4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"D:/dataset/clip_text_emb_ALL/clip-vit-base-patch32_test_ids_y.npz\", allow_pickle = True)\n",
    "\n",
    "X_te = test[\"embeddings\"]\n",
    "y_te = test[\"y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f997ad-64ff-4dba-8abc-259052161914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB(var_smoothing=1e-06)\n",
      "macro-F1 (test): 0.2069 | accuracy (test): 0.2429\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=12, max_features=0.05, min_samples_leaf=2,\n",
      "                       n_estimators=50, n_jobs=-1, random_state=42)\n",
      "macro-F1 (test): 0.2171 | accuracy (test): 0.2332\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=0,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=150, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (test): 0.2528 | accuracy (test): 0.2569\n"
     ]
    }
   ],
   "source": [
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-06),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=12, max_features=0.05, min_samples_leaf=2, n_estimators=50, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.1, max_depth= 6, n_estimators= 150, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_tr, y_tr_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_tr, y_tr)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f504548-c021-4dad-967d-23d2a992415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.2534 | accuracy (test): 0.2559\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 0.0001,\n",
    "        average = True,\n",
    "        class_weight = None,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_tr, y_tr)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
