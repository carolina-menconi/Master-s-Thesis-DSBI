{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe12c130-49a1-45dc-b9f6-29c287565155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, duckdb, torch, timm, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import duckdb, torch\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa78a86c-7db5-4491-85f8-d6623570090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b93830-8d13-485c-8f36-338db766fce6",
   "metadata": {},
   "source": [
    "# EMBEDDINGS EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e140e0c-3cd3-453a-a012-49767e481ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"\"\"CREATE OR REPLACE TABLE img_splits_clip AS\n",
    "# SELECT m.post_id, m.split, i.full_image_file, m.er_bins\n",
    "# FROM md1718 m\n",
    "# JOIN images_manifest1718_clean i ON m.post_id = i.post_id\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25a73b9-1eb6-4371-a41f-8e06c065bccb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso dispositivo: cpu | PyTorch threads: 4\n",
      "Carico CLIP: openai/clip-vit-base-patch32 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP caricato. Embedding dim = 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5250b4315a95484e821ed4c63714fc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabella img_splits caricata. Righe: 2105587\n",
      "                             post_id       split  \\\n",
      "0  breemarieblog-1904832738906943213  validation   \n",
      "1  breemarieblog-1905358465586947802  validation   \n",
      "2  breemarieblog-1906849580570598483  validation   \n",
      "3  breemarieblog-1908197204734451219  validation   \n",
      "4  breemarieblog-1908389940821287367  validation   \n",
      "\n",
      "                                     full_image_file  \n",
      "0  D:/dataset/images_224_rgb\\breemarieblog-190483...  \n",
      "1  D:/dataset/images_224_rgb\\breemarieblog-190535...  \n",
      "2  D:/dataset/images_224_rgb\\breemarieblog-190684...  \n",
      "3  D:/dataset/images_224_rgb\\breemarieblog-190819...  \n",
      "4  D:/dataset/images_224_rgb\\breemarieblog-190838...  \n",
      "train = 960048 | val = 556982 | test = 588557\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "MODEL_TAG  = \"clip_vit_b32\"\n",
    "IMG_DIR = r\"D:/dataset/images_224_rgb\"\n",
    "BATCH_SIZE = 8\n",
    "SHARD_SIZE = 20000\n",
    "OUT_DIR = \"D:/dataset/clip_img_emb_ALL\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "num_threads = max(1, (os.cpu_count() or 4) // 2)\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Uso dispositivo: {device} | PyTorch threads: {num_threads}\")\n",
    "\n",
    "# Upload clip\n",
    "print(f\"Carico CLIP: {MODEL_NAME} ...\")\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_NAME) # Upload CLIP model with the encoder\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME) # It preprocesses images to fit CLIP requests\n",
    "\n",
    "# Freeze weights because we are not training but just extracting featrues\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "clip_model.eval() # to stabilize the embeddings and make them deterministic\n",
    "clip_model.to(device)\n",
    "\n",
    "# Get the image embeddings size: Embedding dim = 512\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 224, 224)\n",
    "    feats_dummy = clip_model.get_image_features(pixel_values=dummy.to(device))\n",
    "    feat_dim = feats_dummy.shape[-1]\n",
    "\n",
    "print(f\"CLIP caricato. Embedding dim = {feat_dim}\")\n",
    "\n",
    "\n",
    "# Upload images tabel and retrieve the path\n",
    "df = con.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM img_splits_clip\n",
    "\"\"\").df()\n",
    "print(\"Tabella img_splits caricata. Righe:\", len(df))\n",
    "\n",
    "if \"full_image_file\" not in df.columns:\n",
    "    if \"image_file\" not in df.columns:\n",
    "        raise ValueError(\"Non trovo né 'full_image_file' né 'image_file' in img_splits.\")\n",
    "    df[\"full_image_file\"] = df[\"image_file\"].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "else:\n",
    "    df[\"full_image_file\"] = df[\"full_image_file\"].apply(lambda x: os.path.join(IMG_DIR, x))\n",
    "\n",
    "# Check\n",
    "print(df[[\"post_id\", \"split\", \"full_image_file\"]].head())\n",
    "\n",
    "# Split\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"validation\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"train = {len(train_df)} | val = {len(val_df)} | test = {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a7c5d8-f986-490b-89c8-938e9f577386",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples creati:  train=960048, val=556982, test=588557\n"
     ]
    }
   ],
   "source": [
    "def build_samples(df, path_col=\"full_image_file\"):\n",
    "    return list(zip(df[path_col].tolist(), df[\"post_id\"].tolist()))\n",
    "\n",
    "train_samples = build_samples(train_df)\n",
    "val_samples   = build_samples(val_df)\n",
    "test_samples  = build_samples(test_df)\n",
    "\n",
    "print(\"Samples creati: \",\n",
    "      f\"train={len(train_samples)}, val={len(val_samples)}, test={len(test_samples)}\")\n",
    "\n",
    "SHARD_SIZE = 20000\n",
    "def extract_and_save_clip(split_name, samples, model, processor, out_dir=OUT_DIR, shard_size=SHARD_SIZE, batch_size=BATCH_SIZE):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    prefix = f\"{MODEL_TAG}_{split_name}\"\n",
    "\n",
    "    n = len(samples)\n",
    "    print(f\"\\n[{split_name}] Totale esempi: {n}\")\n",
    "\n",
    "    if n == 0:\n",
    "        print(f\"[{split_name}] Nessun esempio, salto.\")\n",
    "        return None, None\n",
    "\n",
    "    shard_files = []\n",
    "\n",
    "    for start in range(0, n, shard_size):\n",
    "        shard_samples = samples[start:start + shard_size]\n",
    "        shard_path = out_dir / f\"{prefix}_{start:07d}.npz\"\n",
    "\n",
    "        if shard_path.exists():\n",
    "            print(f\"[{split_name}] [skip] {shard_path.name}\")\n",
    "            shard_files.append(shard_path)\n",
    "            continue\n",
    "\n",
    "        print(f\"[{split_name}] Elaboro shard {start} - {start + len(shard_samples) - 1}\")\n",
    "\n",
    "        feats_buf = []\n",
    "        id_buf = []\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        for batch_start in range(0, len(shard_samples), batch_size):\n",
    "            batch = shard_samples[batch_start:batch_start + batch_size]\n",
    "\n",
    "            pil_imgs = []\n",
    "            pids = []\n",
    "\n",
    "            for path, pid in batch:\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    pil_imgs.append(img)\n",
    "                    pids.append(pid)\n",
    "                except Exception as e:\n",
    "                    print(f\"[{split_name}] [warn] Errore nel leggere {path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if not pil_imgs:\n",
    "                continue \n",
    "\n",
    "            inputs = processor(images=pil_imgs, return_tensors=\"pt\")\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feats = model.get_image_features(pixel_values=pixel_values)\n",
    "                feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            feats = feats.float().cpu().numpy()\n",
    "\n",
    "            feats_buf.append(feats)\n",
    "            id_buf.extend(pids)\n",
    "\n",
    "            del pixel_values, inputs, pil_imgs, pids, feats\n",
    "\n",
    "        if not feats_buf:\n",
    "            print(f\"[{split_name}] [warn] Nessuna immagine valida nello shard starting {start}.\")\n",
    "            continue\n",
    "\n",
    "        F = np.concatenate(feats_buf, axis=0)\n",
    "        I = np.array(id_buf, dtype=object)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        rate = F.shape[0] / max(dt, 1e-9)\n",
    "\n",
    "        np.savez_compressed(\n",
    "            shard_path,\n",
    "            feats=F,\n",
    "            post_id=I,\n",
    "            model=MODEL_TAG,\n",
    "            feat_dim=feat_dim,\n",
    "        )\n",
    "\n",
    "        print(f\"[{split_name}] [save] {shard_path.name} | {F.shape[0]} esempi | \"\n",
    "              f\"{dt:.1f}s | {rate:.1f} img/s\")\n",
    "\n",
    "        shard_files.append(shard_path)\n",
    "\n",
    "    if not shard_files:\n",
    "        print(f\"[{split_name}] Nessuno shard creato, salto concatenazione.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"[{split_name}] Concateno {len(shard_files)} shard...\")\n",
    "\n",
    "    feats_all = []\n",
    "    ids_all = []\n",
    "\n",
    "    for fpath in shard_files:\n",
    "        data = np.load(fpath, allow_pickle=True)\n",
    "        feats_all.append(data[\"feats\"])\n",
    "        ids_all.append(data[\"post_id\"])\n",
    "\n",
    "    F_all = np.concatenate(feats_all, axis=0)\n",
    "    I_all = np.concatenate(ids_all, axis=0)\n",
    "\n",
    "    all_path = out_dir / f\"{prefix}_ALL.npz\"\n",
    "    np.savez_compressed(\n",
    "        all_path,\n",
    "        feats=F_all,\n",
    "        post_id=I_all,\n",
    "        model=MODEL_TAG,\n",
    "        feat_dim=feat_dim,\n",
    "    )\n",
    "\n",
    "    print(f\"[{split_name}] File unico: {all_path.name} | {F_all.shape[0]} esempi totali\")\n",
    "\n",
    "    return F_all, I_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57fb8145-84f6-4fb3-84c2-22c2eb167fa7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train] Totale esempi: 960048\n",
      "[train] Elaboro shard 0 - 19999\n",
      "[train] [save] clip_vit_b32_train_0000000.npz | 20000 esempi | 2000.4s | 10.0 img/s\n",
      "[train] Elaboro shard 20000 - 39999\n",
      "[train] [save] clip_vit_b32_train_0020000.npz | 20000 esempi | 1970.9s | 10.1 img/s\n",
      "[train] Elaboro shard 40000 - 59999\n",
      "[train] [save] clip_vit_b32_train_0040000.npz | 20000 esempi | 1968.6s | 10.2 img/s\n",
      "[train] Elaboro shard 60000 - 79999\n",
      "[train] [save] clip_vit_b32_train_0060000.npz | 20000 esempi | 1957.0s | 10.2 img/s\n",
      "[train] Elaboro shard 80000 - 99999\n",
      "[train] [save] clip_vit_b32_train_0080000.npz | 20000 esempi | 1964.6s | 10.2 img/s\n",
      "[train] Elaboro shard 100000 - 119999\n",
      "[train] [save] clip_vit_b32_train_0100000.npz | 20000 esempi | 1963.3s | 10.2 img/s\n",
      "[train] Elaboro shard 120000 - 139999\n",
      "[train] [save] clip_vit_b32_train_0120000.npz | 20000 esempi | 1969.6s | 10.2 img/s\n",
      "[train] Elaboro shard 140000 - 159999\n",
      "[train] [save] clip_vit_b32_train_0140000.npz | 20000 esempi | 2000.4s | 10.0 img/s\n",
      "[train] Elaboro shard 160000 - 179999\n",
      "[train] [save] clip_vit_b32_train_0160000.npz | 20000 esempi | 1978.7s | 10.1 img/s\n",
      "[train] Elaboro shard 180000 - 199999\n",
      "[train] [save] clip_vit_b32_train_0180000.npz | 20000 esempi | 1970.3s | 10.2 img/s\n",
      "[train] Elaboro shard 200000 - 219999\n",
      "[train] [save] clip_vit_b32_train_0200000.npz | 20000 esempi | 1964.8s | 10.2 img/s\n",
      "[train] Elaboro shard 220000 - 239999\n",
      "[train] [save] clip_vit_b32_train_0220000.npz | 20000 esempi | 1976.7s | 10.1 img/s\n",
      "[train] Elaboro shard 240000 - 259999\n",
      "[train] [save] clip_vit_b32_train_0240000.npz | 20000 esempi | 1983.7s | 10.1 img/s\n",
      "[train] Elaboro shard 260000 - 279999\n",
      "[train] [save] clip_vit_b32_train_0260000.npz | 20000 esempi | 1984.7s | 10.1 img/s\n",
      "[train] Elaboro shard 280000 - 299999\n",
      "[train] [save] clip_vit_b32_train_0280000.npz | 20000 esempi | 2014.6s | 9.9 img/s\n",
      "[train] Elaboro shard 300000 - 319999\n",
      "[train] [save] clip_vit_b32_train_0300000.npz | 20000 esempi | 1990.9s | 10.0 img/s\n",
      "[train] Elaboro shard 320000 - 339999\n",
      "[train] [save] clip_vit_b32_train_0320000.npz | 20000 esempi | 2002.4s | 10.0 img/s\n",
      "[train] Elaboro shard 340000 - 359999\n",
      "[train] [save] clip_vit_b32_train_0340000.npz | 20000 esempi | 2029.2s | 9.9 img/s\n",
      "[train] Elaboro shard 360000 - 379999\n",
      "[train] [save] clip_vit_b32_train_0360000.npz | 20000 esempi | 2016.0s | 9.9 img/s\n",
      "[train] Elaboro shard 380000 - 399999\n",
      "[train] [save] clip_vit_b32_train_0380000.npz | 20000 esempi | 1977.9s | 10.1 img/s\n",
      "[train] Elaboro shard 400000 - 419999\n",
      "[train] [save] clip_vit_b32_train_0400000.npz | 20000 esempi | 1910.1s | 10.5 img/s\n",
      "[train] Elaboro shard 420000 - 439999\n",
      "[train] [save] clip_vit_b32_train_0420000.npz | 20000 esempi | 1902.4s | 10.5 img/s\n",
      "[train] Elaboro shard 440000 - 459999\n",
      "[train] [save] clip_vit_b32_train_0440000.npz | 20000 esempi | 1912.7s | 10.5 img/s\n",
      "[train] Elaboro shard 460000 - 479999\n",
      "[train] [save] clip_vit_b32_train_0460000.npz | 20000 esempi | 1927.0s | 10.4 img/s\n",
      "[train] Elaboro shard 480000 - 499999\n",
      "[train] [save] clip_vit_b32_train_0480000.npz | 20000 esempi | 1952.4s | 10.2 img/s\n",
      "[train] Elaboro shard 500000 - 519999\n",
      "[train] [save] clip_vit_b32_train_0500000.npz | 20000 esempi | 1919.2s | 10.4 img/s\n",
      "[train] Elaboro shard 520000 - 539999\n",
      "[train] [save] clip_vit_b32_train_0520000.npz | 20000 esempi | 1925.5s | 10.4 img/s\n",
      "[train] Elaboro shard 540000 - 559999\n",
      "[train] [save] clip_vit_b32_train_0540000.npz | 20000 esempi | 1904.9s | 10.5 img/s\n",
      "[train] Elaboro shard 560000 - 579999\n",
      "[train] [save] clip_vit_b32_train_0560000.npz | 20000 esempi | 1907.1s | 10.5 img/s\n",
      "[train] Elaboro shard 580000 - 599999\n",
      "[train] [save] clip_vit_b32_train_0580000.npz | 20000 esempi | 1911.4s | 10.5 img/s\n",
      "[train] Elaboro shard 600000 - 619999\n",
      "[train] [save] clip_vit_b32_train_0600000.npz | 20000 esempi | 1921.0s | 10.4 img/s\n",
      "[train] Elaboro shard 620000 - 639999\n",
      "[train] [save] clip_vit_b32_train_0620000.npz | 20000 esempi | 1949.2s | 10.3 img/s\n",
      "[train] Elaboro shard 640000 - 659999\n",
      "[train] [save] clip_vit_b32_train_0640000.npz | 20000 esempi | 1973.6s | 10.1 img/s\n",
      "[train] Elaboro shard 660000 - 679999\n",
      "[train] [save] clip_vit_b32_train_0660000.npz | 20000 esempi | 1891.7s | 10.6 img/s\n",
      "[train] Elaboro shard 680000 - 699999\n",
      "[train] [save] clip_vit_b32_train_0680000.npz | 20000 esempi | 1900.4s | 10.5 img/s\n",
      "[train] Elaboro shard 700000 - 719999\n",
      "[train] [save] clip_vit_b32_train_0700000.npz | 20000 esempi | 1960.6s | 10.2 img/s\n",
      "[train] Elaboro shard 720000 - 739999\n",
      "[train] [save] clip_vit_b32_train_0720000.npz | 20000 esempi | 1991.3s | 10.0 img/s\n",
      "[train] Elaboro shard 740000 - 759999\n",
      "[train] [save] clip_vit_b32_train_0740000.npz | 20000 esempi | 1936.2s | 10.3 img/s\n",
      "[train] Elaboro shard 760000 - 779999\n",
      "[train] [save] clip_vit_b32_train_0760000.npz | 20000 esempi | 1895.6s | 10.6 img/s\n",
      "[train] Elaboro shard 780000 - 799999\n",
      "[train] [save] clip_vit_b32_train_0780000.npz | 20000 esempi | 1881.4s | 10.6 img/s\n",
      "[train] Elaboro shard 800000 - 819999\n",
      "[train] [save] clip_vit_b32_train_0800000.npz | 20000 esempi | 1889.1s | 10.6 img/s\n",
      "[train] Elaboro shard 820000 - 839999\n",
      "[train] [save] clip_vit_b32_train_0820000.npz | 20000 esempi | 1908.3s | 10.5 img/s\n",
      "[train] Elaboro shard 840000 - 859999\n",
      "[train] [save] clip_vit_b32_train_0840000.npz | 20000 esempi | 1928.4s | 10.4 img/s\n",
      "[train] Elaboro shard 860000 - 879999\n",
      "[train] [save] clip_vit_b32_train_0860000.npz | 20000 esempi | 1995.6s | 10.0 img/s\n",
      "[train] Elaboro shard 880000 - 899999\n",
      "[train] [save] clip_vit_b32_train_0880000.npz | 20000 esempi | 1979.8s | 10.1 img/s\n",
      "[train] Elaboro shard 900000 - 919999\n",
      "[train] [save] clip_vit_b32_train_0900000.npz | 20000 esempi | 1944.6s | 10.3 img/s\n",
      "[train] Elaboro shard 920000 - 939999\n",
      "[train] [save] clip_vit_b32_train_0920000.npz | 20000 esempi | 1939.8s | 10.3 img/s\n",
      "[train] Elaboro shard 940000 - 959999\n",
      "[train] [save] clip_vit_b32_train_0940000.npz | 20000 esempi | 1946.7s | 10.3 img/s\n",
      "[train] Elaboro shard 960000 - 960047\n",
      "[train] [save] clip_vit_b32_train_0960000.npz | 48 esempi | 4.9s | 9.8 img/s\n",
      "[train] Concateno 49 shard...\n",
      "[train] File unico: clip_vit_b32_train_ALL.npz | 960048 esempi totali\n"
     ]
    }
   ],
   "source": [
    "F_train, I_train = extract_and_save_clip(\n",
    "    split_name=\"train\",\n",
    "    samples=train_samples,\n",
    "    model=clip_model,\n",
    "    processor=clip_processor,\n",
    "    shard_size = 20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e834cb-8ab7-44bc-8b16-712e26683c45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[validation] Totale esempi: 556982\n",
      "[validation] Elaboro shard 0 - 19999\n",
      "[validation] [save] clip_vit_b32_validation_0000000.npz | 20000 esempi | 2081.9s | 9.6 img/s\n",
      "[validation] Elaboro shard 20000 - 39999\n",
      "[validation] [save] clip_vit_b32_validation_0020000.npz | 20000 esempi | 2007.9s | 10.0 img/s\n",
      "[validation] Elaboro shard 40000 - 59999\n",
      "[validation] [save] clip_vit_b32_validation_0040000.npz | 20000 esempi | 2004.4s | 10.0 img/s\n",
      "[validation] Elaboro shard 60000 - 79999\n",
      "[validation] [save] clip_vit_b32_validation_0060000.npz | 20000 esempi | 2013.2s | 9.9 img/s\n",
      "[validation] Elaboro shard 80000 - 99999\n",
      "[validation] [save] clip_vit_b32_validation_0080000.npz | 20000 esempi | 2004.3s | 10.0 img/s\n",
      "[validation] Elaboro shard 100000 - 119999\n",
      "[validation] [save] clip_vit_b32_validation_0100000.npz | 20000 esempi | 1983.7s | 10.1 img/s\n",
      "[validation] Elaboro shard 120000 - 139999\n",
      "[validation] [save] clip_vit_b32_validation_0120000.npz | 20000 esempi | 1994.9s | 10.0 img/s\n",
      "[validation] Elaboro shard 140000 - 159999\n",
      "[validation] [save] clip_vit_b32_validation_0140000.npz | 20000 esempi | 1937.0s | 10.3 img/s\n",
      "[validation] Elaboro shard 160000 - 179999\n",
      "[validation] [save] clip_vit_b32_validation_0160000.npz | 20000 esempi | 1970.1s | 10.2 img/s\n",
      "[validation] Elaboro shard 180000 - 199999\n",
      "[validation] [save] clip_vit_b32_validation_0180000.npz | 20000 esempi | 1947.7s | 10.3 img/s\n",
      "[validation] Elaboro shard 200000 - 219999\n",
      "[validation] [save] clip_vit_b32_validation_0200000.npz | 20000 esempi | 1941.3s | 10.3 img/s\n",
      "[validation] Elaboro shard 220000 - 239999\n",
      "[validation] [save] clip_vit_b32_validation_0220000.npz | 20000 esempi | 1933.7s | 10.3 img/s\n",
      "[validation] Elaboro shard 240000 - 259999\n",
      "[validation] [save] clip_vit_b32_validation_0240000.npz | 20000 esempi | 1943.9s | 10.3 img/s\n",
      "[validation] Elaboro shard 260000 - 279999\n",
      "[validation] [save] clip_vit_b32_validation_0260000.npz | 20000 esempi | 1989.0s | 10.1 img/s\n",
      "[validation] Elaboro shard 280000 - 299999\n",
      "[validation] [save] clip_vit_b32_validation_0280000.npz | 20000 esempi | 2005.3s | 10.0 img/s\n",
      "[validation] Elaboro shard 300000 - 319999\n",
      "[validation] [save] clip_vit_b32_validation_0300000.npz | 20000 esempi | 1950.3s | 10.3 img/s\n",
      "[validation] Elaboro shard 320000 - 339999\n",
      "[validation] [save] clip_vit_b32_validation_0320000.npz | 20000 esempi | 1966.3s | 10.2 img/s\n",
      "[validation] Elaboro shard 340000 - 359999\n",
      "[validation] [save] clip_vit_b32_validation_0340000.npz | 20000 esempi | 1967.1s | 10.2 img/s\n",
      "[validation] Elaboro shard 360000 - 379999\n",
      "[validation] [save] clip_vit_b32_validation_0360000.npz | 20000 esempi | 2067.8s | 9.7 img/s\n",
      "[validation] Elaboro shard 380000 - 399999\n",
      "[validation] [save] clip_vit_b32_validation_0380000.npz | 20000 esempi | 2023.7s | 9.9 img/s\n",
      "[validation] Elaboro shard 400000 - 419999\n",
      "[validation] [save] clip_vit_b32_validation_0400000.npz | 20000 esempi | 2002.5s | 10.0 img/s\n",
      "[validation] Elaboro shard 420000 - 439999\n",
      "[validation] [save] clip_vit_b32_validation_0420000.npz | 20000 esempi | 2076.9s | 9.6 img/s\n",
      "[validation] Elaboro shard 440000 - 459999\n",
      "[validation] [save] clip_vit_b32_validation_0440000.npz | 20000 esempi | 2041.0s | 9.8 img/s\n",
      "[validation] Elaboro shard 460000 - 479999\n",
      "[validation] [save] clip_vit_b32_validation_0460000.npz | 20000 esempi | 2052.8s | 9.7 img/s\n",
      "[validation] Elaboro shard 480000 - 499999\n",
      "[validation] [save] clip_vit_b32_validation_0480000.npz | 20000 esempi | 1992.4s | 10.0 img/s\n",
      "[validation] Elaboro shard 500000 - 519999\n",
      "[validation] [save] clip_vit_b32_validation_0500000.npz | 20000 esempi | 2037.4s | 9.8 img/s\n",
      "[validation] Elaboro shard 520000 - 539999\n",
      "[validation] [save] clip_vit_b32_validation_0520000.npz | 20000 esempi | 2016.1s | 9.9 img/s\n",
      "[validation] Elaboro shard 540000 - 556981\n",
      "[validation] [save] clip_vit_b32_validation_0540000.npz | 16982 esempi | 1658.8s | 10.2 img/s\n",
      "[validation] Concateno 28 shard...\n",
      "[validation] File unico: clip_vit_b32_validation_ALL.npz | 556982 esempi totali\n"
     ]
    }
   ],
   "source": [
    "F_val, I_val = extract_and_save_clip(\n",
    "    split_name=\"validation\",\n",
    "    samples=val_samples,\n",
    "    model=clip_model,\n",
    "    processor=clip_processor,\n",
    "    shard_size = 20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437d8e93-b2ba-4e63-a19c-8b67d3589e6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[test] Totale esempi: 588557\n",
      "[test] Elaboro shard 0 - 19999\n",
      "[test] [save] clip_vit_b32_test_0000000.npz | 20000 esempi | 2235.6s | 8.9 img/s\n",
      "[test] Elaboro shard 20000 - 39999\n",
      "[test] [save] clip_vit_b32_test_0020000.npz | 20000 esempi | 2122.3s | 9.4 img/s\n",
      "[test] Elaboro shard 40000 - 59999\n",
      "[test] [save] clip_vit_b32_test_0040000.npz | 20000 esempi | 1930.0s | 10.4 img/s\n",
      "[test] Elaboro shard 60000 - 79999\n",
      "[test] [save] clip_vit_b32_test_0060000.npz | 20000 esempi | 1945.6s | 10.3 img/s\n",
      "[test] Elaboro shard 80000 - 99999\n",
      "[test] [save] clip_vit_b32_test_0080000.npz | 20000 esempi | 2084.5s | 9.6 img/s\n",
      "[test] Elaboro shard 100000 - 119999\n",
      "[test] [save] clip_vit_b32_test_0100000.npz | 20000 esempi | 2233.6s | 9.0 img/s\n",
      "[test] Elaboro shard 120000 - 139999\n",
      "[test] [save] clip_vit_b32_test_0120000.npz | 20000 esempi | 1963.2s | 10.2 img/s\n",
      "[test] Elaboro shard 140000 - 159999\n",
      "[test] [save] clip_vit_b32_test_0140000.npz | 20000 esempi | 1959.8s | 10.2 img/s\n",
      "[test] Elaboro shard 160000 - 179999\n",
      "[test] [save] clip_vit_b32_test_0160000.npz | 20000 esempi | 1968.3s | 10.2 img/s\n",
      "[test] Elaboro shard 180000 - 199999\n",
      "[test] [save] clip_vit_b32_test_0180000.npz | 20000 esempi | 1926.6s | 10.4 img/s\n",
      "[test] Elaboro shard 200000 - 219999\n",
      "[test] [save] clip_vit_b32_test_0200000.npz | 20000 esempi | 1953.7s | 10.2 img/s\n",
      "[test] Elaboro shard 220000 - 239999\n",
      "[test] [save] clip_vit_b32_test_0220000.npz | 20000 esempi | 1922.2s | 10.4 img/s\n",
      "[test] Elaboro shard 240000 - 259999\n",
      "[test] [save] clip_vit_b32_test_0240000.npz | 20000 esempi | 1936.5s | 10.3 img/s\n",
      "[test] Elaboro shard 260000 - 279999\n",
      "[test] [save] clip_vit_b32_test_0260000.npz | 20000 esempi | 1977.6s | 10.1 img/s\n",
      "[test] Elaboro shard 280000 - 299999\n",
      "[test] [save] clip_vit_b32_test_0280000.npz | 20000 esempi | 2012.2s | 9.9 img/s\n",
      "[test] Elaboro shard 300000 - 319999\n",
      "[test] [save] clip_vit_b32_test_0300000.npz | 20000 esempi | 2006.4s | 10.0 img/s\n",
      "[test] Elaboro shard 320000 - 339999\n",
      "[test] [save] clip_vit_b32_test_0320000.npz | 20000 esempi | 1971.8s | 10.1 img/s\n",
      "[test] Elaboro shard 340000 - 359999\n",
      "[test] [save] clip_vit_b32_test_0340000.npz | 20000 esempi | 1939.0s | 10.3 img/s\n",
      "[test] Elaboro shard 360000 - 379999\n",
      "[test] [save] clip_vit_b32_test_0360000.npz | 20000 esempi | 1978.3s | 10.1 img/s\n",
      "[test] Elaboro shard 380000 - 399999\n",
      "[test] [save] clip_vit_b32_test_0380000.npz | 20000 esempi | 1990.1s | 10.0 img/s\n",
      "[test] Elaboro shard 400000 - 419999\n",
      "[test] [save] clip_vit_b32_test_0400000.npz | 20000 esempi | 1931.4s | 10.4 img/s\n",
      "[test] Elaboro shard 420000 - 439999\n",
      "[test] [save] clip_vit_b32_test_0420000.npz | 20000 esempi | 1949.3s | 10.3 img/s\n",
      "[test] Elaboro shard 440000 - 459999\n",
      "[test] [save] clip_vit_b32_test_0440000.npz | 20000 esempi | 1948.7s | 10.3 img/s\n",
      "[test] Elaboro shard 460000 - 479999\n",
      "[test] [save] clip_vit_b32_test_0460000.npz | 20000 esempi | 1935.9s | 10.3 img/s\n",
      "[test] Elaboro shard 480000 - 499999\n",
      "[test] [save] clip_vit_b32_test_0480000.npz | 20000 esempi | 1937.5s | 10.3 img/s\n",
      "[test] Elaboro shard 500000 - 519999\n",
      "[test] [save] clip_vit_b32_test_0500000.npz | 20000 esempi | 1958.5s | 10.2 img/s\n",
      "[test] Elaboro shard 520000 - 539999\n",
      "[test] [save] clip_vit_b32_test_0520000.npz | 20000 esempi | 1986.2s | 10.1 img/s\n",
      "[test] Elaboro shard 540000 - 559999\n",
      "[test] [save] clip_vit_b32_test_0540000.npz | 20000 esempi | 1956.5s | 10.2 img/s\n",
      "[test] Elaboro shard 560000 - 579999\n",
      "[test] [save] clip_vit_b32_test_0560000.npz | 20000 esempi | 1935.7s | 10.3 img/s\n",
      "[test] Elaboro shard 580000 - 588556\n",
      "[test] [save] clip_vit_b32_test_0580000.npz | 8557 esempi | 830.7s | 10.3 img/s\n",
      "[test] Concateno 30 shard...\n",
      "[test] File unico: clip_vit_b32_test_ALL.npz | 588557 esempi totali\n"
     ]
    }
   ],
   "source": [
    "F_test, I_test = extract_and_save_clip(\n",
    "    split_name=\"test\",\n",
    "    samples=test_samples,\n",
    "    model=clip_model,\n",
    "    processor=clip_processor,\n",
    "    shard_size = 20000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b6231-9337-40b4-a0e6-9fc34edb8e01",
   "metadata": {},
   "source": [
    "# LOAD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb18ec67-8631-47bc-b3b0-d565bfd3f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960048, 512) 960048\n"
     ]
    }
   ],
   "source": [
    "train = np.load(\"D:/dataset/clip_img_emb_ALL/clip_vit_b32_train_ALL.npz\", allow_pickle = True)\n",
    "\n",
    "X_tr = train[\"feats\"]\n",
    "ids_tr = train[\"post_id\"]\n",
    "\n",
    "print(X_tr.shape, len(ids_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0835d0d3-ae62-4aa0-8e7b-751e63d09b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"post_id\": ids_tr\n",
    "})\n",
    "\n",
    "df[\"emb\"] = list(X_tr)\n",
    "\n",
    "agg = df.groupby(\"post_id\")[\"emb\"].apply(lambda x: np.mean(x.tolist(), axis=0))\n",
    "\n",
    "X_tr = np.stack(agg.values)\n",
    "post_ids_unique = agg.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03d10a9-7703-449e-bc0a-b86642a3ab90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773497, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9099fb83-2bb3-45cc-84c3-91642c78afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_img_emb_ALL/X_tr.npz\", X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a031ea1f-2f77-4090-b25b-3c24bca09dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0a1ac879c44f6082f038b6146b29b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids_list = post_ids_unique.tolist()\n",
    "\n",
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")\n",
    "\n",
    "con.execute(\"CREATE TEMP TABLE tmp_ids (post_id VARCHAR, ord INT)\")\n",
    "\n",
    "con.execute(\n",
    "    \"INSERT INTO tmp_ids VALUES \" +\n",
    "    \", \".join(f\"('{pid}', {i})\" for i, pid in enumerate(ids_list))\n",
    ")\n",
    "\n",
    "targets = con.execute(\"\"\"\n",
    "    SELECT md1718.er_bins\n",
    "    FROM md1718\n",
    "    JOIN tmp_ids USING(post_id)\n",
    "    ORDER BY tmp_ids.ord\n",
    "\"\"\").fetchall()\n",
    "\n",
    "y_tr = np.array([t[0] for t in targets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a19b974-61f3-4b94-8e68-97dcff0c437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_img_emb_ALL/y_tr_5.npy\", y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e798800-4de4-48d0-8eaf-4475bf6a98c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_tr, ids_tr, y_tr, train, df, agg, post_ids_unique, ids_list, targets\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541420fb-0f9b-464d-8ce2-f85b798048b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556982, 512) 556982\n"
     ]
    }
   ],
   "source": [
    "val = np.load(\"D:/dataset/clip_img_emb_ALL/clip_vit_b32_validation_ALL.npz\", allow_pickle = True)\n",
    "\n",
    "X_va = val[\"feats\"]\n",
    "ids_va = val[\"post_id\"]\n",
    "\n",
    "print(X_va.shape, len(ids_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d503252-e0e3-4d10-ba73-b7a6183e4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"post_id\": ids_va\n",
    "})\n",
    "\n",
    "df[\"emb\"] = list(X_va)\n",
    "\n",
    "agg = df.groupby(\"post_id\")[\"emb\"].apply(lambda x: np.mean(x.tolist(), axis=0))\n",
    "\n",
    "X_va = np.stack(agg.values)\n",
    "post_ids_unique_va = agg.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d959040-813f-437e-a152-a625c346c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_img_emb_ALL/X_va.npz\", X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1599229-f6ed-456c-91f0-0ac2bb15779a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up ready\n"
     ]
    }
   ],
   "source": [
    "ids_list = post_ids_unique_va.tolist()\n",
    "\n",
    "DB_PATH = \"D:/db/meta.duckdb\"\n",
    "con = duckdb.connect(DB_PATH)\n",
    "try:\n",
    "    con.execute(\"PRAGMA threads=8;\")\n",
    "except duckdb.InvalidInputException:\n",
    "    pass\n",
    "\n",
    "print(\"Set up ready\")\n",
    "\n",
    "con.execute(\"CREATE TEMP TABLE tmp_ids (post_id VARCHAR, ord INT)\")\n",
    "\n",
    "con.execute(\n",
    "    \"INSERT INTO tmp_ids VALUES \" +\n",
    "    \", \".join(f\"('{pid}', {i})\" for i, pid in enumerate(ids_list))\n",
    ")\n",
    "\n",
    "targets = con.execute(\"\"\"\n",
    "    SELECT md1718.er_bins\n",
    "    FROM md1718\n",
    "    JOIN tmp_ids USING(post_id)\n",
    "    ORDER BY tmp_ids.ord\n",
    "\"\"\").fetchall()\n",
    "\n",
    "y_va = np.array([t[0] for t in targets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b0c2a8-ef72-4128-b0c0-b5bd4be5dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412325 412325\n"
     ]
    }
   ],
   "source": [
    "print(len(y_va), len(ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "555fd1ce-9686-4599-b7cc-66a85ebe64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_img_emb_ALL/y_va_5.npy\", y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be1b7e-0f1d-472d-a0df-01a88583f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_va, ids_va, y_va, val, df, agg, ids_list, targets\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dfde1c2-7074-462d-8130-354d4d31a191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del post_ids_unique_va, ids_list, targets\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02b5108-1828-4f5f-8234-4b45c2a8610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_img_emb_ALL/X_tr.npy\", allow_pickle = True)\n",
    "X_va = np.load(\"D:/dataset/clip_img_emb_ALL/X_va.npy\", allow_pickle = True)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/clip_img_emb_ALL/y_tr_5.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_img_emb_ALL/y_va_5.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96adc71b-b932-4dcc-929a-8366a9477197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': None}\n",
      "macro-F1 (val): 0.2783466562020923 | accuracy (val): 0.28547868792821196\n",
      "\n",
      "Combination: {'alpha': 1e-05, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2687266830552627 | accuracy (val): 0.2903510580246165\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': None}\n",
      "macro-F1 (val): 0.27655811306035166 | accuracy (val): 0.28561692839386404\n",
      "\n",
      "Combination: {'alpha': 0.0001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2680011234738303 | accuracy (val): 0.28998969259685925\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': None}\n",
      "macro-F1 (val): 0.2767911240842179 | accuracy (val): 0.2842272479233614\n",
      "\n",
      "Combination: {'alpha': 0.001, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2689597151968819 | accuracy (val): 0.2890171587946401\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': None}\n",
      "macro-F1 (val): 0.27520197581030825 | accuracy (val): 0.28350209179651975\n",
      "\n",
      "Combination: {'alpha': 0.01, 'class_weight': 'balanced'}\n",
      "macro-F1 (val): 0.2678752656158571 | accuracy (val): 0.2891602498029467\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'alpha': 1e-05, 'class_weight': None}\n",
      "Validation macro-F1: 0.2783466562020923\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "     alpha class_weight  val_macro_f1  val_accuracy\n",
      "0  0.00001         None      0.278347      0.285479\n",
      "4  0.00100         None      0.276791      0.284227\n",
      "2  0.00010         None      0.276558      0.285617\n",
      "6  0.01000         None      0.275202      0.283502\n",
      "5  0.00100     balanced      0.268960      0.289017\n",
      "1  0.00001     balanced      0.268727      0.290351\n",
      "3  0.00010     balanced      0.268001      0.289990\n",
      "7  0.01000     balanced      0.267875      0.289160\n"
     ]
    }
   ],
   "source": [
    "# SGD \n",
    "param_grid = {\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",            \n",
    "        penalty=\"l2\",            \n",
    "        **params,\n",
    "        average = True,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1} | accuracy (val): {acc}\")\n",
    "\n",
    "    results.append({\n",
    "        \"alpha\": params[\"alpha\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbde8a2-1850-4309-a8f8-1f4d50bf9826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'var_smoothing': 1e-09}\n",
      "macro-F1 (val): 0.2233 | accuracy (val): 0.2765\n",
      "\n",
      "Combination: {'var_smoothing': 1e-08}\n",
      "macro-F1 (val): 0.2233 | accuracy (val): 0.2765\n",
      "\n",
      "Combination: {'var_smoothing': 1e-07}\n",
      "macro-F1 (val): 0.2233 | accuracy (val): 0.2765\n",
      "\n",
      "Combination: {'var_smoothing': 1e-06}\n",
      "macro-F1 (val): 0.2233 | accuracy (val): 0.2765\n",
      "\n",
      "Best hyperparameter configuration:\n",
      "{'var_smoothing': 1e-09}\n",
      "Validation macro-F1: 0.22334045982463513\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "   var_smoothing  val_macro_f1  val_accuracy\n",
      "0   1.000000e-09      0.223340      0.276532\n",
      "1   1.000000e-08      0.223340      0.276532\n",
      "3   1.000000e-06      0.223338      0.276527\n",
      "2   1.000000e-07      0.223336      0.276529\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES - GAUSSIAN\n",
    "param_grid_nb = {\n",
    "    \"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_nb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = GaussianNB(**params)\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"var_smoothing\": params[\"var_smoothing\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "# Metto i risultati in un DataFrame per ispezionarli meglio\n",
    "results_df = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afea9961-0b74-4eff-9f26-4c9defd46d06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2534 | accuracy (val): 0.2636\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2523 | accuracy (val): 0.2646\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2533 | accuracy (val): 0.2638\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2521 | accuracy (val): 0.2643\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2524 | accuracy (val): 0.2635\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2510 | accuracy (val): 0.2640\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2528 | accuracy (val): 0.2631\n",
      "\n",
      "Combination: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2515 | accuracy (val): 0.2639\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2632 | accuracy (val): 0.2665\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2647 | accuracy (val): 0.2690\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2636 | accuracy (val): 0.2667\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2638 | accuracy (val): 0.2683\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2628 | accuracy (val): 0.2661\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2629 | accuracy (val): 0.2677\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 50}\n",
      "macro-F1 (val): 0.2626 | accuracy (val): 0.2659\n",
      "\n",
      "Combination: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 80}\n",
      "macro-F1 (val): 0.2634 | accuracy (val): 0.2681\n",
      "\n",
      "Best hyperparameter configuration (Random Forest):\n",
      "{'max_depth': 12, 'max_features': 0.05, 'min_samples_leaf': 2, 'n_estimators': 80}\n",
      "Validation macro-F1: 0.26465415915461177\n",
      "\n",
      "Ordered results by macro-F1 (validation):\n",
      "    n_estimators  max_depth  min_samples_leaf max_features  val_macro_f1  \\\n",
      "9             80         12                 2         0.05      0.264654   \n",
      "11            80         12                 5         0.05      0.263819   \n",
      "10            50         12                 5         0.05      0.263583   \n",
      "15            80         12                 5         sqrt      0.263415   \n",
      "8             50         12                 2         0.05      0.263172   \n",
      "13            80         12                 2         sqrt      0.262914   \n",
      "12            50         12                 2         sqrt      0.262769   \n",
      "14            50         12                 5         sqrt      0.262569   \n",
      "0             50         10                 2         0.05      0.253406   \n",
      "2             50         10                 5         0.05      0.253327   \n",
      "6             50         10                 5         sqrt      0.252775   \n",
      "4             50         10                 2         sqrt      0.252429   \n",
      "1             80         10                 2         0.05      0.252334   \n",
      "3             80         10                 5         0.05      0.252089   \n",
      "7             80         10                 5         sqrt      0.251533   \n",
      "5             80         10                 2         sqrt      0.250975   \n",
      "\n",
      "    val_accuracy  \n",
      "9       0.268975  \n",
      "11      0.268293  \n",
      "10      0.266743  \n",
      "15      0.268143  \n",
      "8       0.266532  \n",
      "13      0.267689  \n",
      "12      0.266069  \n",
      "14      0.265899  \n",
      "0       0.263593  \n",
      "2       0.263831  \n",
      "6       0.263067  \n",
      "4       0.263496  \n",
      "1       0.264633  \n",
      "3       0.264333  \n",
      "7       0.263857  \n",
      "5       0.263971  \n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 80],    # combinazioni leggere\n",
    "    \"max_depth\": [10, 12],         # non troppo profonde\n",
    "    \"min_samples_leaf\": [2, 5],       # regolarizzazione\n",
    "    \"max_features\": [0.05, \"sqrt\"],   # due strategie interessanti\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_rf):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit su TRAIN\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Valutazione su VALIDATION\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_va, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_va, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_estimators\": params[\"n_estimators\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_samples_leaf\": params[\"min_samples_leaf\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    # Aggiorno il best model in base alla macro-F1\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (Random Forest):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "# Metto i risultati in un DataFrame per ispezionarli meglio\n",
    "results_df_rf = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results by macro-F1 (validation):\")\n",
    "print(results_df_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941f6db4-f2a4-4329-ac61-f4693bd61744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2714 | accuracy (val): 0.2778\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2746 | accuracy (val): 0.2806\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2766 | accuracy (val): 0.2817\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2782 | accuracy (val): 0.2833\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2714 | accuracy (val): 0.2778\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2746 | accuracy (val): 0.2806\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2770 | accuracy (val): 0.2821\n",
      "\n",
      "Combination: {'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "macro-F1 (val): 0.2787 | accuracy (val): 0.2837\n",
      "\n",
      "Best hyperparameter configuration (XGBoost):\n",
      "{'colsample_bytree': 0.5, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Validation macro-F1: 0.2786967506094626\n",
      "\n",
      "Ordered results:\n",
      "   colsample_bytree  gamma  learning_rate  max_depth  n_estimators  \\\n",
      "7               0.5      1            0.1          6           150   \n",
      "3               0.5      0            0.1          6           150   \n",
      "6               0.5      1            0.1          6           100   \n",
      "2               0.5      0            0.1          6           100   \n",
      "5               0.5      1            0.1          4           150   \n",
      "1               0.5      0            0.1          4           150   \n",
      "0               0.5      0            0.1          4           100   \n",
      "4               0.5      1            0.1          4           100   \n",
      "\n",
      "   reg_lambda  subsample  val_macro_f1  val_accuracy  \n",
      "7           1        0.8      0.278697      0.283749  \n",
      "3           1        0.8      0.278151      0.283269  \n",
      "6           1        0.8      0.276995      0.282054  \n",
      "2           1        0.8      0.276593      0.281741  \n",
      "5           1        0.8      0.274624      0.280618  \n",
      "1           1        0.8      0.274624      0.280618  \n",
      "0           1        0.8      0.271420      0.277827  \n",
      "4           1        0.8      0.271420      0.277827  \n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "# Convert the labels into numbers\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_val_enc = le.transform(y_va)\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [100, 150],\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"learning_rate\": [0.1], \n",
    "    \"subsample\": [0.8],     \n",
    "    \"colsample_bytree\": [0.5],\n",
    "    \"gamma\": [0, 1],          \n",
    "    \"reg_lambda\": [1],        \n",
    "}\n",
    "\n",
    "results = []\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid_xgb):\n",
    "    print(f\"\\nCombination: {params}\")\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        **params,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    clf.fit(X_tr, y_tr_enc)\n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = clf.predict(X_va)\n",
    "\n",
    "    macro_f1 = f1_score(y_val_enc, y_val_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_val_enc, y_val_pred)\n",
    "\n",
    "    print(f\"macro-F1 (val): {macro_f1:.4f} | accuracy (val): {acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        **params,\n",
    "        \"val_macro_f1\": macro_f1,\n",
    "        \"val_accuracy\": acc,\n",
    "    })\n",
    "\n",
    "    if macro_f1 > best_score:\n",
    "        best_score = macro_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nBest hyperparameter configuration (XGBoost):\")\n",
    "print(best_params)\n",
    "print(\"Validation macro-F1:\", best_score)\n",
    "\n",
    "results_df_xgb = pd.DataFrame(results).sort_values(\"val_macro_f1\", ascending=False)\n",
    "print(\"\\nOrdered results:\")\n",
    "print(results_df_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e42b4-d3ed-447b-8ffb-8ef109918122",
   "metadata": {},
   "source": [
    "# PERFORMANCE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3955850b-f861-4410-b4a4-6c19f8a02ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588557, 512) 588557\n"
     ]
    }
   ],
   "source": [
    "test = np.load(\"D:/dataset/clip_img_emb_ALL/clip_vit_b32_test_ALL.npz\", allow_pickle = True)\n",
    "\n",
    "# accedi alle chiavi con le stringhe tra virgolette\n",
    "X_te = test[\"feats\"]\n",
    "ids_te = test[\"post_id\"]\n",
    "\n",
    "print(X_te.shape, len(ids_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ec8c3c-a027-4db6-843d-960daf6d5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"post_id\": ids_te\n",
    "})\n",
    "\n",
    "# Inseriamo gli embeddings in un array di oggetti\n",
    "df[\"emb\"] = list(X_te)\n",
    "\n",
    "# Aggrega per post_id\n",
    "agg = df.groupby(\"post_id\")[\"emb\"].apply(lambda x: np.mean(x.tolist(), axis=0))\n",
    "\n",
    "X_te = np.stack(agg.values)\n",
    "post_ids_unique_te = agg.index.values\n",
    "\n",
    "# np.save(\"D:/dataset/clip_img_emb_ALL/X_te.npy\", X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36aa49fe-4d93-4e36-aa28-c38775741cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df5e828304e440eac8345d3f947a3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids_list = post_ids_unique_te.tolist()\n",
    "\n",
    "con.execute(\"CREATE TEMP TABLE tmp_ids (post_id VARCHAR, ord INT)\")\n",
    "\n",
    "con.execute(\n",
    "    \"INSERT INTO tmp_ids VALUES \" +\n",
    "    \", \".join(f\"('{pid}', {i})\" for i, pid in enumerate(ids_list))\n",
    ")\n",
    "\n",
    "targets = con.execute(\"\"\"\n",
    "    SELECT md1718.er_bins\n",
    "    FROM md1718\n",
    "    JOIN tmp_ids USING(post_id)\n",
    "    ORDER BY tmp_ids.ord\n",
    "\"\"\").fetchall()\n",
    "\n",
    "y_te = np.array([t[0] for t in targets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c550413-b905-49b2-b121-8b483489b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423604 423604\n"
     ]
    }
   ],
   "source": [
    "print(len(y_te), len(ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab9298c-3193-4fdc-a562-6d3b824ee897",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/dataset/clip_img_emb_ALL/y_te_5.npy\", y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d52a866-3d01-400c-97b1-6ce133d61e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_te, ids_te, y_te, test, df, agg, post_ids_unique_te, ids_list, targets\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b065aef-1fdc-4e52-bd09-0b0d49eca91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/clip_img_emb_ALL/X_tr.npy\", allow_pickle = True)\n",
    "y_tr = np.load(\"D:/dataset/clip_img_emb_ALL/y_tr_5.npy\", allow_pickle = True)\n",
    "\n",
    "X_va = np.load(\"D:/dataset/clip_img_emb_ALL/X_va.npy\", allow_pickle = True)\n",
    "y_va = np.load(\"D:/dataset/clip_img_emb_ALL/y_va_5.npy\", allow_pickle = True)\n",
    "\n",
    "X_trva = np.concatenate((X_tr, X_va), axis = 0)\n",
    "y_trva = np.concatenate((y_tr, y_va), axis = 0)\n",
    "\n",
    "np.save(\"D:/dataset/clip_img_emb_ALL/X_trva.npy\", X_trva)\n",
    "np.save(\"D:/dataset/clip_img_emb_ALL/X_trva_5.npy\", X_trva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bbab6e-9932-4db7-b1be-bebbd75d0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr= np.load(\"D:/dataset/clip_img_emb_ALL/X_trva.npy\", allow_pickle = True)\n",
    "y_tr = np.load(\"D:/dataset/clip_img_emb_ALL/y_trva_5.npy\", allow_pickle = True)\n",
    "\n",
    "X_te = np.load(\"D:/dataset/clip_img_emb_ALL/X_te.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/clip_img_emb_ALL/y_te_5.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d582955-dbc7-4817-99cc-2f994d8a1a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.1582 | accuracy (test): 0.2080\n"
     ]
    }
   ],
   "source": [
    "cfg = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha = 1e-05,\n",
    "        average = True,\n",
    "        class_weight = None,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "    )\n",
    "\n",
    "\n",
    "cfg.fit(X_tr, y_tr)\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ddc10b-2cc9-4b5a-8cca-e13c6667c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: GaussianNB()\n",
      "macro-F1 (train): 0.0793 | accuracy (train): 0.0866\n",
      "\n",
      "Configuration: RandomForestClassifier(max_depth=12, max_features=0.05, min_samples_leaf=2,\n",
      "                       n_estimators=80, n_jobs=-1, random_state=42)\n",
      "macro-F1 (train): 0.1908 | accuracy (train): 0.2854\n",
      "\n",
      "Configuration: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='mlogloss',\n",
      "              feature_types=None, feature_weights=None, gamma=1,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=150, n_jobs=-1, num_class=5, ...)\n",
      "macro-F1 (train): 0.1660 | accuracy (train): 0.2220\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "\n",
    "cfgs = [\n",
    "    GaussianNB(var_smoothing = 1e-09),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=12, max_features=0.05, min_samples_leaf=2, n_estimators=80, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    XGBClassifier(colsample_bytree = 0.5, gamma = 1, learning_rate = 0.1, max_depth= 6, n_estimators= 150, reg_lambda= 1, subsample= 0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(np.unique(y_tr_enc)),\n",
    "        tree_method=\"hist\", eval_metric=\"mlogloss\",\n",
    "        n_jobs=-1, random_state=42, verbosity=0\n",
    "    )\n",
    "]\n",
    "\n",
    "for cfg in cfgs:\n",
    "    print(f\"\\nConfiguration: {cfg}\")\n",
    "\n",
    "    # XGB requires a numerical target\n",
    "    if isinstance(cfg, XGBClassifier):\n",
    "        cfg.fit(X_tr, y_tr_enc)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "\n",
    "    else:\n",
    "        cfg.fit(X_tr, y_tr)\n",
    "        y_te_pred = cfg.predict(X_te)\n",
    "        macro_f1 = f1_score(y_te, y_te_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_te, y_te_pred)\n",
    "\n",
    "    print(f\"macro-F1 (train): {macro_f1:.4f} | accuracy (train): {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
