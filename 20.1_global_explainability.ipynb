{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23d892c-975c-4013-850b-fe1c5c1f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, duckdb, torch, timm, gc, shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import duckdb, torch\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f36db-60ed-468b-aa7c-bd6e1f96ce62",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a40892-2c60-4e7c-9024-29ea2a1eeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modality_shares_from_shap(shap_values_2d: np.ndarray):\n",
    "    abs_shap = np.abs(shap_values_2d)\n",
    "    mean_abs = abs_shap.mean(axis=0) \n",
    "\n",
    "    text_idx = np.arange(0, 512)\n",
    "    img_idx  = np.arange(512, 1024)\n",
    "    meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "    I_text = mean_abs[text_idx].sum()\n",
    "    I_img  = mean_abs[img_idx].sum()\n",
    "    I_meta = mean_abs[meta_idx].sum()\n",
    "    total = I_text + I_img + I_meta\n",
    "\n",
    "    return {\n",
    "        \"text\": float(I_text / total),\n",
    "        \"image\": float(I_img / total),\n",
    "        \"meta\": float(I_meta / total),\n",
    "    }\n",
    "\n",
    "def modality_shares_multiclass(booster, X_ex: np.ndarray, n_classes: int):\n",
    "    X_ex = X_ex.astype(np.float32)\n",
    "    dex = xgb.DMatrix(X_ex)\n",
    "\n",
    "    contrib = booster.predict(dex, pred_contribs=True)  # (n, C, 1053)\n",
    "    assert contrib.ndim == 3, f\"Expected (n, C, 1053), got {contrib.shape}\"\n",
    "    assert contrib.shape[1] == n_classes, f\"Expected {n_classes} classes, got {contrib.shape[1]}\"\n",
    "\n",
    "    # Drop bias for all classes -> (n, C, 1052)\n",
    "    shap_all = contrib[:, :, :-1]\n",
    "\n",
    "    # Overall: class-agnostic aggregation (mean abs across classes) -> (n, 1052)\n",
    "    shap_overall = np.mean(np.abs(shap_all), axis=1)\n",
    "    overall_shares = modality_shares_from_shap(shap_overall)\n",
    "\n",
    "    # Per-class: compute shares for each class separately using abs SHAP for that class\n",
    "    per_class = {}\n",
    "    for c in range(n_classes):\n",
    "        shap_c = shap_all[:, c, :]  # (n, 1052) signed\n",
    "        per_class[c] = modality_shares_from_shap(shap_c)\n",
    "\n",
    "    return overall_shares, per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d0c003-4a62-42d2-adfc-76b532335cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modality_shares_binary_by_true_label(booster, X_ex: np.ndarray, y_ex: np.ndarray):\n",
    "    X_ex = X_ex.astype(np.float32)\n",
    "    y_ex = np.asarray(y_ex).astype(int)\n",
    "\n",
    "    dex = xgb.DMatrix(X_ex)\n",
    "    contrib = booster.predict(dex, pred_contribs=True)  # (n, 1053)\n",
    "    shap_all = contrib[:, :-1]  # (n, 1052)\n",
    "\n",
    "    out = {}\n",
    "    for label in [0, 1]:\n",
    "        mask = (y_ex == label)\n",
    "        if mask.sum() == 0:\n",
    "            out[label] = None\n",
    "        else:\n",
    "            out[label] = modality_shares_from_shap(shap_all[mask])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc01a2e-01c6-434b-b423-0e383b698dd3",
   "metadata": {},
   "source": [
    "# BINARY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464e3903-c46f-4068-a58f-efe7fae2c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.load(\"D:/dataset/multimodal3/X_tr.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/multimodal3/X_te.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/multimodal3/y_tr_2.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/multimodal3/y_te_2.npy\", allow_pickle = True)\n",
    "\n",
    "map_high = {\"low\": 0, \"high\": 1}\n",
    "y_tr_enc = np.vectorize(map_high.get)(y_tr).astype(int)\n",
    "y_te_enc = np.vectorize(map_high.get)(y_te).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44fc4ec-d2e3-46f7-9856-17ea08ba460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.6877 | accuracy (test): 0.6878\n"
     ]
    }
   ],
   "source": [
    "# Train and save the model\n",
    "\n",
    "cfg = XGBClassifier(\n",
    "    colsample_bytree=0.5,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "cfg.fit(X_tr, y_tr_enc)\n",
    "\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")\n",
    "\n",
    "cfg.save_model(\"D:/dataset/explainability/xgb_clip_binary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1246261e-702f-49e6-aaa2-c7d067bf3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_binary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e92812f-13ab-43b9-a999-41e2383ccb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random numbers for selecting the indexes\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "te_explain_size = 10000 # number of test examples to compute shap on\n",
    "\n",
    "# same but for the test examples\n",
    "ex_idx = rng.choice(X_te.shape[0], size=te_explain_size, replace=False)\n",
    "\n",
    "X_ex = X_te[ex_idx]\n",
    "\n",
    "X_ex = X_ex.astype(np.float32)\n",
    "\n",
    "# Build DMatrix\n",
    "dex = xgb.DMatrix(X_ex)\n",
    "\n",
    "contrib = booster.predict(dex, pred_contribs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee2d7de-d8f0-497e-86ec-c81b9b17d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality shares:\n",
      "{'text': 0.16660605370998383, 'image': 0.4277467429637909, 'meta': 0.4056472182273865}\n"
     ]
    }
   ],
   "source": [
    "# Show the global modality contribution\n",
    "\n",
    "# Remove the last column (bias term)\n",
    "shap_values = contrib[:, :-1]\n",
    "\n",
    "abs_shap = np.abs(shap_values)\n",
    "mean_abs = abs_shap.mean(axis=0)\n",
    "\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "I_text = mean_abs[text_idx].sum()\n",
    "I_img  = mean_abs[img_idx].sum()\n",
    "I_meta = mean_abs[meta_idx].sum()\n",
    "total = I_text + I_img + I_meta\n",
    "\n",
    "print(\"Modality shares:\")\n",
    "print({\n",
    "    \"text\": float(I_text / total),\n",
    "    \"image\": float(I_img / total),\n",
    "    \"meta\": float(I_meta / total),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1116e444-c828-4088-8462-a75918a90d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.5562009\n",
      "13 0.17242076\n",
      "9 0.12112257\n",
      "7 0.08417354\n",
      "6 0.078865945\n",
      "27 0.065481834\n",
      "1 0.045130923\n",
      "10 0.0444128\n",
      "3 0.043978024\n",
      "25 0.040058717\n"
     ]
    }
   ],
   "source": [
    "# Show metadata feature importance\n",
    "\n",
    "meta_importance = mean_abs[meta_idx]\n",
    "top = np.argsort(meta_importance)[::-1][:10]\n",
    "\n",
    "for k in top:\n",
    "    print(k, meta_importance[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccdc3599-8266-48e3-b328-736c27e057dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability check (modality shares):\n",
      "\n",
      "N =   5000 | text: 0.166 | image: 0.429 | meta: 0.405\n",
      "N =  10000 | text: 0.168 | image: 0.430 | meta: 0.402\n",
      "N =  20000 | text: 0.168 | image: 0.429 | meta: 0.404\n",
      "N =  50000 | text: 0.167 | image: 0.428 | meta: 0.405\n"
     ]
    }
   ],
   "source": [
    "# Stability check for different explain set sizes\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# load booster\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(\"D:/dataset/explainability/xgb_clip_binary.json\")\n",
    "\n",
    "# feature groups\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "# explain sizes to test\n",
    "explain_sizes = [5000, 10000, 20000, 50000]\n",
    "\n",
    "print(\"Stability check (modality shares):\\n\")\n",
    "\n",
    "for explain_size in explain_sizes:\n",
    "    ex_idx = rng.choice(X_te.shape[0], size=explain_size, replace=False)\n",
    "    X_ex = X_te[ex_idx]\n",
    "\n",
    "    dex = xgb.DMatrix(X_ex)\n",
    "    contrib = booster.predict(dex, pred_contribs=True)\n",
    "\n",
    "    # remove bias term\n",
    "    shap_values = contrib[:, :-1]\n",
    "\n",
    "    abs_shap = np.abs(shap_values)\n",
    "    mean_abs = abs_shap.mean(axis=0)\n",
    "\n",
    "    I_text = mean_abs[text_idx].sum()\n",
    "    I_img  = mean_abs[img_idx].sum()\n",
    "    I_meta = mean_abs[meta_idx].sum()\n",
    "    total = I_text + I_img + I_meta\n",
    "\n",
    "    print(f\"N = {explain_size:6d} | \"\n",
    "          f\"text: {I_text/total:.3f} | \"\n",
    "          f\"image: {I_img/total:.3f} | \"\n",
    "          f\"meta: {I_meta/total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c750493-6367-4e71-8694-6ae43e284f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary shares by y_true:\n",
      "y=0: {'text': 0.16838540136814117, 'image': 0.43672969937324524, 'meta': 0.3948848843574524}\n",
      "y=1: {'text': 0.16564491391181946, 'image': 0.4192452132701874, 'meta': 0.4151098430156708}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "ex_idx = rng.choice(X_te.shape[0], size=20000, replace=False)\n",
    "X_ex = X_te[ex_idx]\n",
    "y_ex = y_te_enc[ex_idx]\n",
    "\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_binary.json\")\n",
    "\n",
    "shares_by_true = modality_shares_binary_by_true_label(booster, X_ex, y_ex)\n",
    "print(\"Binary shares by y_true:\")\n",
    "print(\"y=0:\", shares_by_true[0])\n",
    "print(\"y=1:\", shares_by_true[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7443d-12ec-4260-ac70-3ffb7d3cdcbf",
   "metadata": {},
   "source": [
    "# 3 CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e41b4f5-01a9-4915-87e4-86075fe9faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> high\n",
      "1 -> low\n",
      "2 -> medium\n"
     ]
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/multimodal3/X_tr.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/multimodal3/X_te.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/multimodal3/y_tr_3.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/multimodal3/y_te_3.npy\", allow_pickle = True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    print(i, \"->\", cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e744ffd-5002-49f2-ab2b-890a0b79a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.5180 | accuracy (test): 0.5216\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "cfg = XGBClassifier(\n",
    "    colsample_bytree=0.5,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    objective=\"multi:softprob\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    num_class = 3\n",
    ")\n",
    "\n",
    "cfg.fit(X_tr, y_tr_enc)\n",
    "\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")\n",
    "\n",
    "cfg.save_model(\"D:/dataset/explainability/xgb_clip_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3db122-4756-4ef1-8c24-e4d5ebbfadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1584178-3b4b-4327-aa08-734192c11a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrib shape: (10000, 3, 1053)\n",
      "contrib ndim: 3\n"
     ]
    }
   ],
   "source": [
    "# Select random numbers for selecting the indexes\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "te_explain_size = 10000 # number of test examples to compute shap on\n",
    "ex_idx = rng.choice(X_te.shape[0], size=te_explain_size, replace=False)\n",
    "\n",
    "# Define the subsets\n",
    "X_ex = X_te[ex_idx]\n",
    "\n",
    "X_ex = X_ex.astype(np.float32)\n",
    "dex = xgb.DMatrix(X_ex)\n",
    "\n",
    "# Compute SHAP contributions\n",
    "\n",
    "contrib = booster.predict(dex, pred_contribs=True)\n",
    "print(\"contrib shape:\", contrib.shape)\n",
    "print(\"contrib ndim:\", contrib.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c672699-39c9-4b4e-9dd2-9e6b7e830ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality shares (3-class, mean over classes):\n",
      "{'text': 0.1693553328514099, 'image': 0.4173530340194702, 'meta': 0.4132916331291199}\n"
     ]
    }
   ],
   "source": [
    "shap_all = contrib[:, :, :-1]\n",
    "\n",
    "shap_values = np.mean(np.abs(shap_all), axis=1)\n",
    "\n",
    "mean_abs = shap_values.mean(axis=0)\n",
    "\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "I_text = mean_abs[text_idx].sum()\n",
    "I_img  = mean_abs[img_idx].sum()\n",
    "I_meta = mean_abs[meta_idx].sum()\n",
    "total = I_text + I_img + I_meta\n",
    "\n",
    "print(\"Modality shares (3-class, mean over classes):\")\n",
    "print({\n",
    "    \"text\": float(I_text / total),\n",
    "    \"image\": float(I_img / total),\n",
    "    \"meta\": float(I_meta / total),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51587602-d71c-47b7-8758-288a3c07a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.28172255\n",
      "13 0.080772385\n",
      "9 0.056016944\n",
      "7 0.03924979\n",
      "27 0.033874292\n",
      "25 0.024347482\n",
      "6 0.024136435\n",
      "10 0.021430768\n",
      "3 0.02122503\n",
      "12 0.021020668\n"
     ]
    }
   ],
   "source": [
    "meta_importance = mean_abs[meta_idx]\n",
    "top = np.argsort(meta_importance)[::-1][:10]\n",
    "\n",
    "for k in top:\n",
    "    print(k, meta_importance[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1eaad3-83db-4a55-ad10-cf741bab2920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability check (modality shares) - 3-class (mean over classes):\n",
      "\n",
      "N =   5000 | text: 0.169 | image: 0.418 | meta: 0.414\n",
      "N =  10000 | text: 0.170 | image: 0.419 | meta: 0.412\n",
      "N =  20000 | text: 0.170 | image: 0.418 | meta: 0.413\n",
      "N =  50000 | text: 0.169 | image: 0.417 | meta: 0.413\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# load booster\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_3.json\")\n",
    "\n",
    "# feature groups\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "# explain sizes to test\n",
    "explain_sizes = [5000, 10000, 20000, 50000]\n",
    "\n",
    "print(\"Stability check (modality shares) - 3-class (mean over classes):\\n\")\n",
    "\n",
    "for explain_size in explain_sizes:\n",
    "    ex_idx = rng.choice(X_te.shape[0], size=explain_size, replace=False)\n",
    "    X_ex = X_te[ex_idx].astype(np.float32)\n",
    "\n",
    "    dex = xgb.DMatrix(X_ex)\n",
    "    contrib = booster.predict(dex, pred_contribs=True) \n",
    "\n",
    "    shap_all = contrib[:, :, :-1]\n",
    "\n",
    "    shap_values = np.mean(np.abs(shap_all), axis=1)\n",
    "\n",
    "    mean_abs = shap_values.mean(axis=0)\n",
    "\n",
    "    I_text = mean_abs[text_idx].sum()\n",
    "    I_img  = mean_abs[img_idx].sum()\n",
    "    I_meta = mean_abs[meta_idx].sum()\n",
    "    total = I_text + I_img + I_meta\n",
    "\n",
    "    print(f\"N = {explain_size:6d} | \"\n",
    "          f\"text: {I_text/total:.3f} | \"\n",
    "          f\"image: {I_img/total:.3f} | \"\n",
    "          f\"meta: {I_meta/total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85da6508-1e10-4e64-9232-124741e6c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL (mean over classes): {'text': 0.16893193125724792, 'image': 0.4168980121612549, 'meta': 0.4141699969768524}\n",
      "CLASS 0: {'text': 0.13781584799289703, 'image': 0.4197740852832794, 'meta': 0.44241005182266235}\n",
      "CLASS 1: {'text': 0.1542525738477707, 'image': 0.4044705927371979, 'meta': 0.4412768483161926}\n",
      "CLASS 2: {'text': 0.296787291765213, 'image': 0.44449183344841003, 'meta': 0.25872087478637695}\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "ex_idx = rng.choice(X_te.shape[0], size=10000, replace=False)\n",
    "X_ex = X_te[ex_idx]\n",
    "\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_3.json\")\n",
    "\n",
    "overall, per_class = modality_shares_multiclass(booster, X_ex, n_classes=3)\n",
    "\n",
    "print(\"OVERALL (mean over classes):\", overall)\n",
    "for c, shares in per_class.items():\n",
    "    print(f\"CLASS {c}:\", shares)\n",
    "\n",
    "# 0 high, 1 low, 2 medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e85459-1320-4f88-9f38-a0412732f6d0",
   "metadata": {},
   "source": [
    "# 5 CLASSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a0f7a3-756b-4ba8-8dbd-8b48cfcf2be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> high\n",
      "1 -> low\n",
      "2 -> medium\n",
      "3 -> very_high\n",
      "4 -> very_low\n"
     ]
    }
   ],
   "source": [
    "X_tr = np.load(\"D:/dataset/multimodal3/X_tr.npy\", allow_pickle = True).astype(np.float32)\n",
    "X_te = np.load(\"D:/dataset/multimodal3/X_te.npy\", allow_pickle = True).astype(np.float32)\n",
    "\n",
    "y_tr = np.load(\"D:/dataset/multimodal3/y_tr_5.npy\", allow_pickle = True)\n",
    "y_te = np.load(\"D:/dataset/multimodal3/y_te_5.npy\", allow_pickle = True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_tr_enc = le.fit_transform(y_tr)\n",
    "y_te_enc = le.transform(y_te)\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    print(i, \"->\", cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7cb1fb-cfd7-4b7a-ae93-42f0d091e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-F1 (test): 0.3381 | accuracy (test): 0.3507\n"
     ]
    }
   ],
   "source": [
    "cfg = XGBClassifier(\n",
    "    colsample_bytree=0.5,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=150,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    objective=\"multi:softprob\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    num_class = 3\n",
    ")\n",
    "\n",
    "cfg.fit(X_tr, y_tr_enc)\n",
    "\n",
    "y_te_pred = cfg.predict(X_te)\n",
    "macro_f1 = f1_score(y_te_enc, y_te_pred, average=\"macro\")\n",
    "acc = accuracy_score(y_te_enc, y_te_pred)\n",
    "print(f\"macro-F1 (test): {macro_f1:.4f} | accuracy (test): {acc:.4f}\")\n",
    "\n",
    "cfg.save_model(\"D:/dataset/explainability/xgb_clip_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e29fe2d8-580b-436a-bdc7-5ced2a373e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e904622-02a9-4ae4-b47e-f3cc28e97b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrib shape: (10000, 5, 1053)\n",
      "contrib ndim: 3\n"
     ]
    }
   ],
   "source": [
    "# Select random numbers for selecting the indexes\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "te_explain_size = 10000 # number of test examples to compute shap on\n",
    "ex_idx = rng.choice(X_te.shape[0], size=te_explain_size, replace=False)\n",
    "\n",
    "X_ex = X_te[ex_idx]\n",
    "X_ex = X_ex.astype(np.float32)\n",
    "\n",
    "dex = xgb.DMatrix(X_ex)\n",
    "\n",
    "contrib = booster.predict(dex, pred_contribs=True)\n",
    "print(\"contrib shape:\", contrib.shape)\n",
    "print(\"contrib ndim:\", contrib.ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c64cb6-6dde-49c2-8e1b-a01ad553bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality shares (5-class, mean over classes):\n",
      "{'text': 0.16394230723381042, 'image': 0.4095773696899414, 'meta': 0.42648032307624817}\n"
     ]
    }
   ],
   "source": [
    "shap_all = contrib[:, :, :-1]\n",
    "\n",
    "shap_values = np.mean(np.abs(shap_all), axis=1)\n",
    "\n",
    "mean_abs = shap_values.mean(axis=0)\n",
    "\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "I_text = mean_abs[text_idx].sum()\n",
    "I_img  = mean_abs[img_idx].sum()\n",
    "I_meta = mean_abs[meta_idx].sum()\n",
    "total = I_text + I_img + I_meta\n",
    "\n",
    "print(\"Modality shares (5-class, mean over classes):\")\n",
    "print({\n",
    "    \"text\": float(I_text / total),\n",
    "    \"image\": float(I_img / total),\n",
    "    \"meta\": float(I_meta / total),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9c1577-bb71-478f-a6a8-85b22fb2a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.3057474\n",
      "13 0.0879403\n",
      "9 0.056693677\n",
      "7 0.040388916\n",
      "27 0.03629002\n",
      "25 0.024588961\n",
      "6 0.023743853\n",
      "12 0.023381483\n",
      "1 0.022200642\n",
      "20 0.021179441\n"
     ]
    }
   ],
   "source": [
    "meta_importance = mean_abs[meta_idx]\n",
    "top = np.argsort(meta_importance)[::-1][:10]\n",
    "\n",
    "for k in top:\n",
    "    print(k, meta_importance[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3263053b-619a-4257-a321-365ecae1eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability check (modality shares) - 5-class (mean over classes):\n",
      "\n",
      "N =   5000 | text: 0.163 | image: 0.410 | meta: 0.427\n",
      "N =  10000 | text: 0.164 | image: 0.411 | meta: 0.425\n",
      "N =  20000 | text: 0.164 | image: 0.410 | meta: 0.426\n",
      "N =  50000 | text: 0.164 | image: 0.410 | meta: 0.427\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# load booster\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_5.json\")\n",
    "\n",
    "# feature groups\n",
    "text_idx = np.arange(0, 512)\n",
    "img_idx  = np.arange(512, 1024)\n",
    "meta_idx = np.arange(1024, 1052)\n",
    "\n",
    "# explain sizes to test\n",
    "explain_sizes = [5000, 10000, 20000, 50000]\n",
    "\n",
    "print(\"Stability check (modality shares) - 5-class (mean over classes):\\n\")\n",
    "\n",
    "for explain_size in explain_sizes:\n",
    "    ex_idx = rng.choice(X_te.shape[0], size=explain_size, replace=False)\n",
    "    X_ex = X_te[ex_idx].astype(np.float32)\n",
    "\n",
    "    dex = xgb.DMatrix(X_ex)\n",
    "    contrib = booster.predict(dex, pred_contribs=True) \n",
    "\n",
    "    shap_all = contrib[:, :, :-1]\n",
    "\n",
    "    shap_values = np.mean(np.abs(shap_all), axis=1)\n",
    "\n",
    "    mean_abs = shap_values.mean(axis=0)\n",
    "\n",
    "    I_text = mean_abs[text_idx].sum()\n",
    "    I_img  = mean_abs[img_idx].sum()\n",
    "    I_meta = mean_abs[meta_idx].sum()\n",
    "    total = I_text + I_img + I_meta\n",
    "\n",
    "    print(f\"N = {explain_size:6d} | \"\n",
    "          f\"text: {I_text/total:.3f} | \"\n",
    "          f\"image: {I_img/total:.3f} | \"\n",
    "          f\"meta: {I_meta/total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8475a091-e249-4b84-8907-7b690ffc06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL (mean over classes): {'text': 0.16346345841884613, 'image': 0.4090849757194519, 'meta': 0.427451491355896}\n",
      "CLASS 0: {'text': 0.1628570258617401, 'image': 0.43152013421058655, 'meta': 0.4056228995323181}\n",
      "CLASS 1: {'text': 0.17063139379024506, 'image': 0.4482387900352478, 'meta': 0.38112974166870117}\n",
      "CLASS 2: {'text': 0.29856738448143005, 'image': 0.4448404312133789, 'meta': 0.25659215450286865}\n",
      "CLASS 3: {'text': 0.13278134167194366, 'image': 0.3851625621318817, 'meta': 0.4820561110973358}\n",
      "CLASS 4: {'text': 0.14538677036762238, 'image': 0.38811931014060974, 'meta': 0.46649396419525146}\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "ex_idx = rng.choice(X_te.shape[0], size=10000, replace=False)\n",
    "X_ex = X_te[ex_idx]\n",
    "\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(r\"D:/dataset/explainability/xgb_clip_5.json\")\n",
    "\n",
    "overall, per_class = modality_shares_multiclass(booster, X_ex, n_classes=5)\n",
    "\n",
    "print(\"OVERALL (mean over classes):\", overall)\n",
    "for c, shares in per_class.items():\n",
    "    print(f\"CLASS {c}:\", shares)\n",
    "\n",
    "# 0 -> high\n",
    "# 1 -> low\n",
    "# 2 -> medium\n",
    "# 3 -> very_high\n",
    "# 4 -> very_low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CLIP Env)",
   "language": "python",
   "name": "clip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
